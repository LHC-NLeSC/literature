"Document Title",Authors,"Author Affiliations","Publication Title",Date Added To Xplore,"Publication Year","Volume","Issue","Start Page","End Page","Abstract","ISSN",ISBNs,"DOI",Funding Information,PDF Link,"Author Keywords","IEEE Terms","INSPEC Controlled Terms","INSPEC Non-Controlled Terms","Mesh_Terms",Article Citation Count,Patent Citation Count,"Reference Count","License",Online Date,Issue Date,"Meeting Date","Publisher",Document Identifier
"Drone identification based on CenterNet-TensorRT","L. Tao; T. Hong; Y. Guo; H. Chen; J. Zhang","Beihang University,College of Electronic and Information Engineering,Beijing,China; Yunnan Innovation Institute·BUAA,Kunming,China; Beihang University,College of Electronic and Information Engineering,Beijing,China; Beihang University,College of Electronic and Information Engineering,Beijing,China; Beijing University of Agriculture,College of Computer and Information Engineering,Beijing,China","2020 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB)","19 Mar 2021","2020","","","1","5","In recent years, due to the frequent occurrence of incidents of drone invasion and collisions, there exists a higher rate of safety accidents in crowded places. With the rapid development of drone recognition in the security field, there exist more and more drone recognition methods, most of which are costly and hard to implement. In the context of the rapid development of 5G technology, we propose a method utilizing the existing monitoring network to acquire data and deep learning approaches to detect drone targets to achieve drone recognizing, tracking and positioning. Our approach uses an improved CenterNet model to detect the presence of drones in the video frames. It also takes advantage of TensorRT to accelerate the inference of the trained model and abstract the spatio-temporal features of the drones in each video frame. A proportion integration differentiation (PID) framework is used to adjust the center of the camera and acquire the consequential actual coordinate to achieve positioning. We establish a largescale database, which includes 1097 images of drones in variant environments. Both the COCO2017 dataset and our drone dataset are used to test the original model and the accelerated model of resdcn101 and resdcn18, respectively. Extensive evaluations on both datasets demonstrate that the accelerated models with TensorRT can achieve a higher speed and a maintained detection precision.","2155-5052","978-1-7281-5784-9","10.1109/BMSB49480.2020.9379645","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9379645","5G;object detection;CenterNet;TensorRT;drone recognition","Target tracking;Target recognition;Life estimation;Cameras;Data models;Acceleration;Drones","5G mobile communication;aerospace communication;aerospace computing;aircraft control;collision avoidance;control engineering computing;data acquisition;deep learning (artificial intelligence);remotely operated vehicles;robot vision;tensors;three-term control;video signal processing;visual databases","drone identification;CenterNet-TensorRT;drone invasion;drone recognition;drone target detection;drone tracking;video frame;proportion integration differentiation framework;drone dataset;drone collisions;PID framework;drone positioning;COCO2017 dataset;5G technology;deep learning;data acquisition","","4","","21","IEEE","19 Mar 2021","","","IEEE","IEEE Conferences"
"A high-efficiency dirty-egg detection system based on YOLOv4 and TensorRT","X. Wang; X. Yue; H. Li; L. Meng","Department of Electronic and Computer Engineering, Ritsumeikan University Kusatsu, Shiga, JAPAN; Department of Electronic and Computer Engineering, Ritsumeikan University Kusatsu, Shiga, JAPAN; Department of Electronic and Computer Engineering, Ritsumeikan University Kusatsu, Shiga, JAPAN; Department of Electronic and Computer Engineering, Ritsumeikan University Kusatsu, Shiga, JAPAN","2021 International Conference on Advanced Mechatronic Systems (ICAMechS)","3 Jan 2022","2021","","","75","80","Eggshells are easily stained with dirt and the dirty eggs are more likely to be contaminated with bacteria, which has adverse effects on the health of consumers. The traditional methods to filter out the dirty eggs are mainly based on image processing, which suffers low accuracy that results in the need for manual screening in many cases. The paper proposes an embedded system based on artificial intelligence to detect dirty spots on eggshells. The proposal adopts the neural network YOLOv4 to detect dirty eggs and accelerates the detection process by utilizing the TensorRT, and finally, the system is implemented on Jetson Nano. The system achieves a high speed with no accuracy loss. Experimental results show that the system can identify multiple dirty spots of multiple eggs within one image, the accuracy reaches 75.88% and the speed is up to 2.3 fps. The system offers a promising solution in the field of dirty spots detection for eggs.","2325-0690","978-1-6654-1752-5","10.1109/ICAMechS54019.2021.9661509","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9661509","dirty eggs detection;YOLOv4;TensorRT;Jetson","COVID-19;Embedded systems;Mechatronics;Image processing;Neural networks;Manuals;Production","edge detection;image colour analysis;neural nets;quality control","high-efficiency dirty-egg detection system;TensorRT;eggshells;embedded system;YOLOv4 neural network;detection process;multiple dirty spots;dirty spots detection","","3","","26","IEEE","3 Jan 2022","","","IEEE","IEEE Conferences"
"Demystifying TensorRT: Characterizing Neural Network Inference Engine on Nvidia Edge Devices","O. Shafi; C. Rai; R. Sen; G. Ananthanarayanan","Department of Computer Science and Engineering, Indian Institute of Technology, Delhi, New Delhi, India; Department of Computer Science and Engineering, Indian Institute of Technology, Delhi, New Delhi, India; Department of Computer Science and Engineering, Indian Institute of Technology, Delhi, New Delhi, India; Department of Computer Science and Engineering, Indian Institute of Technology, Dharwad, Karnataka, India","2021 IEEE International Symposium on Workload Characterization (IISWC)","13 Jan 2022","2021","","","226","237","Edge devices are seeing tremendous growth in sensing and computational capabilities. Running state-of-the-art deep neural network (NN) based data processing on multi-core CPU processors, embedded Graphics Processing Units (GPU), Tensor Processing Units (TPU), Neural Processing Units (NPU), Deep Learning Accelerators (DLA) etc., edge devices are now able to handle heavy data computations with limited or without cloud connectivity. In addition to hardware resources, software frameworks that optimize a trained neural network (NN) model through weight clustering and pruning, weight and input-output quantization to fewer bits, fusing NN layers etc., for more efficient execution of NN inferences on edge platforms, play an important role in making machine learning at the edge (namely EdgeML) a reality. This paper is a first effort in characterizing these software frameworks for DNN inference optimizations on edge devices, especially edge GPUs which are now ubiquitously used in all embedded deep learning systems. The interactions between software optimizations and the underlying GPU hardware is carefully examined. As most NN optimization engines are proprietary softwares with undocumented internal details in the public domain, our empirical analysis on real embedded GPU platforms using a variety of widely used DNNs, provide various interesting findings. We observe tremendous performance gain and non-negligible accuracy gain from the software optimizations, but also find highly unexpected non-deterministic behaviors such as different outputs on same inputs or increased execution latency for same NN model on more powerful hardware platforms. Application developers using these proprietary software optimization engines, would benefit from our analysis and the discussed implications of our findings, with examples from real applications like intelligent traffic intersection control and Advanced Driving Assistance Systems (ADAS). There are important implications of our findings on performance modeling and prediction research too, that focus on micro-architecture modeling based application performance prediction, but should now additionally consider optimization engines that this paper examines.","","978-1-6654-4173-5","10.1109/IISWC53511.2021.00030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9668285","Neural Network;GPU;TensorRT","Deep learning;Performance evaluation;Tensors;Graphics processing units;Artificial neural networks;Predictive models;Software","coprocessors;deep learning (artificial intelligence);graphics processing units;inference mechanisms;multiprocessing systems;pattern clustering","deep learning accelerators;heavy data computations;trained neural network model;input-output quantization;NN layers;NN inferences;edge platforms;machine learning;DNN inference optimizations;edge GPUs;embedded deep learning;GPU hardware;NN optimization engines;embedded GPU platforms;hardware platforms;proprietary software optimization engines;microarchitecture modeling based application performance prediction;neural network inference engine;multicore CPU processors;embedded graphics processing units;deep neural network based data processing;TensorRT;Nvidia edge devices;tensor processing units;neural processing units","","6","","68","IEEE","13 Jan 2022","","","IEEE","IEEE Conferences"
"Deep Learning Inference Parallelization on Heterogeneous Processors With TensorRT","E. Jeong; J. Kim; S. Tan; J. Lee; S. Ha","Department of Computer Science and Engineering, Seoul National University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Seoul National University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Seoul National University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Seoul National University, Seoul, Republic of Korea; Department of Computer Science and Engineering, Seoul National University, Seoul, Republic of Korea","IEEE Embedded Systems Letters","25 Feb 2022","2022","14","1","15","18","As deep learning (DL) inference applications are increasing, an embedded device tends to equip neural processing units (NPUs) in addition to a CPU and a GPU. For fast and efficient development of DL applications, TensorRT is provided as the software development kit for the NVIDIA hardware platform, including optimizer and runtime that delivers low latency and high throughput for DL inference. Like most DL frameworks, TensorRT assumes that the inference is executed on a single processing element, GPU, or NPU, not both. In this letter, we propose a parallelization methodology to maximize the throughput of a single DL application using both GPU and NPU by exploiting various types of parallelism on TensorRT. With six real-life benchmarks, we could achieve 81%–391% throughput improvement over the baseline inference using GPU only.","1943-0671","","10.1109/LES.2021.3087707","National Research Foundation of Korea (NRF) grant; Korea Government (MSIT)(grant numbers:NRF-2019R1A2B5B02069406); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9449896","Acceleration;deep learning (DL);optimization","Graphics processing units;Pipeline processing;Throughput;Optimization;Deep learning;Engines;Space exploration","deep learning (artificial intelligence);graphics processing units;inference mechanisms;parallel architectures","inference parallelization;heterogeneous processors;TensorRT;deep learning inference applications;embedded device;neural processing units;GPU;DL applications;software development kit;NVIDIA hardware platform;DL inference;single processing element;NPU;parallelization methodology;single DL application;throughput improvement;baseline inference","","15","","16","IEEE","9 Jun 2021","","","IEEE","IEEE Journals"
"Acceleration Study of Two-Stage and Deep-Learning Based Facial Direction Detection on GPU-Based Edge Device","H. -L. Chen; K. -H. Chen; Y. -T. Hwang; C. -P. Fan","Department of Electrical Engineering, National Chung Hsing University, Taichung, R.O.C.; Department of Electronic Engineering, Feng Chia University, Taichung, R.O.C.; Department of Electrical Engineering, National Chung Hsing University, Taichung, R.O.C.; Department of Electrical Engineering, National Chung Hsing University, Taichung, R.O.C.","2022 IEEE 4th Global Conference on Life Sciences and Technologies (LifeTech)","14 Apr 2022","2022","","","429","430","When an intelligent autonomous mover is active, the facial direction messages of pedestrians become important for the use of social-aware navigation in surrounding-crowds environments. To speed-up the performance of the two-stage processing based convolutional neural network (CNN) design for facial direction detection on the cost-effective GPU-based edge device, in this paper, three acceleration methodologies are applied, including TensorRT, Multithreading, and Overclocking technologies, to speed-up the frames per second (FPS) performance on NVIDIA Jetson Nano platform. Compared with the direct two-stage implementation without using accelerations, the accelerated implementation performs more than three times speed-up ratios for facial direction detections of pedestrians.","","978-1-6654-1904-8","10.1109/LifeTech53646.2022.9754820","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9754820","Acceleration;TensorRT;multithreading;overclocking;deep-learning;two-stage interferences","Performance evaluation;Multithreading;Navigation;Image edge detection;Nanoscale devices;Real-time systems;Life sciences","convolutional neural nets;deep learning (artificial intelligence);face recognition;graphics processing units","facial direction detection;intelligent autonomous mover;social-aware navigation;surrounding-crowds environments;two-stage processing;convolutional neural network design;cost-effective GPU-based edge device;acceleration methodologies;two-stage implementation;accelerated implementation performs;acceleration study;deep-learning;facial direction messages;CNN design;TensorRT;multithreading technologies;overclocking technologies;frames per second performance;NVIDIA Jetson Nano platform","","2","","8","IEEE","14 Apr 2022","","","IEEE","IEEE Conferences"
"An improved semantic segmentation and fusion method for semantic SLAM","Z. Jia; H. Yan","School of Software, Faculty of Information Technology, Beijing University of Technology, Beijing, China; School of Software, Faculty of Information Technology, Beijing University of Technology, Beijing, China","2021 IEEE International Conference on Artificial Intelligence and Computer Applications (ICAICA)","2 Aug 2021","2021","","","209","212","Perception of the environment is an important part of robot intelligence. In order to better interact with the environment, the robot should not only know the shape of objects but also their semantics. In order to meet diversified needs, robot products are becoming more and more miniaturized, and related technologies have become research hotspots in the field. In response to this situation, this paper focuses on speed optimization based on the existing semantic map construction method to make it suitable for operation in embedded systems. This paper makes improvements to semantic segmentation and uses TensorRT to build a fast inference engine to accelerate target detection and speed up its inference speed on embedded devices. This paper uses Bayesian fusion method to fuse the semantic information of different locations to build an accurate map. Finally, in order to evaluate the real-time performance and effectiveness of this method, a test on the ADE20K data set was carried out, and the experimental results were analyzed to prove the effectiveness of the optimization of this algorithm.","","978-1-6654-1867-6","10.1109/ICAICA52286.2021.9497930","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9497930","semantic segmentation;TensorRT;embedded;Bayesian fusion","Performance evaluation;Simultaneous localization and mapping;Fuses;Shape;Semantics;Object detection;Real-time systems","Bayes methods;embedded systems;image segmentation;inference mechanisms;mobile robots;object detection;optimisation;robot vision;sensor fusion;SLAM (robots)","improved semantic segmentation;semantic SLAM;robot intelligence;robot products;miniaturized technologies;speed optimization;embedded systems;fast inference engine;Bayesian fusion method;semantic information;semantic map construction method;TensorRT;target detection","","","","16","IEEE","2 Aug 2021","","","IEEE","IEEE Conferences"
"Performance Evaluation of Deep Learning Compilers for Edge Inference","G. Verma; Y. Gupta; A. M. Malik; B. Chapman","Stony Brook University, Stony Brook, New York, USA; Stony Brook University, Stony Brook, New York, USA; Brookhaven National Laboratory, Upton, New York, USA; Brookhaven National Laboratory, Upton, New York, USA","2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)","24 Jun 2021","2021","","","858","865","Recently, edge computing has received considerable attention as a promising means to provide Deep Learning (DL) based services. However, due to the limited computation capability of the data processing units (such as CPUs, GPUs, and specialized accelerators) in edge devices, using the devices’ limited resources efficiently is a challenge that affects deep learning-based analysis services. This has led to the development of several inference compilers such as TensorRT, TensorFlow Lite, Relay, and TVM, which optimize DL inference models specifically for edge devices. These compilers operate on the standard DL models available for inferencing in various frameworks, e.g., PyTorch, TensorFlow, Caffe, PaddlePaddle, and transform them into a corresponding lightweight model. TensorFlow Lite and TensorRT are considered state-of-the-art inference compilers and encompass most of the compiler optimization techniques that have been proposed for edge computing. This paper presents a detailed performance study of TensorFlow Lite (TFLite) and TensorFlow TensorRT (TF-TRT) using commonly employed DL models for edge devices on varying hardware platforms. The work compares throughput, latency performance, and power consumption. We find that the integrated TF-TRT consistently performs better at the high precision floating point on different DL architectures, especially with GPUs using tensor cores. However, it loses its edge for model compression to TFLite at low precision. TFLite which is primarily designed for mobile applications, performs better with lightweight DL models than the deep neural network-based models. It is the first detailed performance comparison of TF-TRT and TFLite inference compilers to the best of our knowledge.","","978-1-6654-3577-2","10.1109/IPDPSW52791.2021.00128","Office of the Under Secretary of Defense; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9460587","TensorFlow-TensorRT;TensorFlow Lite;Compilers for DL;Inference at Edge","Performance evaluation;Deep learning;Tensors;Power demand;Computational modeling;Transforms;Throughput","deep learning (artificial intelligence);inference mechanisms;multiprocessing systems;neural nets;performance evaluation;program compilers","edge inference;edge computing;Deep Learning based services;computation capability;data processing units;edge devices;deep learning-based analysis services;TensorFlow Lite;DL inference models;standard DL models;lightweight model;compiler optimization techniques;detailed performance study;TensorFlow TensorRT;TF-TRT;model compression;lightweight DL models;deep neural network-based models;TFLite inference compilers","","12","","36","IEEE","24 Jun 2021","","","IEEE","IEEE Conferences"
"A Fast and Accurate Method of Power Line Intelligent Inspection Based on Edge Computing","M. Liu; Z. Li; Y. Li; Y. Liu","Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China; Department of Electrical Engineering, Shanghai Jiao Tong University, Shanghai, China","IEEE Transactions on Instrumentation and Measurement","8 Mar 2022","2022","71","","1","12","The combination of power intelligent inspection and edge computing plays a crucial role in the construction of power Internet of Things and transparent power grid. However, as a consequence of the low computing power of edge devices, the detection speed of the model running on edge devices tends to be slow. Accordingly, a fast and accurate method of power line edge intelligent inspection is proposed in this article. First, for key component detection of power lines, RepVGG, diverse branch block (DBB), efficient channel attention (ECA), and improved spatial pyramid pooling (SPP) are utilized to improve YOLOv5, giving rise to RepYOLO. In addition, to solve the problem of low accuracy in pin defect detection, a two-stage cascaded method is proposed. The first stage localizes various connection fittings in the input image, and the second stage recognizes normal pins and missing pins in these connection fittings. Finally,  $\text{C}++$  language combined with TensorRT is employed to optimize and accelerate the model on the NVIDIA Jetson Xavier NX embedded platform, fulfilling efficient power line edge intelligent inspection. Experimental results show that the TensorRT optimized RepYOLO algorithm is four times the inference speed of YOLOv5 with a 1.2% increase in accuracy. Moreover, TensorRT optimized two-stage cascaded method can achieve accurate and real-time pin defect detection on edge devices.","1557-9662","","10.1109/TIM.2022.3152855","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9722917","Edge computing;power intelligent inspection;RepYOLO;TensorRT;two-stage cascaded method","Inspection;Image edge detection;Pins;Power transmission lines;Kernel;Real-time systems;Convolution","automatic optical inspection;C++ language;cloud computing;Internet of Things;object detection;power engineering computing;power grids;power transmission lines","two-stage cascaded method;connection fittings;YOLOv5;real-time pin defect detection;edge devices;edge computing;transparent power grid;low computing power;detection speed;key component detection;efficient channel attention;power Internet of Things;power line edge intelligent inspection;RepVGG;diverse branch block;DBB;ECA;spatial pyramid pooling;SPP;RepYOLO;C++ language;TensorRT;NVIDIA Jetson Xavier NX embedded platform","","7","","33","IEEE","28 Feb 2022","","","IEEE","IEEE Journals"
"A Performance Study Depending on Execution Times of Various Frameworks in Machine Learning Inference","M. Sever; S. &#x00D6;&#x011F;&#x00FC;t","REH&#x0130;S ASELSAN A.&#x015E;., TOBB University of Economics and Technology, Ankara, Turkey; Electrical and Electronics Engineering, Bilkent University, Ankara, Turkey","2021 15th Turkish National Software Engineering Symposium (UYMS)","5 Jan 2022","2021","","","1","5","This work is intended to compare the latency of various frameworks in machine learning inference through an average power calculation model. This model is created in terms of a 2-layer neural network with PyTorch, in Python. Then, it is converted to a traced Torch Script module and also to ONNX file format. Afterwards, the C++ front-end is used for the inference process. The traced model is run with Libtorch on CPU and GPU, the ONNX file is run with ONNX Runtime on both CPU and GPU and it is also run with TensorRT on GPU. The inference execution times for 100 trials are averaged for all cases and it is realized that TensorRT with ONNX file format significantly outperforms its counterparts as expected. Hence, this work highlights the performance of TensorRT in machine learning inference and sheds light into the future by proposing several extensions.","","978-1-6654-1070-0","10.1109/UYMS54260.2021.9659677","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9659677","machine learning;inference;optimization;ONNX Runtime;TensorRT","Runtime;Neural networks;Graphics processing units;Machine learning;C++ languages;Time measurement;Python","C++ language;inference mechanisms;learning (artificial intelligence);neural nets;program diagnostics;Python","GPU;ONNX Runtime;TensorRT;inference execution times;ONNX file format;machine learning inference;2-layer neural network;traced Torch Script module;C++ front-end;Libtorch","","","","17","IEEE","5 Jan 2022","","","IEEE","IEEE Conferences"
"YOLOX on Embedded Device With CCTV & TensorRT for Intelligent Multicategories Garbage Identification and Classification","Z. Chunxiang; Q. Jiacheng; B. Wang","Engineering Training Center, China Jiliang University, Hangzhou, China; Engineering Training Center, China Jiliang University, Hangzhou, China; Engineering Training Center, China Jiliang University, Hangzhou, China","IEEE Sensors Journal","16 Aug 2022","2022","22","16","16522","16532","Today’s garbage classification is not fully automated and mostly relies on manual labor. In order to efficiently classify garbage and increase resource re-usability, society needs a modern solution. The development of artificial intelligence, especially in object detection, provides an excellent opportunity to develop real-time accurate garbage detection. This project is about real-time garbage detection using YOLOX and is used to detect seventeen categories of garbage. After the camera detects the garbage, it will automatically identify and analyze the type and specific the position of the garbage, which will be automatically picked up by a mechanical jaws. The datasets used in this paper consists of 5000 images with label. With the officially supported TensorRT optimization and acceleration technology, the model can be used in low-cost embedded devices and can detect garbage in near real-time without the need for a stable Internet connection. Our evaluation and comparison of these models includes some key metrics such as mean accuracy (mAP), inference time per frame, floating point operations(FLOPs), and the number of parameters of the model. The test results show that YOLOX’s mAP0.5:0.95 can exceed 97% and the inference speed can exceed 32 fps. The performance far exceeds that of other YOLO series, with high accuracy and speed in complex environments, making it highly applicable to today’s garbage classification needs.","1558-1748","","10.1109/JSEN.2022.3181794","Key Research and Development Program of Zhejiang Province(grant numbers:2021C01069); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9830530","Boosting;concurrent computing;intelligent robots;multi-layer neural network;neural network hardware","Costs;Object detection;Hardware;Cameras;Training;Real-time systems;Statistics","cameras;closed circuit television;computational complexity;data analysis;embedded systems;Internet;object detection;storage management","acceleration technology;low-cost embedded devices;garbage classification needs;YOLOX;intelligent multicategories garbage identification;object detection;real-time accurate garbage detection;real-time garbage detection","","","","35","IEEE","14 Jul 2022","","","IEEE","IEEE Journals"
"An intelligent hand-washing monitoring platform based on gesture recognition technology","Z. Yue; F. Wang; Z. Liu","College of Artificial Intelligence, Nankai University, Tianjin, China; College of Artificial Intelligence, Nankai University, Tianjin, China; College of Artificial Intelligence, Nankai University, Tianjin, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","940","944","This paper uses gesture recognition technology based on YOLO v3 to successfully identify and detect the seven steps of the seven-step hand-washing method prescribed by the hospital. The paper also uses TensorRT to improve the trained seven-step hand-washing gestures recognition model and increase its detection frame rate on video files. Finally, the model runs on NVIDIA’s Jetson Nano, a smart mobile terminal, which is able to detect the hand-washing gestures in real time with a camera and other equipment for convenience.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9728292","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9728292","gesture recognition;YOLO v3;TensorRT;deep learning","Technological innovation;Gesture recognition;Speech recognition;Object detection;Life estimation;Streaming media;Data models","cameras;cleaning;computerised monitoring;gesture recognition;image sensors;intelligent sensors;learning (artificial intelligence)","intelligent hand-washing monitoring platform;hand-washing gestures recognition model;YOLO v3;seven-step hand-washing method;TensorRT;seven-step hand-washing gesture recognition model;frame rate video file detection;NVIDIA's Jetson Nano;smart mobile terminal;camera","","1","","15","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"Real-Time Video Anonymization in Smart City Intersections","A. Angus; Z. Duan; G. Zussman; Z. Kostić","Dept. of Electrical Engineering, Columbia University, New York City; Dept. of Electrical Engineering, Columbia University, New York City; Dept. of Electrical Engineering, Columbia University, New York City; Dept. of Electrical Engineering, Columbia University, New York City","2022 IEEE 19th International Conference on Mobile Ad Hoc and Smart Systems (MASS)","19 Dec 2022","2022","","","514","522","Video cameras in smart cities can be used to provide data to improve pedestrian safety and traffic management. Video recordings inherently violate privacy, and technological solutions need to be found to preserve it. Smart city applications deployed on top of the COSMOS research testbed in New York City are envisioned to be privacy friendly. This contribution presents one approach to privacy preservation - a video anonymization pipeline implemented in the form of blurring of pedestrian faces and vehicle license plates. The pipeline utilizes customized deep-learning models based on YOLOv4 for detection of privacy-sensitive objects in street-level video recordings. To achieve real time inference, the pipeline includes speed improvements via NVIDIA TensorRT optimization. When applied to the video dataset acquired at an intersection within the COSMOS testbed in New York City, the proposed method anonymizes visible faces and license plates with recall of up to 99% and inference speed faster than 100 frames per second. The results of a comprehensive evaluation study are presented. A selection of anonymized videos can be accessed via the COSMOS testbed portal.","2155-6814","978-1-6654-7180-0","10.1109/MASS56207.2022.00078","NSF(grant numbers:CNS-1827923,CNS-1910757,OAC-2029295,CNS-2038984,CNS-2148128); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9973552","Smart City;Sensors;Video Surveillance;Privacy Protection;Object Detection;Deep Learning;TensorRT","Data privacy;Privacy;Smart cities;Pipelines;Streaming media;Real-time systems;Information filtering","data privacy;learning (artificial intelligence);object detection;pedestrians;portals;road safety;traffic engineering computing;video cameras;video signal processing","99%;anonymized videos;COSMOS research;deep-learning models;inference speed;method anonymizes visible faces;New York City;NVIDIA TensorRT optimization;pedestrian faces;pedestrian safety;privacy friendly;privacy preservation;privacy-sensitive objects;smart cities;smart city applications;smart City intersections;speed improvements;street-level video recordings;technological solutions;time inference;time video anonymization;traffic management;vehicle license plates;video anonymization pipeline;video cameras;video dataset;YOLOv4","","","","48","IEEE","19 Dec 2022","","","IEEE","IEEE Conferences"
"Implementation of Mask-wearing Detection for Embedded Platform","S. Yu; C. Lv; Y. Xu; Y. Liang","College of Arts and Sciences, Beijing Institute of Fashion Technology, Beijing, China; College of Arts and Sciences, Beijing Institute of Fashion Technology, Beijing, China; College of Arts and Sciences, Beijing Institute of Fashion Technology, Beijing, China; School of Fashion, Beijing Institute of Fashion Technology, Beijing, China","2022 IEEE 10th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","3 Aug 2022","2022","10","","2481","2486","With the vigorous development of target detection technology, many mask-wearing detection algorithms have come into being, but the detection task still has the problem of being difficult to detect at the embedded platform. In this regard, a mask-wear detection model based on improved YOLOv5 is proposed, and the implementation of the embedded platform of detection was studied. Firstly, the implementation of the mask-wearing embedding terminal was analyzed; then the mainstream embedded platform was compared, and the NVIDIA Jetson Nano was selected for the embedding terminal deployment; finally, the tensorRT acceleration principle was introduced and the acceleration experiment was completed. The results show that the mAP of the post-feed algorithm reaches 99.6% and the FPS is 30, and the demand for mask-wear detection can be met on the embedded platform NVIDIA Jetson Nano, which has reference significance for practical applications.","2693-2865","978-1-6654-2207-9","10.1109/ITAIC54216.2022.9836870","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9836870","mask-wearing detection;YOLOv5;NVIDIA Jetson nano;TensorRT","Conferences;Object detection;Nanoscale devices;Detection algorithms;Task analysis;Information technology;Artificial intelligence","embedded systems;medical image processing;microprocessor chips;object detection","target detection technology;mask-wearing detection algorithms;mask-wear detection model;mask-wearing embedding terminal;embedding terminal deployment;NVIDIA Jetson Nano embedded platform;tensorRT acceleration principle;post-feed algorithm","","","","15","IEEE","3 Aug 2022","","","IEEE","IEEE Conferences"
"Research and examination on implementation of super-resolution models using deep learning with INT8 precision","S. Hirose; N. Wada; J. Katto; H. Sun","School of Fundamental Science and Engineering Faculty of Science and Engineering, Waseda University Shinjuku, Tokyo, Japan; School of Fundamental Science and Engineering Faculty of Science and Engineering, Waseda University Shinjuku, Tokyo, Japan; School of Fundamental Science and Engineering Faculty of Science and Engineering, Waseda University Shinjuku, Tokyo, Japan; Presto, JST, Saitama, Japan","2022 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)","1 Mar 2022","2022","","","133","137","Fixed-point arithmetic is a technique for treating weights and intermediate values as integers in deep learning. Since deep learning models generally store each weight as a 32-bit floating-point value, storing by 8-bit integers can reduce the size of the model. In addition, memory usage can be reduced, and inference can be much faster by hardware acceleration when special hardware for int8 inference is provided. On the other hand, when inferences are carried out by fixed-point weights, accuracy of the model is reduced due to loss of dynamic range of the weights and intermediate layer values. For this reason, inference frameworks such as TensorRT and TensorFlow Lite, provide a function called ""calibration"" to suppress the deterioration of the accuracy caused by quantization by measuring the distribution of input data and numerical values in the intermediate layer when quantization is performed. In this paper, after quantizing a pre-trained model that performs super-resolution, speed and accuracy are measured using TensorRT. As a result, the trade-off between the runtime and the accuracy is confirmed. The effect of calibration is also confirmed.","","978-1-6654-5818-4","10.1109/ICAIIC54071.2022.9722655","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9722655","Tensor RT;Quantization;Super resolution;Real-time inference","Deep learning;Tensors;Quantization (signal);Runtime;Superresolution;Dynamic range;Calibration","computational complexity;deep learning (artificial intelligence);fixed point arithmetic;floating point arithmetic;image resolution","INT8 precision;fixed-point arithmetic;intermediate values;deep learning models;memory usage;hardware acceleration;special hardware;INT8 inference;fixed-point weights;intermediate layer values;inference frameworks;quantization;super-resolution models;floating-point value;TensorRT;TensorFlow Lite;input data distribution","","1","","12","IEEE","1 Mar 2022","","","IEEE","IEEE Conferences"
"Deep learning application based on embedded GPU","J. Xu; B. Wang; J. Li; C. Hu; J. Pan","Department of Automatic Test and Control, Harbin Institute of Technology, Harbin, China; Department of Automatic Test and Control, Harbin Institute of Technology, Harbin, China; Department of Automatic Test and Control, Harbin Institute of Technology, Harbin, China; School of Electronic Engineering and Automation, Guilin University of Electronic Technology, Guilin, China; Fujian Provincial Key Lab of Big Data Mining and Applications, Fujian University of Technology, Fuzhou, China","2017 First International Conference on Electronics Instrumentation & Information Systems (EIIS)","22 Feb 2018","2017","","","1","4","Since the Deep Neural Networks(DNNs) in the machine vision challenge has made outstanding achievements, a variety of excellent models are used in a variety of scenes. Embedded applications based on deep learning have become the key to becoming a product. This paper introduces the task of image recognition based on deep learning. And starts with the actual demand, it introduces the application of depth learning in commercial video analysis and accelerates the existing algorithm model. The results show that the optimized algorithm can realize the deep learning for embedded applications.","","978-1-5386-0843-2","10.1109/EIIS.2017.8298723","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8298723","Deep Learning;Embedded System;GPU;GoogLeNet;Caffe;TensorRT","Convolution;Feature extraction;Machine learning;Graphics processing units;Image recognition;Biological neural networks","computer vision;embedded systems;graphics processing units;image recognition;learning (artificial intelligence);neural nets","Deep learning application;embedded GPU;machine vision challenge;embedded applications;depth learning;Deep Neural Networks;DNNs;image recognition","","8","","15","IEEE","22 Feb 2018","","","IEEE","IEEE Conferences"
"Application of Deep Learning Model Inference with Batch Size Adjustment","S. Oh; J. Moon; S. Kum","Information Media Research Center, Korea Electronics Technology Institute, Seoul, Republic of Korea; Information Media Research Center, Korea Electronics Technology Institute, Seoul, Republic of Korea; Information Media Research Center, Korea Electronics Technology Institute, Seoul, Republic of Korea","2022 13th International Conference on Information and Communication Technology Convergence (ICTC)","25 Nov 2022","2022","","","2146","2148","The demand for deep learning model applications by users on edge devices is increasing. If the AI model inferences are directly operated on the edge device rather than the cloud or server, it allows customers to proceed additional tasks more conveniently. In order for a deep learning model to be used more efficiently in an embedded environment, studies such as model weight reduction and network compression have been conducted. In addition, studies related to GPU optimization that maximize the usage of limited GPU resources are also attracting a lot of attention. In this paper, in order to maximize the GPU allocation of edge resources, we conducted some experiments to infer images of multiple frames at once, not just one input. For the experiments, the YOLOv4 model was converted into several models with different input layer shapes, and the inference was performed by adjusting the input batch size. NVIDIA Jetson AGX Xavier was used as the edge device, and all other models with different batch sizes obtained the same inference results. As the number of batches increases as much as possible according to the resource limit, the inference speed increases.","2162-1241","978-1-6654-9939-2","10.1109/ICTC55196.2022.9952641","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9952641","Deep learning;Edge Computing;Batch Inference;Model Serving;TensorRT","Deep learning;Performance evaluation;Shape;Image edge detection;Computational modeling;Graphics processing units;Time measurement","deep learning (artificial intelligence);graphics processing units;inference mechanisms;learning (artificial intelligence);optimisation;resource allocation","AI model inferences;deep learning model applications;deep learning model inference;edge device;edge resources;embedded environment;GPU allocation;GPU optimization;GPU resources;input batch size;input layer;model weight reduction;network compression;YOLOv4 model","","","","12","IEEE","25 Nov 2022","","","IEEE","IEEE Conferences"
"Real-Time Semantic Segmentation on Edge Devices with Nvidia Jetson AGX Xavier","H. -H. Nguyen; D. N. -N. Tran; J. W. Jeon","Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Korea; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Korea; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, Korea","2022 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)","28 Nov 2022","2022","","","1","4","Semantic segmentation is the problem of classifying every pixel in the image from a predefined set of classes or categories. It is a vital part of an autonomous vehicle vision system to perform traffic scene parsing. In such application, having accurate and low-latency performance is necessary but also challenging due to the task's high computational complexity. This study presents our implementation and performance evaluation of recent state-of-the-art semantic segmentation methods aiming to operate in high processing speed while maintaining high accuracy. Our experimental results on Cityscapes dataset and on Nvidia Jetson AGX Xavier mobile embedded platform demonstrate that it is possible to achieve that goal on resource-limited devices.","","978-1-6654-6434-5","10.1109/ICCE-Asia57006.2022.9954835","National Research Foundation of Korea; MSIT(grant numbers:2020R1A2C3011286); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954835","semantic segmentation;real-time;edge device;TensorRT","Performance evaluation;Quantization (signal);Semantic segmentation;Machine vision;Semantics;Real-time systems;Task analysis","image segmentation;road traffic","autonomous vehicle vision system;Cityscapes dataset;edge devices;low-latency performance;Nvidia Jetson AGX Xavier mobile embedded platform;performance evaluation;processing speed;real-time semantic segmentation;resource-limited devices;semantic segmentation methods;traffic scene parsing","","","","21","IEEE","28 Nov 2022","","","IEEE","IEEE Conferences"
"Artificial Intelligence for Smart Cities: Comparing Latency in Edge and Cloud Computing","B. Naets; W. Raes; R. Devillé; C. Middag; N. Stevens; B. Minnaert","Odisee University College of Applied Sciences, Ghent, Belgium; Dep. of Electrical Eng., DRAMCO Research Group, ESAT, KU Leuven, Ghent, Belgium; Knowledge centre AI, Erasmus Brussels University college, Brussels, Belgium; Knowledge centre AI, Erasmus Brussels University college, Brussels, Belgium; Dep. of Electrical Eng., DRAMCO Research Group, ESAT, KU Leuven, Ghent, Belgium; Odisee University College of Applied Sciences, Ghent, Belgium","2022 IEEE European Technology and Engineering Management Summit (E-TEMS)","22 Nov 2022","2022","","","55","59","A smart city collects and uses data to streamline and improve multiple facets of city life. This approach is becoming an important strategy to keep cities a desirable place to live in while keeping them attractive and beneficial for business. The applications which constitute a smart city range from traffic to waste management, but are all dependent on data acquisition and data processing. This work focuses on the latency difference between edge and cloud computing for smart cities, which is illustrated by a real-life example: a dash cam in a car to monitor traffic in real time. Two scenarios are compared: the first one uses a single board computer on the edge to process the data and to realise inference, whereas in the second scenario, the analysis is done in the cloud, but the result is still returned to the edge. The results show the benefits and disadvantages of edge and cloud computing for a smart city environment for which latency is an important parameter, in particular when time-sensitive applications are considered such as on-board capturing of traffic situations.","","978-1-6654-1100-4","10.1109/E-TEMS53558.2022.9944509","Flanders Innovation & Entrepreneurship VLAIO(grant numbers:HBC.2019.2638); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9944509","smart cities;artificial intelligence;cloud;cloud computing;jetson nano;edge computing;tensorrt","Waste management;Cloud computing;Smart cities;Data acquisition;Europe;Data processing;Real-time systems","cloud computing;data acquisition;inference mechanisms;mobile computing;mobile handsets;road traffic;smart cities","city life;cloud computing;data acquisition;single board computer;smart city environment;smart city range","","","","14","IEEE","22 Nov 2022","","","IEEE","IEEE Conferences"
"Packaging Defect Detection System Based on Machine Vision and Deep Learning","J. Sa; Z. Li; Q. Yang; X. Chen","School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Information Engineering, Wuhan University of Technology, Wuhan, China; School of Information Engineering, Wuhan University of Technology, Wuhan, China","2020 5th International Conference on Computer and Communication Systems (ICCCS)","16 Jun 2020","2020","","","404","408","Detecting packaging defection with high accuracy and efficiency is of great significance in product quality. We use OpenCV to preprocess images which come from damaged package according to characteristics of the image. The processed data is combined with deep learning and based on neural network model ResNet. Meanwhile the processed image data is sent to a convolutional neural network (CNN) for model training. We establish a detection system for product packaging. The detection system provides a solution for automatic detection of package defection, which realizes rapid and accurate detection of product packaging.","","978-1-7281-6136-5","10.1109/ICCCS49078.2020.9118413","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9118413","packaging defection detection;image preprocessing;convolution neural network;TensorRT;ResNet;Keras","Packaging;Training;Inspection;Machine vision;Feature extraction;Convolution;Convolutional neural networks","computer vision;convolutional neural nets;learning (artificial intelligence);packaging","defect detection system;machine vision;deep learning;product quality;neural network model ResNet;convolutional neural network;product packaging;package defection;image data processed;OpenCV;CNN","","","","7","IEEE","16 Jun 2020","","","IEEE","IEEE Conferences"
"Performance Evaluation of Deep Learning Models on Embedded Platform for Edge AI-Based Real time Traffic Tracking and Detecting Applications","H. T. Minh; L. Mai; T. V. Minh","Schools of Electrical Engineering, International University, Ho Chi Minh City, Vietnam; Schools of Electrical Engineering, International University, Ho Chi Minh City, Vietnam; Schools of Electrical Engineering, International University, Ho Chi Minh City, Vietnam","2021 15th International Conference on Advanced Computing and Applications (ACOMP)","10 Jan 2022","2021","","","128","135","Edge Artificial Intelligence based traffic tracking and detecting sensors are very essential for smart cities, especially for smart transportation applications. These sensors are not only used to collect large amount of traffic data, but also reduce the bandwidth of the communication network to transfer and reduce the workload to process huge data on the cloud or server side. It is very necessary to process, store, and extract useful data at the edge of the Internet before transferring the data to central server which can be called Artificial Intelligence on The Edge. This research aims at studying, implementing and evaluating machine learning models which are suitable for running on limited computing embedded computers. Computer vision and real-time object detection techniques are applied on Nvidia Jetson Nano Embedded Computer to build an Edge-AI based traffic tracking and detecting sensor, two popular models (MobileNet-SSD and YOLOv4) have been studied and implemented to compare the performance on vehicle counting and license plate detection applications. There has been a new propose method to apply TensorRT engine for these two models to increase the processing speed. The data source used in this project is manually collected at actual traffic routes and parking lots in Vietnam with more than 11700 images of Vietnamese vehicles and license plates then trained on Google Colab. The performance evaluation results show that both models have high accuracy in real-time license plate detection and vehicle counting application when implemented on the Edge computer Jetson Nano with the mAPs of both model are higher than 90 percent during training session. The MobileNet-SSD model has good speed (40 FPS) which is very much faster than some previous works (25 FPS), this model is suitable for real-time applications. The YOLOv4 model, after being optimized by TensorRT engine, has a better speed (7.2 FPS) than the original version (1.7 FPS), although the YOLOv4 model has low speed than MobileNet-SSD model but can detect smaller size, this model is suitable for some applications that need to detect complicated objects with small sizes.","2688-0202","978-1-6654-0639-0","10.1109/ACOMP53746.2021.00024","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9668204","Real-time Object Detection;Edge-AI;Jetson Nano;License Plate;MobileNet-SSD;YOLOv4;TensorRT","Computers;Performance evaluation;Image edge detection;Computational modeling;Object detection;Licenses;Real-time systems","artificial intelligence;computer vision;deep learning (artificial intelligence);edge detection;embedded systems;feature extraction;object detection;object recognition;real-time systems;traffic engineering computing","deep learning models;Embedded platform;Edge AI-based real time traffic tracking;Edge Artificial Intelligence based traffic tracking;smart cities;smart transportation applications;traffic data;machine learning models;real-time object detection techniques;Nvidia Jetson NanoEmbedded Computer;Edge-AI based traffic tracking;license plate detection applications;data source;actual traffic routes;performance evaluation results;real-time license plate detection;vehicle counting application;Edge computer Jetson Nano;MobileNet-SSD model;real-time applications;YOLOv4 model;limited computing embedded computers","","1","","29","IEEE","10 Jan 2022","","","IEEE","IEEE Conferences"
"Using Siamese Networks to Detect Shading on the Edge of Solar Farms","S. Shapsough; I. Zualkernan; R. Dhaouadi; A. R. Sajun","Department of Computer Science and Engineering, American University of Sharjah, Sharjah, UAE; Department of Computer Science and Engineering, American University of Sharjah, Sharjah, UAE; Department of Electrical Engineering, American University of Sharjah, Sharjah, UAE; Department of Computer Science and Engineering, American University of Sharjah","2020 7th International Conference on Internet of Things: Systems, Management and Security (IOTSMS)","2 Feb 2021","2020","","","1","8","Solar power is one of the most promising sources of green power for future cities. However, real-time anomaly detection remains a challenge. Internet of Things (IoT) is an effective platform for real-time monitoring of large-scale solar farms. Using low-cost edge devices such as the Raspberry Pi (RPI), it is possible to not only read power and irradiance values from in-situ sensors, but to also apply machine learning and deep learning algorithms for real-time analysis and for detecting anomalous behaviors. This paper presents the design and implementation of an edge analytics application that uses RPI as an edge device. The Isolation Forest algorithm was first used to detect shading anomalies. A Siamese neural network was then trained to create a latent-space mapping. An anomaly detection model based on the latent space and a neural network and kNN was developed. These models could detect shading anomalies with an F1-Score of 0.94. Embedded variants of the model based on TensorFlow Lite and TensorRT were evaluated to service a large number of solar panels at 1Hz. The results are that a single RPI could do parallel anomaly detection of 512 solar panels at 1 Hz with 0% failures. The TensorRT variant consumed more resources than the TensorFlow Lite implementation, but the maximum CPU utilization remained below 75%.","","978-0-7381-2460-5","10.1109/IOTSMS52051.2020.9340189","American University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9340189","Internet of things;solar power;Siamese networks;shading;TensorFlow Lite;TensorRT;Isolation Forest","Economic indicators;Image edge detection;Real-time systems;Sensors;Solar panels;Internet of Things;Anomaly detection","computer network security;Internet of Things;learning (artificial intelligence);neural nets;power engineering computing;real-time systems;solar power;solar power stations","Siamese networks;detect shading;solar power;promising sources;green power;future cities;real-time anomaly detection;IoT;real-time monitoring;large-scale solar farms;low-cost edge devices;Raspberry Pi;irradiance values;in-situ sensors;machine learning;deep learning algorithms;real-time analysis;anomalous behaviors;edge analytics application;edge device;Isolation Forest algorithm;shading anomalies;Siamese neural network;latent-space mapping;anomaly detection model;latent space;F1-Score;solar panels;TensorFlow Lite implementation;frequency 1.0 Hz","","","","51","IEEE","2 Feb 2021","","","IEEE","IEEE Conferences"
"Lane Transformer: A High-Efficiency Trajectory Prediction Model","Z. Wang; J. Guo; Z. Hu; H. Zhang; J. Zhang; J. Pu","Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, Shanghai, China; Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, Shanghai, China; Autonomous Driving General Algorithm Department, Mogo Auto Intelligence and Telematics Information Technology Company Ltd., Beijing, China; Autonomous Driving General Algorithm Department, Mogo Auto Intelligence and Telematics Information Technology Company Ltd., Beijing, China; School of Computer Science, Fudan University, Shanghai, China; Institute of Science and Technology for Brain-Inspired Intelligence, Fudan University, Shanghai, China","IEEE Open Journal of Intelligent Transportation Systems","16 Jan 2023","2023","4","","2","13","Trajectory prediction is a crucial step in the pipeline for autonomous driving because it not only improves the planning of future routes, but also ensures vehicle safety. On the basis of deep neural networks, numerous trajectory prediction models have been proposed and have already achieved high performance on public datasets due to the well-designed model structure and complex optimization procedure. However, the majority of these methods overlook the fact that vehicles’ limited computing resources can be utilized for online real-time inference. We proposed a Lane Transformer to achieve high accuracy and efficiency in trajectory prediction to tackle this problem. On the one hand, inspired by the well-known transformer, we use attention blocks to replace the commonly used Graph Convolution Network (GCN) in trajectory prediction models, thereby drastically reducing the time cost while maintaining the accuracy. In contrast, we construct our prediction model to be compatible with TensorRT, allowing it to be further optimized and easily transformed into a deployment-friendly form of TensorRT. Experiments demonstrate that our model outperforms the baseline LaneGCN model in quantitative prediction accuracy on the Argoverse dataset by a factor of  $10\times $  to  $25\times $ . Our  $7ms$  inference time is the fastest among all open source methods currently available. Our code is publicly available at: https://github.com/mmdzb/Lane-Transformer.","2687-7813","","10.1109/OJITS.2023.3233952","Shanghai Municipal Science and Technology Major Project(grant numbers:2018SHZDZX01); ZJ Lab; Shanghai Center for Brain Science and Brain-Inspired Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10005191","Trajectory prediction;transformer;multi-head attention;TensorRT","Trajectory;Transformers;Predictive models;Roads;Feature extraction;Task analysis;Convolution","","","","","","43","CCBY","3 Jan 2023","","","IEEE","IEEE Journals"
"3.3 Kunlun: A 14nm High-Performance AI Processor for Diversified Workloads","J. Ouyang; X. Du; Y. Ma; J. Liu","Baidu, Beijing, China; Baidu, Beijing, China; Baidu, Beijing, China; Baidu, Beijing, China","2021 IEEE International Solid- State Circuits Conference (ISSCC)","3 Mar 2021","2021","64","","50","51","In order to be able to handle a wide range of AI applications, such as for speech, image, language and autonomous driving, it is necessary that an AI accelerator be flexible enough to handle diversified workloads. Baidu Kunlun, an AI chip designed in-house by Baidu, achieves this capability with high programmability, flexibility and performance. Baidu Kunlun was inspired by the XPU architecture [1]. The chip is implemented in Samsung 14nm process technology. Its peak performance is 230TOPS@INT8 at 900MHz and up to 281TOPS@INT8 at 1.1GHz boost frequency. The memory bandwidth is 512GB/s and the peak power is 160W. Baidu Kunlun achieves good performance across various types of workloads. With 900MHz base frequency, the latencies of BERT, ResNet50, YOLOv3 are 1.7x, 1.2x and 2x less than an Nvidia T4 GPU, respectively, with optimizations from TensorRT. Recently, Baidu Kunlun has been deployed in data centers in Baidu to serve many applications. It achieves 1.5-to-3x better performance for several models within the search engine vs. the Nvidia T4.","2376-8606","978-1-7281-9549-0","10.1109/ISSCC42613.2021.9366056","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9366056","","Web and internet services;AI accelerators;Graphics processing units;Search engines;Solid state circuits;Integrated circuit modeling;Optimization","AI chips;computational complexity;graphics processing units;integrated circuit design;microprocessor chips;optimisation","diversified workloads;AI applications;autonomous driving;AI accelerator;AI chip;Baidu Kunlun;Samsung process technology;high-performance AI processor;XPU architecture;BERT latency;ResNet50 latency;YOLOv3 latency;Nvidia T4 GPU;TensorRT optimizations;Baidu data centers;power 160.0 W;size 14.0 nm;frequency 900.0 MHz;frequency 1.1 GHz;byte rate 512.0 GByte/s","","2","","5","IEEE","3 Mar 2021","","","IEEE","IEEE Conferences"
"Real-time retinal layer segmentation of adaptive optics optical coherence tomography angiography with deep learning","Y. Jian; S. Borkovkina; W. Japongsori; A. Camino; M. V. Sarunic","Casey Eye Institute, Oregon Health & Science University, Portland, USA; Department of Engineering Science, Simon Fraser University, Burnaby, Canada; Department of Engineering Science, Simon Fraser University, Burnaby, Canada; Casey Eye Institute, Oregon Health & Science University, Portland, USA; Department of Engineering Science, Simon Fraser University, Burnaby, Canada","2020 IEEE Photonics Conference (IPC)","13 Nov 2020","2020","","","1","2","Real time rendering of en face optical coherence tomography (OCT) and OCT-angiography (OCTA) of arbitrary retinal layers in ophthalmic imaging sessions can be used to increase the yield rate of high-quality acquisitions, provide real-time feedback during image-guided surgeries and compensate aberrations in sensorless adaptive optics (AO) OCT and OCTA. However, real-time en face visualizations rely critically on the accurate segmentation of retinal layers in the three-dimensional OCT volumes. Here, we demonstrate a compact deep-learning architecture that segmented batches of OCT B-scans and produced the corresponding OCT and OCTA projections within only 41 ms. The short latency was possible due to a low complexity neural network structure, CNN compression using TensorRT, and the use of Tensor Cores on GPU hardware to accelerate the computation of convolutions. Inferencing of the original U-net was accelerated by 21 times without reducing the accuracy. To the best our knowledge, our work is the first demonstration of an ophthalmic imager with embedded artificial intelligence (AI) providing real-time feedback.","2575-274X","978-1-7281-5891-4","10.1109/IPC47351.2020.9252343","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9252343","ophthalmic imaging;optical coherence tomography;angiography;deep learning;segmentation;real time","Retina;Real-time systems;Optical coherence tomography;Faces;Angiography;Computer architecture;Image segmentation","aberrations;adaptive optics;biomedical optical imaging;eye;image coding;image segmentation;learning (artificial intelligence);medical image processing;optical tomography;rendering (computer graphics);surgery","OCT B-scans;OCTA;ophthalmic imager;real-time feedback;adaptive optics optical coherence tomography angiography;deep learning;real time rendering;arbitrary retinal layers;ophthalmic imaging sessions;image-guided surgeries;sensorless adaptive optics OCT;face visualizations;three-dimensional OCT volumes;real-time retinal layer segmentation;aberrations;CNN compression;TensorRT;Tensor Cores;embedded artificial intelligence;time 41.0 ms","","1","","10","IEEE","13 Nov 2020","","","IEEE","IEEE Conferences"
"Quantized YOLOv5x6 for Traffic Object Detection","H. -J. Jeon; J. W. Jeon","Department of Electrical and Computer Engineering, Sungkyunkwan University; Department of Electrical and Computer Engineering, Sungkyunkwan University","2022 IEEE International Conference on Consumer Electronics-Asia (ICCE-Asia)","28 Nov 2022","2022","","","1","3","There are many powerful object detectors proposed up to this day. Those models are mostly targeted to general purpose object detection. However, this is unnecessary in the realm of autonomous driving. In fact, the complexity of the models seriously requires huge computational power from workstation-scale GPUs, rendering the models inapplicable in embedded environments. We attempt to address this issue by performing quantization using TensorRT. Choosing the YOLOv5x6 as the object detection model, the model is trained and tested on the KITTI 2D Object Dataset. The model is then quantized to half-precision floating point. Results show that the model can achieve faster speed when the trained model parameters are quantized, while having the detection mAP similar to the non-quantized version.","","978-1-6654-6434-5","10.1109/ICCE-Asia57006.2022.9954686","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9954686","Quantized YOLOv5x6;object detection;autonomous driving","Quantization (signal);Computational modeling;Object detection;Detectors;Rendering (computer graphics);Complexity theory;Autonomous vehicles","floating point arithmetic;object detection","autonomous driving;embedded environments;general purpose object detection;half-precision floating point;huge computational power;KITTI 2D Object Dataset;object detection model;powerful object detectors;quantization;quantized YOLOv5x;TensorRT;traffic object detection;trained model parameters;workstation-scale GPUs;YOLOv5x6","","","","17","IEEE","28 Nov 2022","","","IEEE","IEEE Conferences"
"Research on Intelligent Target Detection and Coder-decoder Technology Based on Embedded Platform","X. Zhao; X. Zhang; X. Cheng; F. Chen; Z. Zhou; T. Xu","Unmanned System Research Institute, Northwestern Polytechnical University, Xi'an, China; Unmanned System Research Institute, Northwestern Polytechnical University, Xi'an, China; Institute of 365, Northwestern Polytechnical University, Xi'an, China; Flight Control Department, AVIC Xi'an Flight Automatic Control Research Institute, Xi'an, China; Xi'an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi'an, China; Unmanned System Research Institute, Northwestern Polytechnical University, Xi'an, China","2019 IEEE International Conference on Unmanned Systems and Artificial Intelligence (ICUSAI)","25 Jun 2020","2019","","","210","215","In order to meet the embedded application requirements of machine learning algorithm, the intelligent target detection and recognition algorithm based on convolutional neural network and corresponding optimal process are studied. Detailed network structure analysis and network performance analysis are carried out. Based on GPU embedded platform, TensorRT technology is used to accelerate the embedded application of intelligent target detection and recognition algorithm, including fp16 and int8 inference modes. Satisfactory verification results are achieved on embedded platform. In addition, an integrated system of real-time machine learning and H.265 encoding and decoding technology is realized. Firstly, the compressed image data sent by the camera is received by embedded platform and decoded in real time in H.265 format. Then the real-time intelligent target detection and recognition algorithm basing on TensorRT technology is done for RGB data obtained by hardware decoding process. Finally, the data is compressed in H.265 format, and subsequently storage and data transmission are carried out. The experimental results show that TensorRT technology can improve the inference speed of neural network in embedded platform. The network structure optimized by TensorRT technology can achieve three times the speed increase, with limited accuracy loss. Hardware coding and decoding of H.265 can also cause corresponding delay to program inevitably.","","978-1-7281-5859-4","10.1109/ICUSAI47366.2019.9124858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9124858","Convolutional Neural Network;target detection and recognition;network optimization;coder-decoder system;embedded platform","","convolutional neural nets;data compression;decoding;embedded systems;learning (artificial intelligence);object detection;object recognition;optimisation;video cameras;video coding","TensorRT technology;embedded platform;network structure analysis;coder-decoder technology;embedded application requirements;machine learning algorithm;convolutional neural network;network performance analysis;real-time machine learning;real-time intelligent target detection;recognition algorithm;hardware decoding process;int8 inference modes;fp16 inference modes;camera;H.265 format;image data compression;RGB data;data transmission;hardware coding;H.265 encoding technology;H.265 decoding technology","","3","","30","IEEE","25 Jun 2020","","","IEEE","IEEE Conferences"
"Cloud Versus Edge Deployment Strategies of Real-Time Face Recognition Inference","A. Koubaa; A. Ammar; A. Kanhouch; Y. AlHabashi","Department of Computer Science, and the Robotics, and Internet-of-Things Lab, Prince Sultan University, Riyadh, Saudi Arabia; ROITU LAB, Prince Sultan University, Riyadh, Saudi Arabia; ROITU LAB, Prince Sultan University, Riyadh, Saudi Arabia; ROITU LAB, Prince Sultan University, Riyadh, Saudi Arabia","IEEE Transactions on Network Science and Engineering","12 Jan 2022","2022","9","1","143","160","Choosing the appropriate deployment strategy for any Deep Learning (DL) project in a production environment has always been the most challenging problem for industrial practitioners. There are several conflicting constraints and controversial approaches when it comes to deployment. Among these problems, the deployment on cloud versus the deployment on edge represents a common dilemma. In a nutshell, each approach provides benefits where the other would have limitations. This paper presents a real-world case study on deploying a face recognition application using MTCNN detector and FaceNet recognizer. We report the challenges faced to decide on the best deployment strategy. We propose three inference architectures for the deployment, including cloud-based, edge-based, and hybrid. Furthermore, we evaluate the performance of face recognition inference on different cloud-based and edge-based GPU platforms. We consider different models of Jetson boards for the edge (Nano, TX2, Xavier NX, Xavier AGX) and various GPUs for the cloud (GTX 1080, RTX 2080Ti, RTX 2070, and RTX 8000). We also investigate the effect of deep learning model optimization using TensorRT and TFLite compared to a standard Tensorflow GPU model, and the effect of input resolution. We provide a benchmarking study for all these devices in terms of frames per second, execution times, energy and memory usages. After conducting a total of 294 experiments, the results demonstrate that the TensorRT optimization provides the fastest execution on all cloud and edge devices, at the expense of significantly larger energy consumption (up to +40% and +35% for edge and cloud devices, respectively, compared to Tensorflow). Whereas TFLite is the most efficient framework in terms of memory and power consumption, while providing significantly less (-4% to -62%) processing acceleration than TensorRT. Practitioners Note: The study reported in this paper presents the real-challenges that we faced during our development and deployment of a face-recognition application both on the edge and on the cloud, and the solutions we have developed to solve these problems. The code, results, and interactive analytic dashboards of this paper will be put public upon publication.","2327-4697","","10.1109/TNSE.2021.3055835","Prince Sultan University(grant numbers:SEED-2020-05); Prince Sultan University(grant numbers:SEED-CCIS-2020-05); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9350171","Cloud inference;computation offloading;edge inference;face recognition;FaceNet;jetson boards;production environment","Face recognition;Image edge detection;Deep learning;Cloud computing;Performance evaluation;Real-time systems;Visual analytics","cloud computing;convolutional neural nets;deep learning (artificial intelligence);energy consumption;face recognition;graphics processing units;optimisation","cloud devices;face-recognition application;edge deployment strategies;real-time face recognition inference;appropriate deployment strategy;face recognition application;deep learning model optimization;standard Tensorflow GPU model;edge devices;production environment;cloud-based GPU platform;edge-based GPU platform;TFLite;TensorRT optimization;energy consumption","","10","","73","IEEE","8 Feb 2021","","","IEEE","IEEE Journals"
"PerfNetRT: Platform-Aware Performance Modeling for Optimized Deep Neural Networks","Y. -C. Liao; C. -C. Wang; C. -H. Tu; M. -C. Kao; W. -Y. Liang; S. -H. Hung","National Taiwan University, Taipei, Taiwan; ADLINK Technology Inc., New Taipei, Taiwan; National Cheng Kung University, Tainan, Taiwan; ADLINK Technology Inc., New Taipei, Taiwan; ADLINK Technology Inc., New Taipei, Taiwan; National Taiwan University, Taipei, Taiwan","2020 International Computer Symposium (ICS)","23 Feb 2021","2020","","","153","158","As deep learning techniques based on artificial neural networks have been widely applied to diverse application domains, the delivered performance of such deep learning models on the target hardware platforms should be taken into account during the system design process in order to meet the application-specific timing requirements. Specifically, there are neural network optimization frameworks available for boosting the execution efficiency of a trained model on the vendor-specific hardware platforms, e.g., OpenVINO for Intel hardware and TensorRT for NVIDIA GPUs, and it is important that system designers have access to the estimated performance of the optimized models running on the specific hardware so as to make better design decisions. In this work, we have developed PerfNetRT to facilitate the design making process by offering the estimated inference time of a trained model that is optimized for the NVIDIA GPU using TensorRT. Our preliminary results show that PerfNetRT is able to produce accurate estimates of the inference time for the popular models, including LeNet, AlexNet and VGG16, which are optimized with TensorRT running on NVIDIA GTX 1080Ti.","","978-1-7281-9255-0","10.1109/ICS51289.2020.00039","Ministry of Science and Technology; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359013","machine learning;benchmark;performance pre-diction;machine learning accelerators","Deep learning;Graphics processing units;Predictive models;Hardware;Timing;System analysis and design;Optimization","deep learning (artificial intelligence);graphics processing units;software performance evaluation","PerfNetRT;platform-aware performance;optimized deep neural networks;artificial neural networks;diverse application domains;deep learning models;hardware platforms;system design process;application-specific timing requirements;neural network optimization;trained model;Intel hardware;NVIDIA GPUs;system designers;optimized models;design decisions;design making process;inference time;TensorRT running","","1","","15","IEEE","23 Feb 2021","","","IEEE","IEEE Conferences"
"An AIoT System for Bat Species Classification","I. Zualkernan; J. Judas; T. Mahbub; A. Bhagwagar; P. Chand","Computer Science and Engineering, American University of Sharjah, Sharjah, UAE; Conservation Unit, Emirates Nature - WWF, Dubai, UAE; Computer Science and Engineering, American University of Sharjah, Sharjah, UAE; Computer Science and Engineering, American University of Sharjah, Sharjah, UAE; Computer Science and Engineering, American University of Sharjah, Sharjah, UAE","2020 IEEE International Conference on Internet of Things and Intelligence System (IoTaIS)","23 Feb 2021","2021","","","155","160","Bat species are an integral part of our ecosystem and their monitoring can provide important insights into conservation and tracking viruses like Covid-19. Given the difficulty and high cost of manually monitoring bats in their natural habitats, this paper proposes an Artificially Intelligent Internet of Things (AIoT) system that uses audio-based Convolutional Neural Network (CNN) to monitor bat species using their echolocation calls. The system uses Long Range Wide Area Network (LoRaWAN) to send the classified species to an application server in real-time. The paper compared the performance of three different edge devices, Raspberry Pi Model (RPI) 3B+ (RPi), NVIDIA Jetson Nano, and Google Coral and two deep learning frameworks (TensorFlow Lite and TensorRT). Although all edge devices were able to do the real-time inference (<; 0.5 seconds/inference for a 3-second audio segment), Google Coral appears to be the best choice because it was the fastest (0.3917 seconds/audio segment) and required the least resources (maximum %CPU Utilization = 29.2%). However, if cost was a concern then even the RPI was more than adequate for the task.","","978-1-7281-9448-6","10.1109/IoTaIS50849.2021.9359704","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9359704","AIoT;audio classification;bats;convolutional neural network;Covid-19;deep learning;edge computing;internet of things;LoRaWAN;TensorRT;TensorFlow Lite","Performance evaluation;Deep learning;Economic indicators;Computational modeling;Real-time systems;Internet of Things;Monitoring","bioacoustics;biology computing;convolutional neural nets;deep learning (artificial intelligence);Internet;Internet of Things;pattern classification;zoology","Google Coral;AIoT system;bat species classification;convolutional neural network;Long Range Wide Area Network;edge devices;Raspberry Pi Model 3B+;Artificially Intelligent Internet of Things system;NVIDIA Jetson Nano;deep learning frameworks;time 0.3917 s","","6","","54","IEEE","23 Feb 2021","","","IEEE","IEEE Conferences"
"Optimizing Driver Assistance Systems for Real-Time performance on Resource Constrained GPUs","O. A. Ramwala; C. N. Paunwala; M. C. Paunwala","Electronics Engineering, SVNIT, Surat, India; E & C Engineering, SCET, Surat, India; E & C. Engineering, CKPCET, Surat, India","2019 IEEE Conference on Information and Communication Technology","16 Apr 2020","2019","","","1","4","The importance of Advanced Driver Assistance Systems has increased tremendously due to their ability to reduce road fatalities by facilitating drivers for appropriate action selection in circumstances involving high probability of collisions. One of the major factors contributing to accidents on road is driver distraction and drowsiness. A variety of algorithms including several Forward Collision Warning algorithms have been proposed to alleviate the issue to road accidents. These algorithms are promising approaches to mitigate this problem. However, most of these proposals are computationally complex algorithms and require powerful GPUs to perform in real-time. Such GPUs are not only expensive but also have high power consumption. Thus, it is necessary to yield real time performance on resource constrained GPUs like NVIDIA's Jetson TX2 which is not only one of the most eminent GPU-enabled platforms for autonomous systems but also cost effective and power efficient [1]. This paper proposes utilization of pruning of Neural Networks and TensorFlow TensorRT to optimize computationally complex algorithms utilized for Driver Assistance Systems to obtain real-time functionality on TX2 without compromising the accuracy of the system.","","978-1-7281-5398-8","10.1109/CICT48419.2019.9066239","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9066239","Advanced Driver Assistance Systems;Forward Collision Warning;resource constrained GPUs;TensorFlow TensorRT","Real-time systems;Vehicles;Computational modeling;Computer architecture;Neural networks;Optimization;Graphics processing units","computational complexity;driver information systems;probability;road accidents;road safety;road vehicles","resource constrained GPU;autonomous systems;power consumption;road accidents;forward collision warning algorithms;drowsiness;driver distraction;appropriate action selection;road fatalities;advanced driver assistance systems;real-time performance;real-time functionality;computationally complex algorithms","","1","","26","IEEE","16 Apr 2020","","","IEEE","IEEE Conferences"
"Edge Deployment Framework of GuardBot for Optimized Face Mask Recognition With Real-Time Inference Using Deep Learning","S. Manzoor; E. -J. Kim; S. -H. Joo; S. -H. Bae; G. -G. In; K. -J. Joo; J. -H. Choi; T. -Y. Kuc","Creative Algorithms and Sensor Evolution Laboratory, Suwon, South Korea; Department of Electrical and Computer Engineering, College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, College of Information and Communication Engineering, Sungkyunkwan University, Suwon, South Korea","IEEE Access","29 Jul 2022","2022","10","","77898","77921","Deep learning based models on the edge devices have received considerable attention as a promising means to handle a variety of AI applications. However, deploying the deep learning models in the production environment with efficient inference on the edge devices is still a challenging task due to computation and memory constraints. This paper proposes a framework for the service robot named GuardBot powered by Jetson Xavier NX and presents a real-world case study of deploying the optimized face mask recognition application with real-time inference on the edge device. It assists the robot to detect whether people are wearing a mask to guard against COVID-19 and gives a polite voice reminder to wear the mask. Our framework contains dual-stage architecture based on convolutional neural networks with three main modules that employ (1) MTCNN for face detection, (2) our proposed CNN model and seven transfer learning based custom models which are Inception-v3, VGG16, denseNet121, resNet50, NASNetMobile, XceptionNet, MobileNet-v2 for face mask classification, (3) TensorRT for optimization of all the models to speedup inference on the Jetson Xavier NX. Our study carries out several analysis based on the models’ performance in terms of their frames per second, execution time and images per second. It also evaluates the accuracy, precision, recall & F1-score and makes the comparison of all models before and after optimization with a main focus on high throughput and low latency. Finally, the framework is deployed on a mobile robot to perform experiments in both outdoor and multi-floor indoor environments with patrolling and non-patrolling modes. Compared to other state-of-the-art models, our proposed CNN model for face mask recognition based on the classification obtains 94.5%, 95.9% and 94.28% accuracy on training, validation and testing datasets respectively which is better than MobileNet-v2, Xception and InceptionNet-v3 while it achieves highest throughput and lowest latency than all other models after optimization at different precision levels.","2169-3536","","10.1109/ACCESS.2022.3190538","Korea Evaluation Institute of Industrial Technology (KEIT); Ministry of Trade, Industry, and Energy (MOTIE)(grant numbers:1415180816); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9837903","Face detection;face mask classification;voice alert;service robot;deep learning;transfer learning;Jetson Xavier NX;TensorFlow TensorRT;optimization;inference","Face recognition;Computational modeling;Robots;Optimization;Convolutional neural networks;Deep learning;COVID-19","deep learning (artificial intelligence);face recognition;image classification;mobile robots;service robots","edge deployment framework;GuardBot;edge device;AI applications;deep learning models;production environment;memory constraints;service robot;Jetson Xavier NX;optimized face mask recognition application;polite voice reminder;dual-stage architecture;MTCNN;face detection;MobileNet-v2;face mask classification;mobile robot;transfer learning based custom models;Inception-v3;VGG16;denseNet121;resNet50;NASNetMobile;XceptionNet","","1","","122","CCBY","25 Jul 2022","","","IEEE","IEEE Journals"
"ML-Based Edge Application for Detection of Forced Oscillations in Power Grids","S. A. Dorado-Rojas; S. Xu; L. Vanfretti; M. I. I. Ayachi; S. Ahmed","Electrical, Computer, and Systems Engineering Rensselaer Polytechnic Institute, Troy, NY, USA; Electrical, Computer, and Systems Engineering Rensselaer Polytechnic Institute, Troy, NY, USA; Electrical, Computer, and Systems Engineering Rensselaer Polytechnic Institute, Troy, NY, USA; Electrical and Computer Engineering, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia; Electrical and Computer Engineering, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia","2022 IEEE Power & Energy Society General Meeting (PESGM)","27 Oct 2022","2022","","","1","5","This paper presents a Machine Learning (ML) solution deployed in an Internet-of-Things (IoT) edge device for detecting forced oscillations in power grids. We base our proposal on a one-dimensional (1D) and two-dimensional (2D) Convolutional Neural Network (CNN) architecture, trained offline and deployed on an Nvidia Jetson TX2. Our work also shows the advantages of optimizing the CNNs models, after training, using TensorRT, a library for accelerating deep learning inference in real-time. Both real-world and synthetic measurement signals are employed to validate the applicability of the proposed approach.","1944-9933","978-1-6654-0823-3","10.1109/PESGM48719.2022.9917070","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9917070","Convolutional neural networks;forced oscillations;NVIDIA Jetson TX2;TensorRT;real-time detection","Training;Codes;Image edge detection;Two dimensional displays;Signal generators;Power grids;Real-time systems","","","","","","20","IEEE","27 Oct 2022","","","IEEE","IEEE Conferences"
"Research on Defect Rotation Detection of Aircraft Flared Tube Based on Improved YOLOv4","J. Zhang; K. Wang; G. Dai; P. Zhang","State-Owned Wuhu Machinery Factory, WuHu, China; School of Computer Science and Technology, Xidian University, Xi’an, China; State-Owned Wuhu Machinery Factory, WuHu, China; State-Owned Wuhu Machinery Factory, WuHu, China","2022 5th International Conference on Advanced Electronic Materials, Computers and Software Engineering (AEMCSE)","16 Nov 2022","2022","","","64","69","Aircraft Flared Tube (AFT) is an important part of the aircraft system, and its surface defect detection has become a prerequisite for meeting the long-term normal operation of the aircraft. Although the existing deep learning defect detection methods have made great progress, there are still problems such as difficulty in detecting tiny defect, insufficient model generalization ability and harsh detection environment. Therefore, based on the analysis of the YOLOv4 model, we first utilize the Multistage Attention Module (MAM) to enhance the feature expression ability of the shallow network. Secondly, we exploit rotation detection to accommodate large aspect ratio defects on AFT surfaces. Finally, we convert the pytorch model into a tensorRT engine and deploy it on Agx xavier to achieve high efficiency, high-stability, and low-power inference. Experimental results show that the mean Average Precision (MAP) of our improved model reaches 97.87%, and the single image detection speed reaches 223.23ms, which further proves the good performance of our model on the AFT detection task.","","978-1-6654-8474-9","10.1109/AEMCSE55572.2022.00019","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9948318","component;Rotation Detection;YOLOv4;tensorRT engine","Computational modeling;Atmospheric modeling;Object detection;Feature extraction;Electron tubes;Aircraft;Task analysis","automatic optical inspection;computer vision;feature extraction;learning (artificial intelligence);micro-optomechanical devices;microsensors;object detection;quality control","AFT detection task;AFT surfaces;aircraft Flared Tube;Aircraft Flared Tube;aircraft system;aspect ratio defects;defect rotation detection;detection methods;existing deep learning;feature expression ability;harsh detection environment;improved YOLOv;insufficient model generalization ability;long-term normal operation;Multistage Attention Module;pytorch model;single image detection speed;surface defect detection;time 223.23 ms;tiny defect;YOLOv4 model","","","","13","IEEE","16 Nov 2022","","","IEEE","IEEE Conferences"
"Streaming Detection and Classification Performance of a POWER9 Edge Supercomputer","W. Brewer; C. Geyer; D. Kleiner; C. Horne","DoD HPCMP PET/GDIT, Vicksburg, MS; Parsons Corporation, Centreville, VA; CIPS, Corp., N. Bethesda, MD; Naval Research Laboratory, Washington, DC","2021 IEEE High Performance Extreme Computing Conference (HPEC)","1 Dec 2021","2021","","","1","7","We present training and inference benchmarks on a POWER9 edge supercomputer called SCOUT using two production-level artificial intelligence systems: the Video Processing Exploitation Framework (VPEF) and the Ordnance Threat Target Automated Recognition (OTTAR) system. For training benchmarks, we use Horovod to train ResNet-50 on synthetic data for a baseline benchmark, and then train Faster R-CNN on an available WASABI dataset using SCOUT’s six-way V100 GPU nodes. For inference benchmarks, we use GStreamer to stream both synthetic and real Motion Imagery (MI) and then use Single-Shot Detector trained on MobileNetV2 data with real and synthetic MI at four different resolutions (720p, 1080p, 1280p, and 2160p) while measuring average inference time. We also test reduced-precision inference performance using TensorRT and distributed inference performance using the Inference Benchmark ""iBench"" framework. We compare our results with equivalent work on x86_64 systems and provide suggestions on tuning for optimal performance. We find that V100s work well for training and offline inferencing in batches, while T4 GPUs outperform V100 GPUs only in very specific usage scenarios, such as streaming detection at reduced precision.","2643-1971","978-1-6654-2369-4","10.1109/HPEC49654.2021.9622852","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9622852","power9;distributed;training;inference;GPU;TensorRT;benchmark;Motion Imagery;MI;artificial intelligence;classification;object detection","Training;Image resolution;Target recognition;Image edge detection;Benchmark testing;Streaming media;Supercomputers","benchmark testing;graphics processing units;image classification;inference mechanisms;learning (artificial intelligence);object detection;parallel machines;video coding","average inference time;reduced-precision inference performance;optimal performance;offline inferencing;classification performance;POWER9 edge supercomputer;inference benchmarks;production-level artificial intelligence systems;training benchmarks;synthetic data;baseline benchmark;train Faster R-CNN;single-shot detector;MobileNetV2 data;synthetic MI;inference benchmark iBench framework;synthetic motion imagery;real motion imagery;SCOUT six-way V100 GPU nodes;WASABI dataset;ordnance threat target automated recognition system;video processing exploitation framework","","","","26","IEEE","1 Dec 2021","","","IEEE","IEEE Conferences"
"The Workpiece Sorting Method Based on Improved YOLOv5 For Vision Robotic Arm","Z. Ma; Y. Zeng; L. Zhang; J. Li","Department of Control Engineering CUIT, Chengdu University of Information Technology, Chengdu, Sichuan Province, China; Department of Control Engineering CUIT, Chengdu University of Information Technology, Chengdu, Sichuan Province, China; Department of Control Engineering CUIT, Chengdu University of Information Technology, Chengdu, Sichuan Province, China; Department of Control Engineering CUIT, Chengdu University of Information Technology, Chengdu, Sichuan Province, China","2022 IEEE International Conference on Mechatronics and Automation (ICMA)","22 Aug 2022","2022","","","481","486","In order to solve the problem of high error rate and poor real-time performance in the workpieces sorting process for traditional industrial robotic arms, this paper designed a vision robotic arm testing platform with real-time processing ability, and proposes a kind of workpieces sorting method based on improved YOLOv5 used to the vision robotic arm. By replacing the focus layer in the YOLOv5 backbone network, embedding the coordinate attention module, which re-weights the feature maps from the channel and spatial, improves the object detection accuracy of the YOLOv5 model. The workpiece sorting test platform consists of an NVIDIA Jetson nano controller and a vision robotic arm. The hand-eye calibration of the robotic arm is completed by the Zhang Zhengyou calibration method and the TsarLenz method. The workpiece target image was collected, tagged and data augmented to create the target workpiece dataset. And use TensorRT to optimize the inference acceleration of the model to adapt to the hardware platform requirements. The test shows that the improved YOLOv5 model can well ensure the stable operation of the test platform, and improve the accuracy and real -time performance of workpiece target recognition.","2152-744X","978-1-6654-0853-0","10.1109/ICMA54519.2022.9856190","Engineering Laboratory; Chengdu University; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9856190","Deep learning;Robotic arm sorting;YOLO v5;Object detection;Attention mechanism;TensorRT","Adaptation models;Service robots;Robot kinematics;Computational modeling;Object detection;Life estimation;Manipulators","calibration;cameras;computer vision;industrial robots;object detection;robot vision","YOLOv5 backbone network;test platform;Zhang Zhengyou calibration method;workpiece target image;target workpiece dataset;improved YOLOv5 model;workpiece target recognition;workpiece sorting method;traditional industrial robotic arms;vision robotic arm testing platform","","","","18","IEEE","22 Aug 2022","","","IEEE","IEEE Conferences"
"Vehicle Speed Estimation Using YOLO, Kalman Filter, and Frame Sampling","A. H. Rais; R. Munir","School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia; School of Electrical Engineering and Informatics, Bandung Institute of Technology, Bandung, Indonesia","2021 8th International Conference on Advanced Informatics: Concepts, Theory and Applications (ICAICTA)","16 Dec 2021","2021","","","1","6","Vehicle speed estimation based on video feed can be used to enforce road rules and give traffic insights without the need of physical interference. Common methods are background subtraction, motion detection, and/or convolutional neural network (CNN). The first two methods suffer from inability to differentiate classes and occlusion, whereas CNN suffer from computational complexity. A system based on You Only Look Once (YOLO) as detector and Kalman filter as tracker is proposed. TensorRT and frame sampling are used to further optimize the inference time. From experiment using BrnoCompSpeed dataset on i5 9600k and RTX 2060 Super environment, proposed system runs the entire process at 118 FPS with mean average error (MAE) of 0.96 km/h and [-3,2] error interval at 93.81%. Frame sampling can be used to further improve the FPS, with 1/5 sampling improves the speed by 50% to 177 FPS with only 0.11 km/h MAE tradeoff to 1.07 km/h.","","978-1-6654-1743-3","10.1109/ICAICTA53211.2021.9640272","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9640272","vehicle speed estimation;yolo;tensorrt;kalman filter;frame sampling","Image segmentation;Roads;Pipelines;Interference;Motion detection;Kalman filters;Convolutional neural networks","computational complexity;convolutional neural nets;feature extraction;image motion analysis;Kalman filters;mean square error methods;object detection;road vehicles;traffic engineering computing","vehicle speed estimation;YOLO;Kalman filter;frame sampling;video feed;road rules;traffic insights;background subtraction;motion detection;convolutional neural network;CNN;you only look once;BrnoCompSpeed dataset;mean average error","","","","12","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"Research on Lightweight Method of Image Deep Learning Model for Power Equipment","A. Jiang; N. Yan; B. Shen; C. Gu; H. Huang; H. Zhu","State Grid Shanghai Energy, Interconnection Research Institute, Shanghai, China; State Grid Shanghai Energy, Interconnection Research Institute, Shanghai, China; State Grid Shanghai Energy, Interconnection Research Institute, Shanghai, China; Shanghai Municipal Electric Power Company, Shanghai, China; State Grid Shanghai Energy, Interconnection Research Institute, Shanghai, China; State Grid Shanghai Energy, Interconnection Research Institute, Shanghai, China","2021 China International Conference on Electricity Distribution (CICED)","8 Oct 2021","2021","","","334","337","Image information is the key element to judge the fault and defect of equipment in the power grid inspection. At present, all the automatic image defect analysis technology based on deep learning is to transfer the image to the server cloud for processing. However, due to the wide distribution of power equipment, large amount of image data and long time of data transmission, deep learning of massive image data will lead to severe problems of storage and inference speed. Therefore, it is necessary to carry out automatic analysis and research of power equipment based on edge computing, realize real-time defect analysis of power equipment image, significantly reduce the time of field detection, post-processing data and defect analysis, improve work efficiency and ensure the timeliness of defect detection. This paper analyzes the application scenarios of power equipment image recognition edge computing, summarizes the commonly used power equipment image recognition depth learning model, compares the computing power of existing edge computing chips, and puts forward two kinds of power equipment image recognition depth learning model lightweight schemes, which are network optimization, reasoning optimization and hardware optimization, and obtains the design of substation A scheme example of the application scene for visible light image recognition is provided. The results show that compared with NVIDIA TX2 chip which uses mobile edge computing directly, using TensorRT hardware architecture acceleration and low precision grid acceleration can reduce the recognition time by 48% ~ 72%. Compared with desktop GPU GTX1080TI, using TensorRT hardware architecture acceleration and low-precision grid acceleration recognition time is still longer, so further research on lightweight optimization scheme is needed.","2161-749X","978-1-7281-9229-1","10.1109/CICED50259.2021.9556829","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9556829","power equipment;image recognition;deep learning;model lightweight;edge computing","Deep learning;Analytical models;Image recognition;Substations;Computational modeling;Graphics processing units;Computer architecture","cloud computing;computer vision;deep learning (artificial intelligence);edge detection;graphics processing units;image sensors;inspection;mobile computing;parallel architectures;power apparatus;power engineering computing;power grids;real-time systems;solid modelling;substations","image deep learning model;image information;power grid inspection;automatic image defect analysis technology;massive image data;automatic analysis;real-time defect analysis;post-processing data;defect detection;power equipment image recognition edge computing;computing power;existing edge computing chips;power equipment image recognition depth learning model lightweight schemes;visible light image recognition;mobile edge computing;low-precision grid acceleration recognition time;desktop GPU GTX1080TI;TensorRT hardware architecture acceleration","","2","","15","IEEE","8 Oct 2021","","","IEEE","IEEE Conferences"
"Feasibility Analysis of Machine Learning Optimization on GPU-based Low-cost Edges","J. Suo; X. Zhang; S. Zhang; W. Zhou; W. Shi","School of Software, Yunnan University, Kunming, China; University of Chinese Academy of Sciences, Beijing, China; School of Software, Yunnan University, Kunming, China; School of Software, Yunnan University, Kunming, China; Department of Computer Science, Wayne State University, Detroit, MI, USA","2021 IEEE SmartWorld, Ubiquitous Intelligence & Computing, Advanced & Trusted Computing, Scalable Computing & Communications, Internet of People and Smart City Innovation (SmartWorld/SCALCOM/UIC/ATC/IOP/SCI)","18 Nov 2021","2021","","","89","96","Many AI algorithms have been deployed on edge devices as edge computing has the advantages of reducing latency, saving network bandwidth, and protecting data privacy. Whether edge devices can run AI algorithms is an important challenge due to the low-power and low-cost characteristics of edge devices. Therefore, this paper analyzed the performance of optimization techniques by running YOLOv3 on a typical GPU-based low-cost edge device, NVIDIA Jetson Nano. YOLOv3 is a representative object detection algorithm, which is widely used as the benchmark in AI scenarios. We compared latency, memory, and power consumption of three deep learning frameworks, TensorFlow, PyTorch, and TensorRT. Then we squeezed the extreme performance using multiple optimization techniques, including model quantization, model parallelization, and image scaling on TensorRT. The running speed of YOLOv3 increases from 3.9FPS to 13.1FPS on NVIDIA Jetson Nano. It proves that the resource-limited edge device can run AI applications with high computing power requirements in a real-time manner. Moreover, we summarized nine observations and five insights to guide the selection and design of optimization techniques and verified the generalization of these rules on NVIDIA Jetson Xavier NX. We also provided a series of suggestions to help developers choose the appropriate method to deploy AI algorithms on edge devices.","","978-1-6654-1236-0","10.1109/SWC50871.2021.00022","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9604449","Edge computing;Edge intelligence;YOLOv3;NVIDIA Jetson Nano;NVIDIA Jetson NX;Quantification","Performance evaluation;Deep learning;Quantization (signal);Power demand;Image edge detection;Computational modeling;Memory management","data protection;graphics processing units;learning (artificial intelligence);mobile computing;object detection;object tracking;optimisation","edge computing;AI algorithms;low-cost characteristics;optimization techniques;YOLOv3;NVIDIA Jetson Nano;machine learning optimization;GPU-based low-cost edge device;data privacy protection;representative object detection algorithm;power consumption;deep learning framework;TensorFlow;PyTorch;TensorRT","","","","17","IEEE","18 Nov 2021","","","IEEE","IEEE Conferences"
"Towards Real-Time Vehicle Detection on Edge Devices with Nvidia Jetson TX2","H. -H. Nguyen; D. N. -N. Tran; J. W. Jeon","College of Information and Communication Engineering, Sungkyunkwan University, Suwon, Korea; College of Information and Communication Engineering, Sungkyunkwan University, Suwon, Korea; College of Information and Communication Engineering, Sungkyunkwan University, Suwon, Korea","2020 IEEE International Conference on Consumer Electronics - Asia (ICCE-Asia)","9 Dec 2020","2020","","","1","4","With the development of deep convolutional networks, significant advances in object detection task have been achieved. However, for applications in autonomous vehicles, it is necessary to have an efficient object detector that can process rapidly while maintaining high accuracy. This study presents our implementation and performance evaluation of two object detectors EfficientDet-Lite and Yolov3-tiny on Nvidia Jetson TX2 mobile embedded platform. Our experimental results on the KITTI dataset demonstrate that it is possible to achieve real-time and highly accurate object detection on edge devices with constrained resources.","","978-1-7281-6164-8","10.1109/ICCE-Asia49877.2020.9277463","National Research Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9277463","object detection;real-time;edge device;TensorRT","Object detection;Throughput;Computational modeling;Detectors;Graphics processing units;Feature extraction;Image edge detection","convolutional neural nets;deep learning (artificial intelligence);distributed processing;embedded systems;object detection;road vehicles;traffic engineering computing","Nvidia Jetson TX2 mobile embedded platform;edge devices;real-time vehicle detection;deep convolutional networks;object detection;autonomous vehicles;EfficientDet-Lite object detector;Yolov3-tiny object detector;KITTI dataset","","5","","26","IEEE","9 Dec 2020","","","IEEE","IEEE Conferences"
"Edge-to-client real-time road damage detection system based on Yolov5","M. Li; H. Wang; Y. Peng; X. Pei; T. Wang; T. Hou","School of Electronic and Information Engineering, Beijing Jiao Tong University, Beijing, China; School of Mechanical Engineering and Automation, Beihang University, Beijing, China; School of Mechanical Engineering and Automation, Beihang University, Beijing, China; Security Inspection Division, First Research Institute of The Ministry of Public Security of PRC, Beijing, China; School of Mechanical Engineering and Automation, Beihang University, Beijing, China; School of Electronic and Information Engineering, Beijing Jiao Tong University, Beijing, China","2021 China Automation Congress (CAC)","14 Mar 2022","2021","","","1221","1226","Road Damage may result in a reduction in the efficiency of traffic, and even endanger the safety of human and property. To realize automatic and timely detection of road damage, in this article, we design an edge-to-client road damage detection system based on the YOLO object detection algorithm, which includes roadside information collection platform, edge computing equipment, cloud transfer system, and client. We compared the effects of YOLOv5, YOLOv4, and YOLOv4-tiny on the RDD2020 data set and implemented model deployment and model optimization based on the NVIDIA Jetson NX platform. Experimental results show that the system can realize real-time display of road damage detection.","2688-0938","978-1-6654-2647-3","10.1109/CAC53003.2021.9727536","China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9727536","Road Damage Detection;object detection;YOLO network;edge device;TensorRT;DeepStream","Quantization (signal);Roads;Image edge detection;Computational modeling;Object detection;Streaming media;Real-time systems","cloud computing;object detection;optimisation;road safety;roads;traffic engineering computing","real-time road damage detection system;human property;automatic detection;edge-to-client road damage detection system;YOLO object detection algorithm;roadside information collection platform;edge computing equipment;cloud transfer system;YOLOv5;YOLOv4-tiny;real-time display","","1","","20","IEEE","14 Mar 2022","","","IEEE","IEEE Conferences"
"iBench: a Distributed Inference Simulation and Benchmark Suite","W. Brewer; G. Behm; A. Scheinine; B. Parsons; W. Emeneker; R. P. Trevino","DoD HPCMP PET / Benchmarking, Engineer Research & Development Center, Vicksburg, MS; DoD HPCMP PET / Benchmarking, Engineer Research & Development Center, Vicksburg, MS; DoD HPCMP PET / Benchmarking, Engineer Research & Development Center, Vicksburg, MS; DoD HPCMP, Engineer Research & Development Center, Vicksburg, MS; Machine Learning Group, Maui HPC Center, Kihei, HI; Machine Learning Group, Maui HPC Center, Kihei, HI","2020 IEEE High Performance Extreme Computing Conference (HPEC)","22 Dec 2020","2020","","","1","6","We present a novel distributed inference benchmarking system, called “iBench”, that provides relevant performance metrics for high-performance edge computing systems using trained deep learning models. The proposed benchmark is unique in that it includes data transfer performance through a distributed system, such as a supercomputer, using clients and servers to provide a system-level benchmark. iBench is flexible and robust enough to allow for the benchmarking of custom-built inference servers. This was demonstrated through the development of a custom Flask-based inference server to serve MLPerf's official ResNet50v1.5 model. In this paper, we compare iBench against MLPerf inference performance on an 8-V100 GPU node. iBench is shown to provide two primary advantages over MLPerf: (1) the ability to measure distributed inference performance, and (2) a more realistic measure of benchmark performance for inference servers on HPC by taking into account additional factors to inference time, such as HTTP request-response time, payload pre-processing and packing time, and invest time.","2643-1971","978-1-7281-9219-2","10.1109/HPEC43674.2020.9286169","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9286169","distributed;inference;GPU;TensorRT;ResNet50;benchmark","Computational modeling;Graphics processing units;Benchmark testing;Time measurement;Supercomputers;Servers;Payloads","deep learning (artificial intelligence);graphics processing units;inference mechanisms","iBench;distributed inference simulation;deep learning models;data transfer performance;MLPerf inference performance;8-V100 GPU node;distributed inference performance;system-level benchmarking system;custom flask-based inference server;MLPerf official ResNet50v;HTTP request-response time","","1","","24","IEEE","22 Dec 2020","","","IEEE","IEEE Conferences"
"Thermal Face Detection for High-Speed AI Thermometer","W. Lee; H. Kwon; J. Choi","Department of Electronic Engineering, Hanyang University, Seoul, South Korea; Department of Artificial Intelligence, Hanyang University, Seoul, South Korea; Department of Artificial Intelligence, Hanyang University, Seoul, South Korea","2021 7th IEEE International Conference on Network Intelligence and Digital Content (IC-NIDC)","4 Jan 2022","2021","","","163","167","In the era of COVID-19, temperature measurement becomes a crucial procedure for protecting public spaces against the virus. Artificial intelligence techniques such as object detection deep neural networks (DNNs) have been adopted to enhance the accuracy of contactless temperature measurement. However, the computation-demanding nature of DNNs, along with the time-consuming fusion of video and thermal camera frames, raises hurdles for the cost-effective deployment of such AI thermometer systems. In this work, we propose a high-speed and cost-effective implementation of an AI thermometer. We develop a thermal face detection network to detect faces for temperature measurement without a video camera. We optimize the proposed network's precision and structure to exploit high-throughput reduced-precision computations available in the embedded AI platforms. The resulting AI thermometer system demonstrates a live temperature measurement with a speed of 160 frames per second.","2575-4955","978-1-6654-0582-9","10.1109/IC-NIDC54101.2021.9660583","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9660583","AI thermometer system;Object detection;SSD;NVIDIA Jetson AGX Xavier;TensorRT;Reduced-precision","Temperature measurement;Deep learning;Thermometers;Neural networks;Object detection;Cameras;Coronaviruses","artificial intelligence;face recognition;neural nets;object detection;temperature measurement;thermometers;video cameras","resulting AI thermometer system;live temperature measurement;high-speed AI;COVID-19;crucial procedure;public spaces;virus;artificial intelligence techniques;object detection deep neural networks;DNNs;contactless temperature measurement;computation-demanding nature;thermal camera frames;cost-effective deployment;AI thermometer systems;cost-effective implementation;thermal face detection network;video camera;reduced-precision computations;embedded AI platforms","","","","12","IEEE","4 Jan 2022","","","IEEE","IEEE Conferences"
"Resolving battery status and customer matching to create 24/7 drones based advertisement system","J. Danial; Y. Ben Asher; D. Feldman","computer science, Haifa University, Israel, Haifa; computer science, Haifa University, Israel, Haifa; computer science, Haifa University, Israel, Haifa","2021 Fifth IEEE International Conference on Robotic Computing (IRC)","7 Feb 2022","2021","","","131","136","We consider the problem of devising an advertising system that uses drones that carry advertising signs. The system should satisfy the following constraints:•There are several types of customers the system should identify and classify the current customers.•Each drone can carry a one/double-sided advertisement sign suitable to only one or two types of customers.•The system must use ultra-light drowns whose battery is limited to few flight minutes after which a hovering drone must land for recharge.•The system must use the minimal number of drones that are needed to maintain one drone hovering 24/7.This leads to an algorithmic problem wherein the system must decide A) when to send the hovering drone to recharge and B) which of the recharging drones should replace it. This should be done such that: 1) a maximal number of customers see a sign matching to their type, and 2) maintaining the battery power for the 24/7 mode. The proposed solution is based on a way to classify the discharging curve of the hovering-drone and the charging curve of the currently charging drones. Based on these battery statuses and the current two largest sets of customers, the system maximizes the number of customers that are matched with a suitable sign yet maintains the battery power suffices for the 24/7 hovering mode. The results, obtained by a system of three ultralight drones in a supermarket, show that this algorithm works with high efficiency. We describe how the system was implemented and how the results were obtained.","","978-1-6654-3416-4","10.1109/IRC52146.2021.00032","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9699921","Kalman filter;PID;State-of-charge;self-charging;Autonomous drones;CNN;TensorRT","Conferences;Batteries;Kalman filters;Advertising;Robots;Drones","advertising;aerospace robotics;autonomous aerial vehicles;helicopters","hovering drone;recharging drones;hovering-drone;drones charging;ultralight drones;customer matching;advertisement system;advertising system;advertising signs;current customers;battery power;discharging curve","","","","27","IEEE","7 Feb 2022","","","IEEE","IEEE Conferences"
"Scaled-YOLOv4: Scaling Cross Stage Partial Network","C. -Y. Wang; A. Bochkovskiy; H. -Y. M. Liao","Institute of Information Science, Academia Sinica, Taiwan; Intel Intelligent Systems Lab; Department of Computer Science and Information Engineering, Providence University, Taiwan","2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","2 Nov 2021","2021","","","13024","13033","We show that the YOLOv4 object detection neural network based on the CSP approach, scales both up and down and is applicable to small and large networks while maintaining optimal speed and accuracy. We propose a network scaling approach that modifies not only the depth, width, resolution, but also structure of the network. YOLOv4-large model achieves state-of-the-art results: 55.5% AP (73.4% AP50) for the MS COCO dataset at a speed of ~ 16 FPS on Tesla V100, while with the test time augmentation, YOLOv4-large achieves 56.0% AP (73.3 AP50). To the best of our knowledge, this is currently the highest accuracy on the COCO dataset among any published work. The YOLOv4-tiny model achieves 22.0% AP (42.0% AP50) at a speed of ~443 FPS on RTX 2080Ti, while by using TensorRT, batch size = 4 and FP16-precision the YOLOv4-tiny achieves 1774 FPS.","2575-7075","978-1-6654-4509-2","10.1109/CVPR46437.2021.01283","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9577489","","Computer vision;Computational modeling;Neural networks;Object detection;Pattern recognition","computer vision;learning (artificial intelligence);neural nets;object detection","scaling cross stage partial network;YOLOv4 object detection neural network;CSP approach;optimal speed;network scaling approach;YOLOv4-large model achieves state-of-the-art results;AP50;MS COCO dataset;FPS;YOLOv4-large achieves;YOLOv4-tiny model;AP;Tesla V100;TensorRT","","273","","46","IEEE","2 Nov 2021","","","IEEE","IEEE Conferences"
"Efficient RGB-D Semantic Segmentation for Indoor Scene Analysis","D. Seichter; M. Köhler; B. Lewandowski; T. Wengefeld; H. -M. Gross","Neuroinformatics and Cognitive Robotics Lab, Technische Universität Ilmenau, Ilmenau, Germany; Neuroinformatics and Cognitive Robotics Lab, Technische Universität Ilmenau, Ilmenau, Germany; Neuroinformatics and Cognitive Robotics Lab, Technische Universität Ilmenau, Ilmenau, Germany; Neuroinformatics and Cognitive Robotics Lab, Technische Universität Ilmenau, Ilmenau, Germany; Neuroinformatics and Cognitive Robotics Lab, Technische Universität Ilmenau, Ilmenau, Germany","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","13525","13531","Analyzing scenes thoroughly is crucial for mobile robots acting in different environments. Semantic segmentation can enhance various subsequent tasks, such as (semantically assisted) person perception, (semantic) free space detection, (semantic) mapping, and (semantic) navigation. In this paper, we propose an efficient and robust RGB-D segmentation approach that can be optimized to a high degree using NVIDIA TensorRT and, thus, is well suited as a common initial processing step in a complex system for scene analysis on mobile robots. We show that RGB-D segmentation is superior to processing RGB images solely and that it can still be performed in real time if the network architecture is carefully designed. We evaluate our proposed Efficient Scene Analysis Network (ESANet) on the common indoor datasets NYUv2 and SUNRGB-D and show that we reach state-of-the-art performance while enabling faster inference. Furthermore, our evaluation on the outdoor dataset Cityscapes shows that our approach is suitable for other areas of application as well. Finally, instead of presenting benchmark results only, we also show qualitative results in one of our indoor application scenarios.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561675","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561675","","Image segmentation;Image analysis;Navigation;Semantics;Network architecture;Real-time systems;Decoding","image colour analysis;image segmentation;mobile robots;robot vision","mobile robots;indoor scene analysis;free space detection;RGB-D semantic segmentation approach;NVIDIA TensorRT;efficient scene analysis network;ESANet;SUNRGB-D;outdoor dataset Cityscapes;common indoor datasets","","18","","46","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Deep Learning at Scale on NVIDIA V100 Accelerators","R. Xu; F. Han; Q. Ta","AI Engineering, Dell EMC, Austin, TX, United States; AI Engineering, Dell EMC, Austin, TX, United States; AI Engineering, Dell EMC, Austin, TX, United States","2018 IEEE/ACM Performance Modeling, Benchmarking and Simulation of High Performance Computer Systems (PMBS)","14 Feb 2019","2018","","","23","32","The recent explosion in the popularity of Deep Learning (DL) is due to a combination of improved algorithms, access to large datasets and increased computational power. This had led to a plethora of open-source DL frameworks, each with varying characteristics and capabilities. End users are then left with the difficult task of determining software and hardware configurations to get optimal performance from each framework. We share our experiences and develop best practices for DL training with TensorFlow, MXNet and Caffe2. The paper also looks at DL inferencing with TensorRT on NVIDIA V100 Volta GPUs. It focuses on one of the more prominent neural network architectures, Resnet50, combined with Imagenet dataset. We quantify the impact of hardware attributes on DL workloads such as the usage of PCIe vs NVLink GPUs, performance past a single worker node, effect of high speed interconnect such as InfiniBand EDR on training and the implication of utilizing a network attached storage and its advantages.","","978-1-7281-0182-8","10.1109/PMBS.2018.8641600","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8641600","Deep Learning;Distributed Training;GPU;Benchmarking;V100","Graphics processing units;Training;Deep learning;Peer-to-peer computing;Bandwidth;Computational modeling;Benchmark testing","graphics processing units;learning (artificial intelligence);neural net architecture;public domain software","hardware configurations;optimal performance;DL training;TensorFlow;MXNet;Caffe2;TensorRT;Resnet50;NVLink GPUs;single worker node;network attached storage;computational power;software configurations;neural network architectures;imagenet dataset;deep learning;NVIDIA V100 accelerators;NVIDIA V100 Volta GPU;open source DL frameworks","","13","","29","IEEE","14 Feb 2019","","","IEEE","IEEE Conferences"
"YolactEdge: Real-time Instance Segmentation on the Edge","H. Liu; R. A. Rivera Soto; F. Xiao; Y. Jae Lee","Amazon Web Services, Inc., University of California, Davis; Amazon Web Services, Inc., University of California, Davis; Amazon Web Services, Inc., University of California, Davis; Amazon Web Services, Inc., University of California, Davis","2021 IEEE International Conference on Robotics and Automation (ICRA)","18 Oct 2021","2021","","","9579","9585","We propose YolactEdge, the first competitive instance segmentation approach that runs on small edge devices at real-time speeds. Specifically, YolactEdge runs at up to 30.8 FPS on a Jetson AGX Xavier (and 172.7 FPS on an RTX 2080 Ti) with a ResNet-101 backbone on 550x550 resolution images. To achieve this, we make two improvements to the state-of-the-art image-based real-time method YOLACT [1]: (1) applying TensorRT optimization while carefully trading off speed and accuracy, and (2) a novel feature warping module to exploit temporal redundancy in videos. Experiments on the YouTube VIS and MS COCO datasets demonstrate that YolactEdge produces a 3-5x speed up over existing real-time methods while producing competitive mask and box detection accuracy. We also conduct ablation studies to dissect our design choices and modules. Code and models are available at https://github.com/haotian-liu/yolact_edge.","2577-087X","978-1-7281-9077-8","10.1109/ICRA48506.2021.9561858","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9561858","","Image segmentation;Image resolution;Codes;Automation;Image edge detection;Conferences;Redundancy","distributed processing;image resolution;image segmentation;neural nets;object detection;video signal processing","YolactEdge;real-time instance segmentation;edge devices;real-time speed;Jetson AGX Xavier;ResNet-101 backbone;YOLACT;box detection;TensorRT optimization;feature warping module;temporal redundancy;YouTube VIS;MS COCO;mask detection","","8","","33","IEEE","18 Oct 2021","","","IEEE","IEEE Conferences"
"Vehicle and Pedestrian Detection Algorithm Based on Lightweight YOLOv3-Promote and Semi-Precision Acceleration","H. Xu; M. Guo; N. Nedjah; J. Zhang; P. Li","Jiangsu High Technology Research Key Laboratory for Wireless Sensor Networks, Nanjing, China; Jiangsu High Technology Research Key Laboratory for Wireless Sensor Networks, Nanjing, China; Department of Electronics Engineering and Telecommunications, Rio de Janeiro State University, Rio de Janeiro, Brazil; Xianyang Vocational Technical College, Xianyang, China; School of Computer Science, Nanjing University of Posts and Telecommunications, Nanjing, China","IEEE Transactions on Intelligent Transportation Systems","12 Oct 2022","2022","23","10","19760","19771","Aiming at the shortcomings of the current YOLOv3 model, such as large size, slow response speed, and difficulty in deploying to real devices, this paper reconstructs the target detection model YOLOv3, and proposes a new lightweight target detection network YOLOv3-promote: Firstly, the G-Module combined with the Depth-Wise convolution is used to construct the backbone network of the entire model, and the attention mechanism is introduced and added to perform weighting operations on each channel to get more key features and remove redundant features, thereby strengthening the identification ability of feature network model’s to distinguish target objects among background; Secondly, in order to delete some less important channels to achieve the effect of compressing the model size and improving the calculation speed, the size of the scaling factor gamma in the batch normalization layer is used; Finally, based on NVIDIA’s TensorRT framework model conversion and half-precision acceleration were carried out, and the accelerated model was successfully deployed on the embedded platform Jetson Nano. The performed KITTI experimental results show that the inference speed of our proposed method is about 5 times that of the original model, the parameter volume is reduced to one tenth, the mAP is increased from 86.1% of the original model to 93.1%, and the FPS reaches 25.5fps, realizing the requirements of real-time detection with high precision.","1558-0016","","10.1109/TITS.2021.3137253","National Key Research and Development Program of China(grant numbers:2019YFB2103003); National Natural Science Foundation of China(grant numbers:61872196,61872194,61902196); Scientific and Technological Support Project of Jiangsu Province(grant numbers:BE2017166,BE2019740); Major Natural Science Research Projects in Colleges and Universities of Jiangsu Province(grant numbers:18KJA520008); Postgraduate Research and Practice Innovation Program of Jiangsu Province(grant numbers:KYCX_0973); Six Talent Peaks Project of Jiangsu Province(grant numbers:RJFW-111); 1311 Talent Plan of the Nanjing University of Posts and Telecommunications (NUPT); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9669164","Target detection;lightweight model;model prune;model deployment;semi-precision acceleration","Convolution;Kernel;Computational modeling;Real-time systems;Object detection;Feature extraction;Telecommunications","object detection;pedestrians","accelerated model;backbone network;calculation speed;current YOLOv3 model;depth-wise convolution;feature network model;Jetson Nano;lightweight target detection network YOLOv3-promote;lightweight YOLOv3-promote;NVIDIA's TensorRT framework model conversion;pedestrian detection algorithm;real-time detection;redundant features;semiprecision acceleration;slow response speed;target detection model YOLOv3;target objects;vehicle detection algorithm","","6","","40","CCBY","4 Jan 2022","","","IEEE","IEEE Journals"
"PERSEUS: Characterizing Performance and Cost of Multi-Tenant Serving for CNN Models","M. LeMay; S. Li; T. Guo",Worcester Polytechnic Institute; Worcester Polytechnic Institute; Worcester Polytechnic Institute,"2020 IEEE International Conference on Cloud Engineering (IC2E)","19 May 2020","2020","","","66","72","Deep learning models are increasingly used for end-user applications, supporting both novel features such as facial recognition, and traditional features, e.g. web search. To accommodate high inference throughput, it is common to host a single pre-trained Convolutional Neural Network (CNN) in dedicated cloud-based servers with hardware accelerators such as Graphics Processing Units (GPUs). However, GPUs can be orders of magnitude more expensive than traditional Central Processing Unit (CPU) servers. These resources could also be under-utilized facing dynamic workloads, which may result in inflated serving costs. One potential way to alleviate this problem is by allowing hosted models to share the underlying resources, which we refer to as multi-tenant inference serving. One of the key challenges is maximizing the resource efficiency for multi-tenant serving given hardware with diverse characteristics, models with unique response time Service Level Agreement (SLA), and dynamic inference workloads. In this paper, we present PERSEUS, a measurement framework that provides the basis for understanding the performance and cost trade-offs of multi-tenant model serving. We implemented PERSEUS in Python atop a popular cloud inference server called Nvidia TensorRT Inference Server. Leveraging PERSEUS, we evaluated the inference throughput and cost for serving various models and demonstrated that multi-tenant model serving led to up to 12% cost reduction.","","978-1-7281-1099-8","10.1109/IC2E48712.2020.00014","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9096261","DNN inference;multi-tenancy;performance","Servers;Graphics processing units;Hardware;Load modeling;Computational modeling;Throughput;Machine learning","cloud computing;convolutional neural nets;learning (artificial intelligence);Python","PERSEUS;CNN models;deep learning models;convolutional neural network;graphics processing units;GPUs;multitenant inference serving;service level agreement;dynamic inference workloads;multitenant model serving;Nvidia TensorRT Inference Server;cloud inference server","","5","","42","IEEE","19 May 2020","","","IEEE","IEEE Conferences"
"Object Detection in Thermal Spectrum for Advanced Driver-Assistance Systems (ADAS)","M. A. Farooq; P. Corcoran; C. Rotariu; W. Shariff","School of Engineering, National University of Ireland Galway, Galway, Ireland; School of Engineering, National University of Ireland Galway, Galway, Ireland; Xperi Corporation, Galway, Ireland; School of Engineering, National University of Ireland Galway, Galway, Ireland","IEEE Access","1 Dec 2021","2021","9","","156465","156481","AI-based smart thermal perception systems can cater to the limitations of conventional imaging sensors by providing a more reliable data source in low-lighting conditions and adverse weather conditions. This research evaluates and modifies the state-of-the-art object detection and classifier framework for thermal vision with seven key object classes in order to provide superior thermal sensing and scene understanding input for advanced driver-assistance systems (ADAS). The networks are trained on public datasets and is validated on test data with three different test approaches which include test-time augmentation, test-time with no augmentation, and test-time with model ensembling. Additionally, a new model ensemble-based inference engine is proposed, and its efficacy is tested on locally gathered novel test data comprising of 20K thermal frames captured with an uncooled LWIR prototype thermal camera in challenging weather and environmental scenarios. The performance analysis of trained models is investigated by computing precision, recall, and mean average precision scores (mAP). Furthermore, the smaller network variant of thermal-YOLO architecture is optimized using TensorRT inference accelerator, which is then deployed on GPU and resource-constrained edge hardware Nvidia Jetson Nano. This is implemented to explicitly reduce the inference time on GPU as well as on Nvidia Jetson Nano to evaluate the feasibility for added real-time onboard installations.","2169-3536","","10.1109/ACCESS.2021.3129150","Heliaus (thermal vision augmented awareness) EU project; Electronic Components and Systems for European Leadership (ECSEL) Joint Undertaking (JU)(grant numbers:826131); European Union’s Horizon 2020 research and innovation programme and France, Germany, Ireland, Italy; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9618926","Thermal-infrared;object detection;advanced driver-assistance systems;deep learning;edge computing","Object detection;Detectors;Sensors;Deep learning;Meteorology;Training;Thermal sensors","advanced driver assistance systems;cameras;computer vision;driver information systems;image sensors;inference mechanisms;infrared detectors;infrared imaging;object detection","advanced driver-assistance systems;ADAS;conventional imaging sensors;reliable data source;low-lighting conditions;adverse weather conditions;object detection;thermal vision;superior thermal sensing;scene understanding input;test-time augmentation;model ensembling;thermal-YOLO architecture;AI-based smart thermal perception systems;ensemble-based inference engine;LWIR prototype thermal camera;mean average precision scores;TensorRT inference accelerator;GPU;resource-constrained edge hardware;Nvidia Jetson Nano","","4","","44","CCBYNCND","17 Nov 2021","","","IEEE","IEEE Journals"
"Real-Time Multiple Pedestrian Tracking With Joint Detection and Embedding Deep Learning Model for Embedded Systems","H. -W. Lin; V. M. Shivanna; H. C. Chang; J. -I. Guo","Department of Electronics Engineering, Institute of Electronics, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Department of Electronics Engineering, Institute of Electronics, National Yang Ming Chiao Tung University, Hsinchu, Taiwan; Chunghua Telecom Company Ltd., Taoyuan City, Taiwan; Wistron-NCTU Embedded Artificial Intelligence Research Center, National Yang Ming Chiao Tung University, Hsinchu, Taiwan","IEEE Access","18 May 2022","2022","10","","51458","51471","This paper proposes an improvement to the multi-object tracking system framework based on the image inputs. By analyzing the role and performance of each block in the original multi-objects tracking system, the blocks of the original system are reconstructed to enhance the efficiency and yield a faster processing speed suiting the real-time applications. In the proposed method, the first two parts of the multi-object tracking system are merged into a single neural network designed for object detection and feature extraction. A new object association judgment method and JDE inspired prediction head are included in order to achieve a better and an outstanding association effect resulting in the overall improvement of the original system by 45.2%. The enhanced method is aimed at the application of smart roadside units and uses fixed-viewpoint image input to achieve multi-object tracking on embedded platforms. The proposed method is implemented on the NVIDIA Jetson AGX Xavier embedded platform. The NVIDIA TensorRT software development kit is used to accelerate the neural network. The overall performance of the proposed system yields better efficiency compared to that of the original SDE design and the overall computing performance achieve up to 14–26 images per second, making it ideal for the real-time smart roadside unit applications.","2169-3536","","10.1109/ACCESS.2022.3173408","Center for mmWave Smart Radar Systems and Technologies under the Featured Areas Research Center Program within the framework of the Higher Education Sprout Project; Ministry of Education (MOE), Taiwan; Ministry of Science and Technology (MOST), Taiwan, through Pervasive Artificial Intelligence Research Laboratories (PAIR Labs), Taiwan(grant numbers:MOST 108-3017-F-009-001,MOST 110-2634-F-009-017); Chung-Hwa Telecom Laboratories, Taiwan(grant numbers:RAM09A1151); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9770856","Multiple object tracking;embedded system;advanced driver assistance system (ADAS);smart transportation","Detectors;Feature extraction;Computational modeling;Object tracking;Object detection;Target tracking;Real-time systems","deep learning (artificial intelligence);embedded systems;feature extraction;object detection;object tracking;pedestrians","single neural network;object detection;feature extraction;object association judgment method;fixed-viewpoint image input;real-time smart roadside unit applications;real-time multiple pedestrian tracking;embedding deep learning model;multiobject tracking system framework;joint detection;JDE inspired prediction head;NVIDIA Jetson AGX Xavier embedded platform;NVIDIA TensorRT software development kit;SDE design","","1","","49","CCBYNCND","9 May 2022","","","IEEE","IEEE Journals"
"SwiftLane: Towards Fast and Efficient Lane Detection","O. Jayasinghe; D. Anhettigama; S. Hemachandra; S. Kariyawasam; R. Rodrigo; P. Jayasekara","Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka","2021 20th IEEE International Conference on Machine Learning and Applications (ICMLA)","25 Jan 2022","2021","","","859","864","Recent work done on lane detection has been able to detect lanes accurately in complex scenarios, yet many fail to deliver real-time performance specifically with limited computational resources. In this work, we propose SwiftLane: a simple and light-weight, end-to-end deep learning based framework, coupled with the row-wise classification formulation for fast and efficient lane detection. This framework is supplemented with a false positive suppression algorithm and a curve fitting technique to further increase the accuracy. Our method achieves an inference speed of 411 frames per second, surpassing state-of the-art in terms of speed while achieving comparable results in terms of accuracy on the popular CULane benchmark dataset. In addition, our proposed framework together with TensorRT optimization facilitates real-time lane detection on a Nvidia Jetson AGX Xavier as an embedded system while achieving a high inference speed of 56 frames per second.","","978-1-6654-4337-1","10.1109/ICMLA52953.2021.00142","University of Moratuwa; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9680161","lane detection;deep learning;convolutional neural network;row-wise classification;embedded system","Deep learning;Embedded systems;Lane detection;Network architecture;Benchmark testing;Real-time systems;Inference algorithms","computer vision;deep learning (artificial intelligence);image classification;object detection;traffic engineering computing","SwiftLane;efficient lane detection;end-to-end deep learning based framework;row-wise classification formulation;false positive suppression algorithm;TensorRT optimization;real-time lane detection;CULane benchmark dataset;Nvidia Jetson AGX Xavier","","1","","23","IEEE","25 Jan 2022","","","IEEE","IEEE Conferences"
"Polarmask-Tracker: Lightweight Multi-Object Tracking and Segmentation Model for Edge Device","X. Dong; Z. Ouyang; Z. Guo; J. Niu","Hangzhou Innovation Institution, Beihang University, Binjiang, China; State Key Laboratory of Virtual Reality Technology and Systems; Hangzhou Innovation Institution, Beihang University, Binjiang, China; State Key Laboratory of Virtual Reality Technology and Systems","2021 IEEE Intl Conf on Parallel & Distributed Processing with Applications, Big Data & Cloud Computing, Sustainable Computing & Communications, Social Computing & Networking (ISPA/BDCloud/SocialCom/SustainCom)","22 Dec 2021","2021","","","689","696","The image or video input from the camera is one of the important data sources for unmanned vehicles to perceive the environment. However, the 2D/3D bounding box can only provide a very coarse approximation because one box often contains other targets and background. In order to solve the problem of precise target tracking and computing limitations of edge devices, this paper proposes Polarmask-Tracker, a lightweight segmentation-based multi-object tracking network for vehicular edge devices. Polarmask-Tracker extended the lightweight Polarmask segmentation head with tracking vector. The polar mask replaces the traditional mask prediction by regression of a group of fixed edge points in polar coordinate system, which can greatly optimize the computational complexity and regression difficulty of the mask. With an additional tracking vector branch generated based on mask, the model can learn tracking tasks in an end-to-end manner. Finally, we further accelerated the entire model based on TensorRT and achieve real-time tracking on mobile edge computing platform. Different from previous evaluations on the ImageNet and COCO datasets, this study uses the KITTI tracking dataset to extend the instance segmentation task to segmentation tracking, also called MOTS. At the same time, the target scales captured from the autonomous vehicle camera are usually smaller, which also brings additional challenges. Evaluations on NVidia Jetson AGX show that the final Polarmask-Tracker can achieve 122.55 FPS, 46.57 mAP for mask segmentation, 56.418 HOTA for tracking.","","978-1-6654-3574-1","10.1109/ISPA-BDCloud-SocialCom-SustainCom52081.2021.00100","China Postdoctoral Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9644721","Segmentation;tracking;deep learning;MOTS;autonomous vehicle","Image segmentation;Target tracking;Head;Computational modeling;Soft sensors;Cameras;Real-time systems","computational complexity;image segmentation;learning (artificial intelligence);mobile computing;mobile robots;object detection;object tracking;optimisation;remotely operated vehicles;robot vision;target tracking","multiobject tracking;target tracking;vehicular edge devices;edge points;polar coordinate system;computational complexity;mobile edge computing platform;KITTI tracking dataset;autonomous vehicle camera;tracking vector branch;instance segmentation;Polarmask-Tracker;TensorRT","","1","","67","IEEE","22 Dec 2021","","","IEEE","IEEE Conferences"
"GPU acceleration design method for driver’s seatbelt detection","J. Yongquan; W. Tianshu; L. Jin; Z. Zhijia; G. Chao","School of Information Science and Engineering, Shenyang University of Technology; School of Information Science and Engineering, Shenyang University of Technology; Technology Development Department, Liaoning Aerospace Linghe Automobile Co., Ltd, Shenyang, China; School of Information Science and Engineering, Shenyang University of Technology; Technology Development Department, Liaoning Aerospace Linghe Automobile Co., Ltd, Shenyang, China","2019 14th IEEE International Conference on Electronic Measurement & Instruments (ICEMI)","27 May 2020","2019","","","949","953","With the development and maturity of deep learning algorithms, CNN have emerged in the field of computer vision. Image recognition is one of the important research directions in the field of computer vision. The traditional image recognition method is to extract features by constructing feature descriptors and then classify them by classifiers, such as gradient direction histogram and support vector machine. These methods generally have the problems of poor robustness and insufficient ability to extract features in complex application scenarios. At the same time, convolutional neural network has not been well applied in image recognition due to its large amount of computation and slow speed. With the development of GPU, the parallel computing capability has been greatly improved. This paper designs a GPU acceleration method for the driver's seatbelt detection system based on CNN. The system is based on the Deconv-SSD target detection algorithm for vehicle detection, the Squeeze-YOLO algorithm for vehicle front windshield location, and the semantic segmentation for seat belt detection. Based on the characteristics of GPU, through the off-line merging bath normlization and convolution layer, Tensorrt model conversion technology to realize the GPU optimization speed. The results show that the proposed acceleration method can effectively improve the detection efficiency.","","978-1-7281-0510-9","10.1109/ICEMI46757.2019.9101821","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9101821","GPU acceleration;seatbelt detection;SSD;YOLO","Graphics processing units;Acceleration;Feature extraction;Belts;Convolution;Vehicles;Automotive components","computer vision;convolutional neural nets;feature extraction;image classification;image segmentation;learning (artificial intelligence);object detection;road safety;traffic engineering computing","off-line merging bath normlization;GPU acceleration design method;deep learning algorithms;CNN;computer vision;image recognition method;feature descriptors;classifiers;convolutional neural network;parallel computing capability;GPU acceleration method;Deconv-SSD target detection algorithm;vehicle detection;Squeeze-YOLO algorithm;vehicle front windshield location;driver seat belt detection;Tensorrt model conversion technology","","1","","8","IEEE","27 May 2020","","","IEEE","IEEE Conferences"
"Research on Intelligent Target Recognition Integrated With Knowledge","F. Zhang; H. Fan; K. Wang; Y. Zhao; X. Zhang; Y. Ma","No. 208 Research Institute of China Ordnance Industries, Beijing, China; Beijing Mechanical and Electric Research Institute, Beijing, China; No. 208 Research Institute of China Ordnance Industries, Beijing, China; No. 208 Research Institute of China Ordnance Industries, Beijing, China; No. 208 Research Institute of China Ordnance Industries, Beijing, China; No. 208 Research Institute of China Ordnance Industries, Beijing, China","IEEE Access","12 Oct 2021","2021","9","","137107","137115","With the development of artificial intelligence technology, intelligent weapon systems that can automatically identify, lock on and strike targets have gradually appeared and can replace humans in executing simple decision-making commands. Target detection is a key part of intelligent weapons. At present, large-scale target detection has serious challenges such as long-tail data distributions, severe occlusion, and category ambiguity. The main detection algorithms only detect each independent area without considering the key semantic dependencies between objects. It has become a hot trend to apply deep learning to prior knowledge to form a model. This article uses both internal and external knowledge to instill a target detection system with human reasoning capabilities. Commonly used external embedded knowledge includes geometric relations, attributes, locations, etc. They have a common shortcoming in that they require large amounts of labeled data, and the integration costs are huge. The purpose of this article is to construct a general external prior knowledge module to guide network learning. By paying attention to the characteristics of each object in different semantic contexts, the characteristics of each object are adaptively enhanced, and the high-level semantics of all categories evolve on a global scale. Internal knowledge uses a convolutional attention module that can learn spatial and channel information at multiple scales. The experimental results show the superiority of our knowledge-YOLOv5. The proposed method achieved 1.7%, 2.2%, 1.1%, and 0.7% improvements over YOLOv5s, YOLOv5m, VOLOv51, and YOLOv5x, respectively, on the COCO data sets; and the proposed method also achieves a 0.9% improvement on the self-built data set. The trained lightweight model Knowledge-YOLOv5s is deployed on an NVIDIA Jetson TX2 through TensorRT acceleration, and the real-time detection frame is 20 ms, which meets the real-time detection requirements. This system can also be used as a module of an intelligent weapon system, which has certain referential significance for autonomous weapons and unmanned combat systems.","2169-3536","","10.1109/ACCESS.2021.3116866","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9552859","Prior knowledge;NVIDIA JetsonTX2;YOLOv5;intelligent weapon system;knowledge graph","Feature extraction;Convolution;Object detection;Military aircraft;Semantics;Deep learning","deep learning (artificial intelligence);image recognition;military computing;military systems;object detection;object recognition;weapons","unmanned combat systems;artificial intelligence technology;large-scale target detection;long-tail data distributions;severe occlusion;category ambiguity;deep learning;human reasoning capabilities;network learning;COCO data sets;real-time detection frame;autonomous weapons;intelligent target recognition;intelligent weapon systems;decision-making commands;trained lightweight model Knowledge-YOLOv5s;NVIDIA Jetson TX2;TensorRT acceleration;time 20.0 ms","","1","","26","CCBY","29 Sep 2021","","","IEEE","IEEE Journals"
"HALF: Holistic Auto Machine Learning for FPGAs","J. Ney; D. Loroch; V. Rybalkin; N. Weber; J. Krüger; N. Wehn","University of Kaiserslautern, Kaiserslautern, Germany; Fraunhofer ITWM, Kaiserslautern, Germany; University of Kaiserslautern, Kaiserslautern, Germany; Fraunhofer ITWM, Kaiserslautern, Germany; Fraunhofer ITWM, Kaiserslautern, Germany; University of Kaiserslautern, Kaiserslautern, Germany","2021 31st International Conference on Field-Programmable Logic and Applications (FPL)","12 Oct 2021","2021","","","363","368","Deep Neural Networks (DNNs) are capable of solving complex problems in domains related to embedded systems, such as image and natural language processing. To efficiently implement DNNs on a specific FPGA platform for a given cost criterion, e.g., energy efficiency, an enormous amount of design parameters must be considered from the topology down to the final hardware implementation. Interdependencies between the different design layers must be taken into account and explored efficiently, making it hardly possible to find optimized solutions manually. An automatic, holistic design approach can improve the quality of DNN implementations on FPGA significantly. To this end, we present a cross-layer design space exploration methodology. It comprises optimizations starting from a hardware-aware topology search for DNNs down to the final optimized implementation for a given FPGA platform. The methodology is implemented in our Holistic Auto machine Learning for FPGAs (HALF) framework, which combines an evolutionary search algorithm, various optimization steps, and a library of parametrizable hardware DNN modules. HALF automates both the exploration process and the implementation of optimized solutions on a target FPGA platform for various applications. We demonstrate the performance of HALF on a medical use case for arrhythmia detection for three different design goals, i.e., low-energy, low-power, and high-throughput. Our FPGA implementation outperforms a TensorRT optimized model on an Nvidia Jetson platform in both throughput and energy consumption.","1946-1488","978-1-6654-3759-2","10.1109/FPL53798.2021.00069","Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9556404","Neural Architecture Search;NAS;FPGA;Hardware Library","Energy consumption;Network topology;Throughput;Hardware;Libraries;Natural language processing;Topology","circuit optimisation;deep learning (artificial intelligence);embedded systems;evolutionary computation;field programmable gate arrays;logic design;network topology;search problems","deep neural networks;natural language processing;energy efficiency;automatic design approach;holistic design approach;DNN implementations;cross-layer design space exploration methodology;hardware-aware topology search;evolutionary search algorithm;parametrizable hardware DNN modules;HALF automates;FPGA platform;FPGA implementation;TensorRT optimized model;Nvidia Jetson platform;energy consumption;holistic auto machine learning for FPGA","","1","","23","IEEE","12 Oct 2021","","","IEEE","IEEE Conferences"
"An Improved YOLOv5 Real-time Detection Method for Aircraft Target Detection","Y. Chen; J. Yang; J. Wang; X. Zhou; J. Zou; Y. Li","Shenzhen Institute of Advanced Study University of Electronic Science and Technology of China, Shenzhen, China; Shenzhen Institute of Advanced Study University of Electronic Science and Technology of China, Shenzhen, China; Shenzhen Institute of Advanced Study University of Electronic Science and Technology of China, Shenzhen, China; Shenzhen Institute of Advanced Study University of Electronic Science and Technology of China, Shenzhen, China; Shenzhen Institute of Advanced Study University of Electronic Science and Technology of China, Shenzhen, China; Shenzhen Institute of Advanced Study University of Electronic Science and Technology of China, Shenzhen, China","2022 27th International Conference on Automation and Computing (ICAC)","10 Oct 2022","2022","","","1","6","To address the lack of accuracy and speed of aircraft detection in complex background images, the YOLOv5 model is improved in this paper to meet the growing demand for aircraft detection. Firstly, Mosaic-9 data enhancement is performed on the dataset, and MobileNet V3 Small is used to replace ResNet feature extraction network to enrich small target samples and improve feature extraction speed. Secondly, over-fitting is prevented by Label Smoothing, and channel pruning is used to reduce the complexity and redundancy of the neural network. Finally, TensorRT is used to optimize the underlying hardware resources to improve the detection speed. The experiments use Military Aircraft Detection Dataset for model training and validation on NVIDIA Tesla P100 16GB GPU. The results show that our improvements maintain the accuracy of the model while effectively increasing the training speed and detection speed of the model. Our codes are available at https://github.com/imcyx/Military-Detect","","978-1-6654-9807-4","10.1109/ICAC55051.2022.9911114","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9911114","YOLOv5;Aircraft detection;Data enhancement;Model acceleration","Training;Smoothing methods;Atmospheric modeling;Computational modeling;Graphics processing units;Feature extraction;Hardware","feature extraction;military aircraft;mobile computing;neural nets;object detection;optimisation","aircraft target detection;YOLOv5 model;Mosaic-9 data enhancement;ResNet feature extraction network;neural network;improved YOLOv5 real-time detection method;MobileNet V3 small;TensorRT;military aircraft detection dataset;NVIDIA Tesla P100 16GB GPU","","","","18","IEEE","10 Oct 2022","","","IEEE","IEEE Conferences"
"Edge-based real-time face logging system for security applications","B. Gaikwad; P. Prakash; A. Karmakar","Integrated Systems Lab, CSIR, Central Electronics Engineering and Research Insititute (CEERI), India; Integrated Systems Lab, CSIR, Central Electronics Engineering and Research Insititute (CEERI), India; Integrated Systems Lab, CSIR, Central Electronics Engineering and Research Insititute (CEERI), India","2021 12th International Conference on Computing Communication and Networking Technologies (ICCCNT)","3 Nov 2021","2021","","","1","6","In this work, we have proposed a state-of-the-art face logging system that detects and logs high quality cropped face images of the people in real-time for security applications. Multiple strategies based on resolution, velocity and symmetry of faces have been applied to obtain best quality face images. The proposed system handles the issue of motion blur in the face images by determining the velocities of the detections. The output of the system is the face database, where four faces for each detected person are stored along with the time stamp and ID number tagged to it. The facial features are extracted by our system, which are used to search the person-of-interest instantly. The proposed system has been implemented in a docker container environment on two edge devices: the powerful NVIDIA Jetson TX2 and the cheaper NVIDIA Jetson N ano. The light and fast face detector (LFFD) used for detection, and ResN et50 used for facial feature extraction are optimized using TensorRT over these edge devices. In our experiments, the proposed system achieves the True Acceptance Rate (TAR) of 0.94 at False Acceptance Rate (FAR) of 0.01 while detecting the faces at 20–30 FPS on NVIDIA Jetson TX2 and about 8–10 FPS on NVIDIA Jetson N ano device. The advantage of our system is that it is easily deployable at multiple locations and also scalable based on application requirement. Thus it provides a realistic solution to face logging application as the query or suspect can be searched instantly, which may not only help in investigation of incidents but also in prevention of untoward incidents.","","978-1-7281-8595-8","10.1109/ICCCNT51525.2021.9579758","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9579758","Face-logging;edge computing;biometric systems;face recognition;face detection;smart surveillance","Face recognition;Image edge detection;Surveillance;Smart cameras;Feature extraction;Real-time systems;Nanoscale devices","edge detection;face recognition;feature extraction;recording;security of data","security applications;face database;detected person;edge devices;NVIDIA Jetson TX2;fast face detector;face logging application;edge-based real-time face logging system;high quality cropped face images;motion blur;time stamp;facial feature extraction;NVIDIA Jetson Nano;TensorRT;ResNet50;true acceptance rate;false acceptance rate","","","","26","IEEE","3 Nov 2021","","","IEEE","IEEE Conferences"
"AutoGTCO: Graph and Tensor Co-Optimize for Image Recognition with Transformers on GPU","Y. Bai; X. Yao; Q. Sun; B. Yu",The Chinese University of Hong Kong; SmartMore; The Chinese University of Hong Kong; The Chinese University of Hong Kong,"2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)","23 Dec 2021","2021","","","1","9","Performance optimization is the art of continuously seeking an effective mapping between algorithm and hardware. Existing deep learning compilers or frameworks optimize the computation graph via adapting transformations manually designed by expert efforts. We argue that these methods ignore some possible graph-level optimizations, thus it is difficult to generalize to emerging deep learning models or new operators. In this work, we propose AutoGTCO, a tensor program generation system for vision tasks with the transformer architecture on GPU. Compared with existing fusion strategies, AutoGTCO explores the optimization of operator fusion in the transformer model through a novel dynamic programming algorithm. Specifically, to construct an effective search space of the sampled programs, new sketch generation rules and a search policy are proposed for the batch matrix multiplication and softmax operators in each subgraph, which are capable of fusing them into large computation units, it can then map and transform them into efficient CUDA kernels. Overall, our evaluation on three real-world transformer-based vision tasks shows that AutoGTCO improves the execution performance relative to deep learning engine TensorRT by up to 1.38 ×.","1558-2434","978-1-6654-4507-8","10.1109/ICCAD51958.2021.9643487","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9643487","","Deep learning;Tensors;Computational modeling;Heuristic algorithms;Graphics processing units;Transformers;Dynamic programming","computer vision;deep learning (artificial intelligence);dynamic programming;graph theory;graphics processing units;image recognition;matrix multiplication;parallel architectures;program compilers","AutoGTCO;image recognition;GPU;performance optimization;effective mapping;deep learning compilers;computation graph;expert efforts;graph-level optimizations;deep learning models;tensor program generation system;transformer architecture;operator fusion;transformer model;dynamic programming algorithm;effective search space;sampled programs;sketch generation rules;search policy;batch matrix multiplication;softmax operators;computation units;real-world transformer-based vision tasks;execution performance;deep learning engine TensorRT","","","","37","IEEE","23 Dec 2021","","","IEEE","IEEE Conferences"
"Fabric Defect Detection VIA Unsupervised Neural Networks","K. -H. Liu; S. -J. Chen; C. -H. Chiu; T. -J. Liu","National Taichung University of Science and Technology, Taichung, Taiwan; National Taichung University of Science and Technology, Taichung, Taiwan; National Taichung University of Science and Technology, Taichung, Taiwan; National Chung Hsing University, Taichung, Taiwan","2022 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)","23 Aug 2022","2022","","","1","6","Surface defect detection is a necessary process for quality control in the industry. Currently, popular neural network based defect detection systems usually need to use a large number of defect samples for training, and it takes a lot of manpower to make marks and clean the subsequent data. This is a time-consuming process, and it makes the whole system less effective. In this paper, a deep neural network based model for fabric surface defect detection is proposed and it only uses positive clean samples for training. Since the proposed model does not collect negative defective samples for learning, the landing time of whole system is greatly reduced. In the experiment, we use RTX3080 in the TensorRT model with 250 FPS, and the detection accuracy is 99%, which is suitable for production lines with real time requirements.","","978-1-6654-7218-0","10.1109/ICMEW56448.2022.9859266","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9859266","Autoencoder;defect detection;image reconstruction;image synthesis;unsupervised learning","Training;Surface cleaning;Industries;Neural networks;Process control;Quality control;Production","deep learning (artificial intelligence);fabrics;production engineering computing;quality control;unsupervised learning","necessary process;quality control;defect samples;data cleaning;time-consuming process;deep neural network;fabric surface defect detection;positive clean samples;negative defective samples;detection accuracy;fabric defect detection;unsupervised neural networks;neural network based defect detection systems;RTX3080;TensorRT model","","","","32","IEEE","23 Aug 2022","","","IEEE","IEEE Conferences"
"Deep-CNN based Robotic Multi-Class Under-Canopy Weed Control in Precision Farming","Y. Du; G. Zhang; D. Tsang; M. K. Jawed","Department of Mechanical & Aerospace Engineering, University of California, Los Angeles, Los Angeles, CA; Department of Mechanical & Aerospace Engineering, University of California, Los Angeles, Los Angeles, CA; Department of Mechanical & Aerospace Engineering, University of California, Los Angeles, Los Angeles, CA; Department of Mechanical & Aerospace Engineering, University of California, Los Angeles, Los Angeles, CA","2022 International Conference on Robotics and Automation (ICRA)","12 Jul 2022","2022","","","2273","2279","Smart weeding systems to perform plant-specific operations can contribute to the sustainability of agriculture and the environment. Despite monumental advances in autonomous robotic technologies for precision weed management in recent years, work on under-canopy weeding in fields is yet to be realized. A prerequisite of such systems is reliable detection and classification of weeds to avoid mistakenly spraying and, thus, damaging the surrounding plants. Real-time multi-class weed identification enables species-specific treatment of weeds and significantly reduces the amount of herbicide use. Here, our first contribution is the first adequately large realistic image dataset AIWeeds (one/multiple kinds of weeds in one image), a library of about 10,000 annotated images of flax and the 14 most common weeds in fields and gardens taken from 20 different locations in North Dakota, California, and Central China. Second, we provide a full pipeline from model training with maximum efficiency to deploying the TensorRT-optimized model onto a single board computer. Based on AIWeeds and the pipeline, we present a baseline for classification performance using five benchmark CNN models. Among them, MobileNetV2, with both the shortest inference time and lowest memory consumption, is the qualified candidate for real-time applications. Finally, we deploy MobileNetV2 onto our own compact autonomous robot SAMBot for real-time weed detection. The 90% test accuracy realized in previously unseen scenes in flax fields (with a row spacing of 0.2-0.3 m), with crops and weeds, distortion, blur, and shadows, is a milestone towards precision weed control in the real world. We have publicly released the dataset and code to generate the results at https://github.com/StructuresComp/Multi-class-Weed-Classification.","","978-1-7281-9681-7","10.1109/ICRA46639.2022.9812240","NSF(grant numbers:IIS - 1925360); National Institute of Food and Agriculture; USDA(grant numbers:2021-67022-34200); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9812240","","Training;Computational modeling;Pipelines;Venus;Spraying;Benchmark testing;Real-time systems","agricultural robots;agriculture;agrochemicals;convolutional neural nets;crops;deep learning (artificial intelligence);image classification;mobile robots;multi-robot systems;object detection","TensorRT-optimized model;classification performance;MobileNetV2;shortest inference time;compact autonomous robot SAMBot;flax fields;deep-CNN based robotic multiclass under-canopy weed control;precision farming;smart weeding systems;plant-specific operations;autonomous robotic technologies;precision weed management;large realistic image dataset AIWeeds;single board computer","","","","26","IEEE","12 Jul 2022","","","IEEE","IEEE Conferences"
"A High-Performance Accelerator for Super-Resolution Processing on Embedded GPU","W. Zhao; Q. Sun; Y. Bai; W. Li; H. Zheng; B. Yu; M. D. F. Wong",The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong; The Chinese University of Hong Kong; SmartMore; The Chinese University of Hong Kong; The Chinese University of Hong Kong,"2021 IEEE/ACM International Conference On Computer Aided Design (ICCAD)","23 Dec 2021","2021","","","1","9","Recent years have witnessed impressive progress in super-resolution (SR) processing. However, its real-time inference requirement sets a challenge not only for the model design but also for the on-chip implementation. In this paper, we implement a full-stack SR acceleration framework on embedded GPU devices. The special dictionary learning algorithm used in SR models was analyzed in detail and accelerated via a novel dictionary selective strategy. Besides, the hardware programming architecture together with the model structure is analyzed to guide the optimal design of computation kernels to minimize the inference latency under the resource constraints. With these novel techniques, the communication and computation bottlenecks in the deep dictionary learning-based SR models are tackled perfectly. The experiments on the edge embedded NVIDIA NX and 2080Ti show that our method outperforms the state-of-the-art NVIDIA TensorRT significantly and can achieve real-time performance.","1558-2434","978-1-6654-4507-8","10.1109/ICCAD51958.2021.9643472","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9643472","","Analytical models;Dictionaries;Computational modeling;Superresolution;Graphics processing units;Machine learning;Programming","graphics processing units;image reconstruction;image representation;image resolution;learning (artificial intelligence)","high-performance accelerator;super-resolution processing;full-stack SR acceleration framework;embedded GPU devices;special dictionary learning algorithm;deep dictionary learning-based SR models;dictionary selective strategy;NVIDIA TensorRT;NVIDIA 2080Ti","","","","47","IEEE","23 Dec 2021","","","IEEE","IEEE Conferences"
"Jetson NX-oriented Armored Vehicle Fine-grained Recognition","J. Wang; G. Pan; Y. Li; Y. Wu; Y. He","Command and Control Engineering College, Army Engineering University of PLA, Nanjing, China; Command and Control Engineering College, Army Engineering University of PLA, Nanjing, China; Command and Control Engineering College, Army Engineering University of PLA, Nanjing, China; Command and Control Engineering College, Army Engineering University of PLA, Nanjing, China; Command and Control Engineering College, Army Engineering University of PLA, Nanjing, China","2022 7th International Conference on Signal and Image Processing (ICSIP)","19 Sep 2022","2022","","","253","259","Armored vehicles are the main combat equipment on the land battlefield. After discovering armored vehicles, the first task is to identify their fine-grained types, so as to provide more accurate intelligence information for commanders and provide auxiliary decision support for subsequent actions. At present, the research of armored vehicles mainly focuses on target discovery, but ignores the accurate recognition and edge application conditions after discovery. Therefore, to classify the fine-grained armored vehicles, a lightweight adaptive enhanced recognition model is proposed for Jetson NX edge computing device. The model adopts the lightweight RepVGG as the backbone, adds an adaptive enhancement mapping module to the linear classifier head, and transforms the linear classifier head into a nonlinear classifier head, which improves the adaptability of the model in cross-domain transfer learning. At the same time, facing the carrying capacity and power consumption limitation of the combat platform, the re-parameterization technology and the TensorRT optimizer are used to realize the compression, transformation and speed-up of the model on Jetson NX. On the armored vehicle fine-grained recognition dataset, the proposed method achieves good accuracy and fast computational speed.","","978-1-6654-9563-9","10.1109/ICSIP55141.2022.9886353","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9886353","armored vehicle. fine-grained recognition;convolutional neural network;Jetson NX","Space vehicles;Adaptation models;Image recognition;Head;Target recognition;Computational modeling;Transfer learning","convolutional neural nets;image classification;learning (artificial intelligence);military vehicles;road vehicles","Jetson NX edge computing device;adaptive enhancement mapping module;linear classifier head;nonlinear classifier head;armored vehicle fine-grained recognition dataset;Jetson NX-oriented armored vehicle fine-grained recognition;main combat equipment;intelligence information;edge application conditions;fine-grained armored vehicles;lightweight adaptive enhanced recognition model;lightweight RepVGG;cross-domain transfer learning;TensorRT optimizer","","","","36","IEEE","19 Sep 2022","","","IEEE","IEEE Conferences"
"Real-Time Traffic Counting on Resource Constrained Embedded Systems","K. Kollek; M. Braun; J. -H. Meusener; A. Kummert","School of Electrical, Information and Media Engineering, University of Wuppertal, Wuppertal, Germany; School of Electrical, Information and Media Engineering, University of Wuppertal, Wuppertal, Germany; School of Electrical, Information and Media Engineering, University of Wuppertal, Wuppertal, Germany; School of Electrical, Information and Media Engineering, University of Wuppertal, Wuppertal, Germany","2022 IEEE 65th International Midwest Symposium on Circuits and Systems (MWSCAS)","22 Aug 2022","2022","","","1","4","Real-time traffic monitoring is crucial to meet the demand for intelligent traffic coordination. Adjusted green phases can improve traffic flow and lead to congestion avoidance. Therefore, in addition to object counting, it is crucial for optimal traffic monitoring to track object movement. In this paper, we propose a highly optimized framework for detecting, tracking and counting road users at intersections. The nano YOLOv5 is used for detection and a neural architecture searched network is proposed for feature extraction of the DeepSORT algorithm. Both are compressed using TensorRT. Between detected frames, an optical flow-based tracker provides consistent processing of tracked objects and relieves the embedded system of some computationally intensive neural network inference. Motion and box shape features are taken into account to improve robustness in re-identification and prevent swapping boxes.","1558-3899","978-1-6654-0279-8","10.1109/MWSCAS54063.2022.9859538","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9859538","YOLO;Jetson Nano;embedded system;traffic counting;tracking;DeepSORT;neural architecture search;PC-DARTS;KLT;optical flow","Optical filters;Embedded systems;Tracking;Shape;Neural networks;Computer architecture;Feature extraction","embedded systems;feature extraction;image sequences;multiprocessing systems;neural nets;object detection;object tracking;optimisation;road traffic;road vehicles;traffic engineering computing","time traffic counting;resource constrained embedded systems;TensorRT;box shape features;computationally intensive neural network inference;embedded system;tracked objects;optical flow-based tracker;feature extraction;neural architecture;nanoYOLOv5;detecting tracking;highly optimized framework;object movement;optimal traffic monitoring;congestion avoidance;traffic flow;adjusted green phases;intelligent traffic coordination;real-time traffic monitoring","","","","12","IEEE","22 Aug 2022","","","IEEE","IEEE Conferences"
"Semantic Segmentation Optimized for Low Compute Embedded Devices","H. Son; J. Weiland","Department of Biomedical Engineering, University of Michigan, Ann Arbor, MI, USA; Department of Biomedical Engineering, University of Michigan, Ann Arbor, MI, USA","IEEE Access","19 Sep 2022","2022","10","","96514","96525","Deployment of a deep convolutional neural network (CNN) on low compute devices is an increasingly important area of research. Wearable and robotic systems use semantic information for efficient navigation or gain of contextual information. However, real-time semantic segmentation is challenging on low compute devices. We propose a compact CNN for real-time applications on low compute devices. Our decoder uses pixel shuffling to achieve efficient inferences. We compared our CNN with state-of-the-art models ranked on the Cityscapes real-time semantic segmentation category. We propose a modified Net score that includes frame per seconds to complement traditional metrics mean Intersection of Union (mIoU), the number of multiply-accumulate operations (GFLOPs), and the number of parameters to evaluate mobile computing performance. Our CNN achieved 65.7 FPS on GTX 1080, 76.7% mIoU without ImageNet pre-training, while requiring 25 GFLOPs and 4.55M parameters, resulting in a 127.53 modified Net score compared to 119.89 for Deep Dual Resolution Neural Network (DDRNET23_slim) and 115.39 for Regseg. Using the Camvid test set, our CNN performance (83.3% mIoU and 354 FPS with TensorRT) was superior to published mIoU values for Regseg and other CNNs. The accuracy and FPS on Camvid show state-of-the-art performance. To demonstrate compatibility with low compute devices, we evaluated our CNN on two mobile computing platforms and showed real-time performance (57 fps) on Jetson NX 8 GB with TensorRT and 12.65 fps on Jetson Xavier AGX without TensorRT. Our CNN can operate with high accuracy on low compute devices to support system which benefits from semantic information.","2169-3536","","10.1109/ACCESS.2022.3199418","Ford Motor Company; University of Michigan; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9858046","Semantic segmentation;hardware-constrained;low compute devices;real-time performance;deep convolutional neural network;Jetson Xavier AGX;Jetson Nx","Semantics;Real-time systems;Convolutional neural networks;Performance evaluation;Mobile computing;Measurement;Image segmentation","","","","","","65","CCBYNCND","17 Aug 2022","","","IEEE","IEEE Journals"
"Deployment of Deep Neural Networks for Object Detection on Edge AI Devices with Runtime Optimization","L. Stäcker; J. Fei; P. Heidenreich; F. Bonarens; J. Rambach; D. Stricker; C. Stiller","German Research Center for Artificial Intelligence, Germany; Institute of Measurement and Control Systems, Karlsruhe Institute of Technology, Germany; Stellantis, Opel Automobile GmbH, Germany; Stellantis, Opel Automobile GmbH, Germany; German Research Center for Artificial Intelligence, Germany; German Research Center for Artificial Intelligence, Germany; Institute of Measurement and Control Systems, Karlsruhe Institute of Technology, Germany","2021 IEEE/CVF International Conference on Computer Vision Workshops (ICCVW)","24 Nov 2021","2021","","","1015","1022","Deep neural networks have proven increasingly important for automotive scene understanding with new algorithms offering constant improvements of the detection performance. However, there is little emphasis on experiences and needs for deployment in embedded environments. We therefore perform a case study of the deployment of two representative object detection networks on an edge AI platform. In particular, we consider RetinaNet for image-based 2D object detection and PointPillars for LiDAR-based 3D object detection. We describe the modifications necessary to convert the algorithms from a PyTorch training environment to the deployment environment taking into account the available tools. We evaluate the runtime of the deployed DNN using two different libraries, TensorRT and Torch-Script. In our experiments, we observe slight advantages of TensorRT for convolutional layers and TorchScript for fully connected layers. We also study the trade-off between run-time and performance, when selecting an optimized setup for deployment, and observe that quantization significantly reduces the runtime while having only little impact on the detection performance.","2473-9944","978-1-6654-0191-3","10.1109/ICCVW54120.2021.00118","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9607492","","Performance evaluation;Deep learning;Runtime;Three-dimensional displays;Quantization (signal);Laser radar;Image edge detection","feature extraction;learning (artificial intelligence);neural nets;object detection;optical radar","embedded environments;representative object detection networks;edge AI platform;image-based 2D;LiDAR-based 3D;PyTorch training environment;deployment environment;deployed DNN;detection performance;deep neural networks;edge AI devices;runtime optimization;automotive scene understanding","","2","","25","IEEE","24 Nov 2021","","","IEEE","IEEE Conferences"
"Design and Development of Autonomous Driving Car Using NvidiaJetson Nano Developer Kit","M. R. Kounte; C. v. Aswini Shri; V. Harshvardhan; A. Kumari; S. Dhruv","School of ECE, REVA University, Bengaluru, India; School of ECE, REVA University, Bengaluru, India; School of ECE, REVA University, Bengaluru, India; School of ECE, REVA University, Bengaluru, India; School of ECE, REVA University, Bengaluru, India","2022 IEEE 4th International Conference on Cybernetics, Cognition and Machine Learning Applications (ICCCMLA)","22 Dec 2022","2022","","","486","489","The implementation of the steam engine sky-rocketed humanity’s progress towards the current age of technological marvel. A lot of modern electro-mechanical feats can be traced back to their inception. With the many obvious benefits of an autonomous driving system, the future of self-driving vehicles is looking very bright. A system like this could greatly benefit our environment as the usage of self-driving vehicles could reduce fuel consumption worldwide by over 10%. In this paper, we present a design of a system consisting of a self-driving car model which employs Jetracer, an AI framework for autonomous driving cars. We make use of AI and ML frameworks such as Pytorch, OpenCV, and TensorRT. We apply image recognition to capture traffic signs and classify them using the mentioned AI and ML frameworks and respond to them in real-time through the Jetson Nano’s interface.In our work, we have employed Jetracer using interactive web programming via a web browser. It allows high frame rate processing due to torch2trt (PyTorch to TensorRT translator) optimization, to achieve faster autonomous line driving using the jetson nano. Also, we have summarized the results highlighting the self-driving car model that uses AI, Machine Learning, and Neural Networks to autonomously drive on a track","","978-1-6654-6246-4","10.1109/ICCCMLA56841.2022.9989127","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9989127","DNN (Deep Neural Network);SSH (Secure share);REST API;Convolution Neural Network (CNN) and Machine learning","Image recognition;Neural networks;Machine learning;Programming;Steam engines;Real-time systems;Autonomous automobiles","","","","","","13","IEEE","22 Dec 2022","","","IEEE","IEEE Conferences"
"CELR: Cloud Enhanced Local Reconstruction from low-dose sparse Scanning Electron Microscopy images","F. De Putter; M. Peemen; P. Potocek; R. Schoenmakers; H. Corporaal","Eindhoven University of Technology, Eindhoven, The Netherlands; Thermo Fisher Scientific, Eindhoven, The Netherlands; Thermo Fisher Scientific, Eindhoven, The Netherlands; Thermo Fisher Scientific, Eindhoven, The Netherlands; Eindhoven University of Technology, Eindhoven, The Netherlands","2022 25th Euromicro Conference on Digital System Design (DSD)","4 Jan 2023","2022","","","577","584","Current Scanning Electron Microscopy (SEM) acquisition techniques are far too slow to capture large volumes in a feasible time. One solution is to use low-dose and sparse imaging. By computationally denoising and inpainting an image with acceptable quality can be approximated. This approach, however, requires significant compute resources. Therefore, this paper proposes CELR, a framework, that hides the computationally expensive workload of reconstructing low-dose sparse SEM images, such that (delayed) live reconstruction is possible. Live reconstruction is possible by using Convolutional Neural Networks (CNNs) that approximate a classical reconstruction algorithm like GOAL. The reconstruction by CNNs is done locally, while recurring training of CNNs is done in the cloud. Moreover, training labels are generated by GOAL in the cloud. Next to the framework, this paper evaluates and optimizes the CNN reconstruction throughput by employing Nvidia's TensorRT. This paper also touches upon open research questions about on-the-fly CNN training. The combination of CELR and TensorRT enables large volume acquisitions with a dwell-time of $\mathbf{1}\mu s$ and 10% pixel coverage to be reconstructed on a single GPU.","2771-2508","978-1-6654-7404-7","10.1109/DSD57027.2022.00083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9996717","","Training;Scanning electron microscopy;Noise reduction;Graphics processing units;Imaging;Reconstruction algorithms;Throughput","","","","","","15","IEEE","4 Jan 2023","","","IEEE","IEEE Conferences"
"A Convolution Neural Network-Based Speckle Tracking Method for Ultrasound Elastography","B. Peng; Y. Xian; J. Jiang","School of Computer Science, Southwest Petroleum University, Chengdu, China; School of Computer Science, Southwest Petroleum University, Chengdu, China; Department of Biomedical Engineering, Michigan Technological University, Houghton, USA","2018 IEEE International Ultrasonics Symposium (IUS)","20 Dec 2018","2018","","","206","212","Accurate tracking of tissue motion is critically important for several ultrasound elastography methods including strain elastography and shear wave elastography. In this study, we investigate the feasibility of using a convolution neural network (CNN)-based speckle tracking method for elastographic applications. Additional data sets produced by finite element simulations were used to train an existing Flownet 2.0 model. After training, the improved network was evaluated using computer-simulated and tissue-mimicking phantoms, and in vivo breast ultrasound data. Our preliminary results were compared to a published 2D high-quality speckle tracking method by our group. Our preliminary results showed that the improved CNN outperformed the original Flownet 2.0. More specifically, in a tissue-mimicking phantom and one set of in vivo breast ultrasound data, the proposed CNN-based method achieved higher contrast-to-noise ratios (0.64 vs 1.05 and 1.16 vs. 1.40, respectively), as compared with the original Flownet 2.0 model. However, the improved CNN was still inferior to the coupled tracking algorithm. Currently, the inference process after the training of the proposed CNN can achieve approximately 60 frames/second for 2D speckle tracking under the NVIDIA TensorRT™ framework. Overall, we conclude that applying the proposed CNN-based speckle tracking method is feasible and good-quality strain elastography data can be obtained in TM phantoms and in vivo breast data. Our future work includes applying this technique to in vivo 3D whole breast ultrasound data.","1948-5727","978-1-5386-3425-7","10.1109/ULTSYM.2018.8580034","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8580034","Speckle Tracking;Ultrasound Elastography;Neural Network;Motion Tracking;Strain Elastography","Strain;Ultrasonic imaging;Phantoms;Tracking;Training;Breast;Elastography","biological tissues;biomechanics;biomedical ultrasonics;elasticity;medical image processing;neural nets;phantoms;speckle","convolution neural network-based speckle tracking method;tissue motion;shear wave elastography;finite element simulations;tissue-mimicking phantom;high-quality speckle tracking method;CNN-based method;coupled tracking algorithm;CNN-based speckle tracking method;ultrasound elastography;in vivo 3D whole breast ultrasound data;strain elastography data;Flownet 2.0 model;TM phantoms","","13","","9","IEEE","20 Dec 2018","","","IEEE","IEEE Conferences"
"Mozart: Designing for Software Maturity and the Next Paradigm for Chip Architectures","K. Sankaralingam; T. Nowatzki; G. Wright; P. Palamuttam; J. Khare; V. Gangadhar; P. Shah",UW-Madison and SimpleMachines; UCLA; SimpleMachines; SimpleMachines; SimpleMachines; SimpleMachines; SimpleMachines,"2021 IEEE Hot Chips 33 Symposium (HCS)","20 Oct 2021","2021","","","1","20","Where does AI hardware/software stand today? 1. The computational diversity needed to support AI is increasing2. The software user experience expectations is increasing3. GPU software maturity* is unrivalled in completeness and hence allows near complete dominance among AI industry deployment and researchers.4. This support for model diversity is fuelling these trends and increasing GPU adoption!* NVIDIA DL stack - cuDNN, TensorRT, etc.","2573-2048","978-1-6654-1397-8","10.1109/HCS52781.2021.9567306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9567306","","Industries;Computational modeling;Graphics processing units;Computer architecture;Market research;Software;User experience","AI chips;Capability Maturity Model;electronic engineering computing;graphics processing units;user experience","computational diversity;software user experience expectations;AI hardware;GPU software maturity;chip architectures;AI software","","1","","0","IEEE","20 Oct 2021","","","IEEE","IEEE Conferences"
"Accelerated identification method of industrial instrument based on yolov5","S. Yu; H. Zheng; C. Song; P. Gao","Chinese Academy of Sciences, Institutes for Robotics and Intelligent Manufacturing, Shenyang, China; University of Chinese Academy of Sciences, Beijing, China; Chinese Academy of Sciences, Institutes for Robotics and Intelligent Manufacturing, Shenyang, China; University of Chinese Academy of Sciences, Beijing, China","2022 International Conference on Image Processing, Computer Vision and Machine Learning (ICICML)","12 Jan 2023","2022","","","236","239","There are many problems in industrial instrument detection, such as complex background and visual Angle, etc. Traditional machine learning methods are difficult to achieve high-precision object detection and real-time monitoring applications. This paper proposes an accelerated identification method of industrial instruments based on YOLOv5. First, sample data are collected on site for image annotation, and YOLOv5 network structure is designed for transfer learning training, so as to realize image recognition of four types of industrial instruments. After that, the model is accelerated based on TensorRT on Jetson TX2 platform. The speed of instrument online detection reached 30 fps and mAP0.5 is 0.989.","","978-1-6654-6468-0","10.1109/ICICML57342.2022.10009712","Nature; State Key Laboratory of Robotics; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=10009712","industrial instrument;object detection;Yolov5;TX2","Training;Visualization;Computer vision;Image recognition;Instruments;Computational modeling;Transfer learning","","","","","","12","IEEE","12 Jan 2023","","","IEEE","IEEE Conferences"
"A Cityscape Image Detail Extraction Enhancement Method for Lightweight Semantic Segmentation","X. Yu; H. Xu; L. Weng","College of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; College of Computer and Information Engineering, Xiamen University of Technology, Xiamen, China; School of Design and Art, Xiamen University of Technology, China","2022 IEEE 16th International Conference on Anti-counterfeiting, Security, and Identification (ASID)","3 Jan 2023","2022","","","129","133","Lightweight semantic segmentation is widely used in automotive driving. But the existing methods lack the ability to extract the detailed features of urban street scenes, and the semantic segmentation network structure lacks the logical relationship of interdependence. In order to improve semantic segmentation performance in automotive driving, this paper is based on BisenetV2 to propose: (1) The re-parametrization strategy to improve the ability to extract details features. (2) The SENet channel attention mechanism is adopted to explicitly establish the interdependence between feature channels. (3) Using the larger kernel in the deep layer of the network structure increases the accuracy of semantic segmentation and hardly affects the calculated amount. We tested the Cityscapes test dataset to achieve 72.23% mIoU at 2048×1024 resolution with the speed of 39.55 FPS on one NVIDIA RTX A5000 card without pre-training and accelerated implementations like TensorRT, which is 1.8% more accurate than the latest methods while almost as fast.","2163-5056","978-1-6654-9067-2","10.1109/ASID56930.2022.9995858","National Natural Science Foundation of China(grant numbers:U1805264,61772444); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9995858","semantic segmentation;bilateral network;re-parameterization;attention mechanism","Image resolution;Semantic segmentation;Semantics;Life estimation;Feature extraction;Transformers;Computational efficiency","","","","","","19","IEEE","3 Jan 2023","","","IEEE","IEEE Conferences"
"Research on an aerial object detection algorithm based on improved YOLOv5","Y. Huang; H. Cui; J. Ma; Y. Hao","Control technology and Systems Laboratory, Beijing Research Institute of Telemetry, Beijing, China; Control technology and Systems Laboratory, Beijing Research Institute of Telemetry, Beijing, China; Control technology and Systems Laboratory, Beijing Research Institute of Telemetry, Beijing, China; Control technology and Systems Laboratory, Beijing Research Institute of Telemetry, Beijing, China","2022 3rd International Conference on Computer Vision, Image and Deep Learning & International Conference on Computer Engineering and Applications (CVIDL & ICCEA)","18 Jul 2022","2022","","","396","400","Aerial target detection is an important direction in target detection. Due to the limitations of UAV’s volume and computing power, it is necessary to balance the time complexity and accuracy of the algorithm. This paper selects YOLOv5s as the benchmark method of the article. By introducing the feature extraction structure of shufflenetv2, the backbone of YOLOv5s is improved to reduce the amount of calculation of the network, and then the attention mechanism CBMA module is added to the network. Finally, the detection comparison experiment is carried out on visdrone data set. Experimental results show that flops is reduced from 15.7G to 3.7G. After accelerated by tensorRT framework, the improved algorithm can run 53fps on Jetson nano, which can meet the needs of real-time UAV aerial target detection.","","978-1-6654-5911-2","10.1109/CVIDLICCEA56201.2022.9825196","National Science Foundation; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9825196","Artificial intelligence;Deep learning;Object detection;Image processing;UAV aerial photography","Photography;Object detection;Benchmark testing;Autonomous aerial vehicles;Feature extraction;Real-time systems;Complexity theory","autonomous aerial vehicles;feature extraction;object detection;remotely operated vehicles","feature extraction structure;YOLOv5s;attention mechanism CBMA module;detection comparison experiment;real-time UAV aerial target detection;aerial object detection algorithm;improved YOLOv;important direction;UAV's volume;time complexity;benchmark method","","","","15","IEEE","18 Jul 2022","","","IEEE","IEEE Conferences"
"High Performance Neural Network Inference, Streaming, and Visualization of Medical Images Using FAST","E. Smistad; A. Østvik; A. Pedersen","Department of Circulation and Medical Imaging, Norwegian University of Science and Technology, Trondheim, Norway; Department of Circulation and Medical Imaging, Norwegian University of Science and Technology, Trondheim, Norway; SINTEF Medical Technology, Trondheim, Norway","IEEE Access","1 Oct 2019","2019","7","","136310","136321","Deep convolutional neural networks have quickly become the standard for medical image analysis. Although there are many frameworks focusing on training neural networks, there are few that focus on high performance inference and visualization of medical images. Neural network inference requires an inference engine (IE), and there are currently several IEs available including Intel's OpenVINO, NVIDIA's TensorRT, and Google's TensorFlow which supports multiple backends, including NVIDIA's cuDNN, AMD's ROCm and Intel's MKL-DNN. These IEs only work on specific processors and have completely different application programming interfaces (APIs). In this paper, we presents methods for extending FAST, an open-source high performance framework for medical imaging, to use any IE with a common programming interface. Thereby making it easier for users to deploy and test their neural networks on different processors. This article provides an overview of current IEs and how they can be combined with existing software such as FAST. The methods are demonstrated and evaluated on three performance demanding medical use cases: real-time ultrasound image segmentation, computed tomography (CT) volume segmentation, and patch-wise classification of whole slide microscopy images. Runtime performance was measured on the three use cases with several different IEs and processors. This revealed that the choice of IE and processor can affect performance of medical neural network image analysis considerably. In the most extreme case of processing 171 ultrasound frames, the difference between the fastest and slowest configuration were half a second vs. 24 seconds. For volume processing, using the CPU or the GPU, showed a difference of 2 vs. 53 seconds, and for processing an whole slide microscopy image, the difference was 81 seconds vs. almost 16 minutes. Source code, binary releases and test data can be found online on GitHub at https://github.com/smistad/FAST/.","2169-3536","","10.1109/ACCESS.2019.2942441","Norges Forskningsråd(grant numbers:270941,237887); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8844665","Deep learning;inference;neural networks;medical imaging;high performance","Biomedical imaging;Ultrasonic imaging;Streaming media;Artificial neural networks;Libraries","application program interfaces;biomedical ultrasonics;image classification;image recognition;image segmentation;inference mechanisms;learning (artificial intelligence);medical image processing;neural nets","high performance neural network inference;FAST;convolutional neural networks;medical image analysis;high performance inference;inference engine;Intel's MKL-DNN;completely different application programming interfaces;open-source high performance framework;medical imaging;current IEs;medical use cases;real-time ultrasound image segmentation;slide microscopy image;runtime performance;medical neural network image analysis;GPU;ultrasound frame processing;time 24.0 s;time 53.0 s;time 81.0 s;time 16.0 min","","16","","23","CCBY","19 Sep 2019","","","IEEE","IEEE Journals"
"POD: Practical Object Detection With Scale-Sensitive Network","J. Peng; M. Sun; Z. -X. Zhang; T. Tan; J. Yan","Center for Research on Intelligent Perception and Computing; SenseTime Group Limited; University of Chinese Academy of Sciences, Beijing, China; Center for Research on Intelligent Perception and Computing; SenseTime Group Limited","2019 IEEE/CVF International Conference on Computer Vision (ICCV)","27 Feb 2020","2019","","","9606","9615","Scale-sensitive object detection remains a challenging task, where most of the existing methods not learn it explicitly and not robust to scale variance. In addition, the most existing methods are less efficient during training or slow during inference, which are not friendly to real-time application. In this paper, we propose a practical object detection with scale-sensitive network.Our method first predicts a global continuous scale ,which shared by all position, for each convolution filter of each network stage. To effectively learn the scale, we average the spatial features and distill the scale from channels. For fast-deployment, we propose a scale decomposition method that transfers the robust fractional scale into combinations of fixed integral scales for each convolution filter, which exploit the dilated convolution. We demonstrate it on one-stage and two-stage algorithm under almost different configure. For practical application, training of our method is of efficiency and simplicity which gets rid of complex data sampling or optimize strategy. During testing, the proposed method requires no extra operation and is very friendly to hardware acceleration like TensorRT and TVM.On the COCO test-dev, our model could achieve a 41.5mAP on one-stage detector and 42.1 mAP on two-stage detectors based on ResNet-101, outperforming baselines by 2.4 and 2.1 respectively without extra FLOPS.","2380-7504","978-1-7281-4803-8","10.1109/ICCV.2019.00970","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9010019","","Convolution;Detectors;Object detection;Hardware;Proposals;Training;Optimization","image filtering;learning (artificial intelligence);object detection;optimisation","practical object detection;scale-sensitive network;scale-sensitive object detection;scale variance;global continuous scale;convolution filter;network stage;scale decomposition method;robust fractional scale;fixed integral scales","","14","","43","IEEE","27 Feb 2020","","","IEEE","IEEE Conferences"
"An Open Source GPU-Based Beamformer for Real-Time Ultrasound Imaging and Applications","D. Hyun; Y. L. Li; I. Steinberg; M. Jakovljevic; T. Klap; J. J. Dahl","Department of Radiology, Stanford University, Stanford, CA; Department of Radiology, Stanford University, Stanford, CA; Department of Radiology, Stanford University, Stanford, CA; Department of Radiology, Stanford University, Stanford, CA; Independent; Department of Radiology, Stanford University, Stanford, CA","2019 IEEE International Ultrasonics Symposium (IUS)","8 Dec 2019","2019","","","20","23","Recent technological advances in graphics processing unit (GPU)-based computing have made it possible to visualize customized beamforming pipelines and algorithms in real-time. However, GPU programming is challenging and poses a significant barrier to its widespread adoption in the ultrasound research community. Here, we present an open source GPU beamformer with the intent of making GPU beamforming more accessible to a wider audience. The beamformer was written in C++/CUDA and is comprised of a library of core classes to perform typical ultrasound-related tasks, such as applying focusing delays. Classes are arranged into a computational graph that is fixed at compile-time, enabling high throughput at runtime. Concrete examples are provided to demonstrate how to interface the beamforming library with the MATLAB-based Verasonics platform to perform live B-mode and Doppler imaging. Also provided is an example of deploying a TensorFlow neural network in real-time via TensorRT. Compilation was performed using CMake to allow for cross-platform compatibility. Including overhead for data acquisition, the beamformer achieved live B-mode imaging with a Verasonics Vantage 256 system at 55 frames per second using a single NVIDIA Titan V GPU. The open source GPU beamformer can be used as a starting point for real-time algorithm deployment.","1948-5727","978-1-7281-4596-9","10.1109/ULTSYM.2019.8926193","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8926193","","Graphics processing units;Real-time systems;Imaging;Ultrasonic imaging;Libraries;Neural networks;Array signal processing","array signal processing;biomedical ultrasonics;data acquisition;graphics processing units;medical image processing;parallel architectures;public domain software;ultrasonic imaging","TensorFlow neural network;Doppler imaging;C++-CUDA;real-time ultrasound imaging;open source GPU-based beamformer;B-mode imaging;MATLAB-based Verasonics platform;beamforming library;computational graph;ultrasound-related tasks;ultrasound research community;GPU programming;graphics processing unit-based computing","","12","","11","IEEE","8 Dec 2019","","","IEEE","IEEE Conferences"
"Improving Neural Network Efficiency via Post-training Quantization with Adaptive Floating-Point","F. Liu; W. Zhao; Z. He; Y. Wang; Z. Wang; C. Dai; X. Liang; L. Jiang","Shanghai Qi Zhi Institute; Shanghai Qi Zhi Institute; Shanghai Jiao Tong University; Northeastern University; Shanghai Jiao Tong University; DeepBlue Technology (Shanghai) Co., Ltd.; Shanghai Jiao Tong University; Shanghai Qi Zhi Institute","2021 IEEE/CVF International Conference on Computer Vision (ICCV)","28 Feb 2022","2021","","","5261","5270","Model quantization has emerged as a mandatory technique for efficient inference with advanced Deep Neural Networks (DNN) by representing model parameters with fewer bits. Nevertheless, prior model quantization either suffers from the inefficient data encoding method thus leading to noncompetitive model compression rate, or requires time-consuming quantization aware training process. In this work, we propose a novel Adaptive Floating-Point (AFP) as a variant of standard IEEE-754 floating-point format, with flexible configuration of exponent and mantissa segments. Leveraging the AFP for model quantization (i.e., encoding the parameter) could significantly enhance the model compression rate without accuracy degradation and model re-training. We also want to highlight that our proposed AFP could effectively eliminate the computationally intensive de-quantization step existing in the dynamic quantization technique adopted by the famous machine learning frameworks (e.g., pytorch, tensorRT, etc.). Moreover, we develop a framework to automatically optimize and choose the adequate AFP configuration for each layer, thus maximizing the compression efficacy. Our experiments indicate that AFP-encoded ResNet-50/MobileNet-v2 only has ∼0.04/0.6% accuracy degradation w.r.t its full-precision counterpart. It outperforms the state-of-the-art works by 1.1% in accuracy using the same bit-width while reducing the energy consumption by 11.2×, which is quite impressive for inference. Code is released at: https://github.com/MXHX7199/ICCV_2021_AFP","2380-7504","978-1-6654-2812-5","10.1109/ICCV48922.2021.00523","National Natural Science Foundation of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9710763","Efficient training and inference methods;Recognition and classification","Degradation;Training;Adaptation models;Energy consumption;Quantization (signal);Computational modeling;Encoding","","","","10","","35","IEEE","28 Feb 2022","","","IEEE","IEEE Conferences"
"Real-Time Defect Detection of Track Components: Considering Class Imbalance and Subtle Difference Between Classes","Z. Tu; S. Wu; G. Kang; J. Lin","Key Laboratory of Magnetic Suspension Technology and Maglev Vehicle, Ministry of Education, Southwest Jiaotong University, Chengdu, China; Key Laboratory of Magnetic Suspension Technology and Maglev Vehicle, Ministry of Education, Southwest Jiaotong University, Chengdu, China; CRRC Zhuzhou Electric Locomotive Institute Company Ltd., Zhuzhou, China; CRRC Zhuzhou Electric Locomotive Institute Company Ltd., Zhuzhou, China","IEEE Transactions on Instrumentation and Measurement","12 Oct 2021","2021","70","","1","12","During the operation of the railway system, it is inevitable that there will be track defects that affect train operation safety, especially the defects of the rail and fastener. Therefore, it is necessary to detect these defects in time to ensure the safety of train operations. In recent years, with the development of deep learning and computer vision technology, intelligent detection of track defects has made great progress. However, the existing methods not only suffer from the scarcity of defective samples but also their fine-grained recognition is low due to subtle differences between similar classes; furthermore, their real-time performance also needs to be improved. To solve these problems, this article presents a real-time defect detection method for track components based on instance segmentation. First, an improved lightweight instance segmentation network is proposed for the segmentation and location of fastener and rail. Second, a method that detects fastener defects by analyzing the geometric features of the fastener masks is proposed, which can overcome the scarcity of defective fastener samples. A cascade defect detection network is proposed to realize multiresolution and high-resolution detection of rail, which improves the accuracy of rail defect detection. Finally, the TensorRT inference framework is used to accelerate the defect detection network and realize edge end deployment. The experimental results show that the proposed method can achieve 95.1% recall rate of fastener defects, 98% detection rate of rail defects, and 93.5% classification accuracy of rail defects; 33.4 FPS defect detection speed is implemented in the Jetson AGX Xavier embedded device, which verifies the accuracy and real-time performance of the proposed method.","1557-9662","","10.1109/TIM.2021.3117357","Major Science and Technology Projects of Sichuan, China(grant numbers:20QYCX0095); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9559954","Defect detection;fastener;instance segmentation;lightweight;rail","Rails;Fasteners;Real-time systems;Image segmentation;Deep learning;Feature extraction;Performance evaluation","computer vision;condition monitoring;deep learning (artificial intelligence);fasteners;feature extraction;image classification;image segmentation;mechanical engineering computing;object detection;pattern classification;railway safety","track components;subtle difference;track defects;train operation safety;train operations;deep learning;computer vision technology;intelligent detection;defective samples;similar classes;real-time performance;real-time defect detection method;improved lightweight instance segmentation network;fastener defects;fastener masks;defective fastener samples;cascade defect detection network;rail defect detection;detection rate;rail defects;class imbalance;FPS defect detection speed;Jetson AGX Xavier embedded device","","7","","44","IEEE","5 Oct 2021","","","IEEE","IEEE Journals"
"Making Convolutions Resilient Via Algorithm-Based Error Detection Techniques","S. K. S. Hari; M. B. Sullivan; T. Tsai; S. W. Keckler","NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA; NVIDIA, Santa Clara, CA, USA","IEEE Transactions on Dependable and Secure Computing","8 Jul 2022","2022","19","4","2546","2558","Convolutional Neural Networks (CNNs) are being increasingly used in safety-critical and high-performance computing systems. As such systems require high levels of resilience to errors, CNNs must execute correctly in the presence of hardware faults. Full duplication provides the needed assurance but incurs a prohibitive 100 percent overhead. In this article, we focus on algorithmically verifying convolutions, the most resource-demanding operations in CNNs. We use checksums to verify convolutions. We identify the feasibility and performance related challenges that arise in algorithmically detecting errors in convolutions in optimized CNN inference deployment platforms (e.g., TensorFlow or TensorRT on GPUs) that fuse multiple network layers and use reduced-precision operations, and demonstrate how to overcome them. We propose and evaluate variations of the algorithm-based error detection (ABED) techniques that offer implementation complexity, runtime overhead, and coverage trade-offs. Results show that ABED can detect all transient hardware errors that might otherwise corrupt output with low runtime overheads (6-23 percent). Only about 1.4 percent of the total computations in a CNN are not protected by ABED, which can be duplicated for full CNN protection. ABED for the compute-intensive convolutions and duplicating the rest can offer at least 1.6× throughput compared to full duplication.","1941-0018","","10.1109/TDSC.2021.3063083","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9366780","Resilience;hardware error detection;convolutional neural networks","Hardware;Safety;Runtime;Resilience;Training;Inference algorithms;Computational modeling","convolutional neural nets;fault tolerant computing;parallel processing;safety-critical software","CNN;reduced-precision operations;ABED;compute-intensive convolutions;convolutional neural networks;safety-critical system;high-performance computing system;hardware faults;convolution resilient;algorithm-based error detection;multiple network layer fusion","","6","","40","IEEE","2 Mar 2021","","","IEEE","IEEE Journals"
"Deep Learning-based Real-time Object Detection for Empty-Dish Recycling Robot","X. Yue; H. Li; M. Shimizu; S. Kawamura; L. Meng","Department of Electronic and Computer Engineering, Ritsumeikan University, Kusatsu, Japan; Department of Electronic and Computer Engineering, Ritsumeikan University, Kusatsu, Japan; Research Organization of Science and Technology, Ritsumeikan University, Kusatsu, Japan; Department of Robotics, Ritsumeikan University, Kusatsu, Japan; Department of Electronic and Computer Engineering, Ritsumeikan University, Kusatsu, Japan","2022 13th Asian Control Conference (ASCC)","20 Jul 2022","2022","","","2177","2182","The world is facing a shrinking workforce by the sagging birth rate and an aging population. Robot techniques are one of the best solutions for taking place of humans and overcoming this emergency issue. This paper introduces a deep learning-based empty-dish recycling robot for realizing the automatic empty-dish recycling after breakfast, dinner, or lunch in a restaurant, canteen, or cafeteria. A deep learning model–You Only Look Once (YOLO)–is equipped for dish detection such as cups, bowls, chopsticks, towels et al., and catch points are calculated for controlling the robot arm to recycle the target dishes. Finally, the YOLOv4 model is quantized by TensorRT and deployed on Jetson Nano. The real-time dish detection YOLO is focused on this paper, the experimental results show that after the YOLO model quantization, the detection time of a single image is increased from 3.93s to 0.44s, with more than 96.00% high accuracy on Precision, Recall, and F1 values. The functions of empty-dish recycling are realized, which will lead to further development toward practical use.","2770-8373","978-89-93215-23-6","10.23919/ASCC56756.2022.9828060","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9828060","YOLOv4;Empty-Dish Recycling Robot;Deep Learning;Real-time Object Detection","Quantization (signal);Image coding;Computational modeling;Sociology;Object detection;Manipulators;Real-time systems","catering industry;deep learning (artificial intelligence);intelligent robots;learning (artificial intelligence);mobile robots;object detection;object recognition;robot vision","single image;experimental results;Jetson Nano;catch points;dish detection;YOLO;deep learning model-you only look once;automatic empty-dish recycling;emergency issue;detection time;YOLO model quantization;real-time dish detection YOLO;YOLOv4 model;target dishes;robot arm;deep learning model-You;deep learning-based empty-dish recycling robot;robot techniques;aging population;sagging birth rate;shrinking workforce;real-time object detection","","4","","31","","20 Jul 2022","","","IEEE","IEEE Conferences"
"A Deployment Scheme of YOLOv5 with Inference Optimizations Based on the Triton Inference Server","J. Fang; Q. Liu; J. Li","School of Software Engineering, South China University of Technology, Guangzhou, China; School of Software Engineering, South China University of Technology, Guangzhou, China; Zhuhai Unitech Power Technology Company Ltd","2021 IEEE 6th International Conference on Cloud Computing and Big Data Analytics (ICCCBDA)","2 Jun 2021","2021","","","441","445","Object detection constitutes a large part of computer vision applications. You Only Look Once (YOLO) v5 is a salient object detection algorithm that provides high accuracy and real-time performance. This paper illustrates a deployment scheme of YOLOv5 with inference optimizations on Nvidia graphics cards using an open-source deep-learning deployment framework named Triton Inference Server. Moreover, we developed a non-maximum suppression (NMS) operator with dynamic-batch-size support in TensorRT to accelerate inference. The experimental results show that both throughput and latency are improved significantly through our deployment scheme.","","978-1-6654-2311-3","10.1109/ICCCBDA51879.2021.9442557","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9442557","deployment;object detection;performance optimizations;triton inference server;you only look once (YOLO)","Graphics;Heuristic algorithms;Conferences;Object detection;Throughput;Real-time systems;Inference algorithms","computer vision;deep learning (artificial intelligence);inference mechanisms;object detection;optimisation;public domain software","deployment scheme;YOLOv5;inference optimizations;computer vision applications;salient object detection algorithm;real-time performance;Nvidia graphics cards;nonmaximum suppression operator;Triton inference server;open-source deep-learning deployment framework;dynamic-batch-size","","3","","19","IEEE","2 Jun 2021","","","IEEE","IEEE Conferences"
"Efficient Implementation of Convolutional Neural Networks with End to End Integer-Only Dataflow","Y. Yao; B. Dong; Y. Li; W. Yang; H. Zhu","Yidun Lab, NetEase Inc, Hangzhou, China; Yidun Lab, NetEase Inc, Hangzhou, China; Yidun Lab, NetEase Inc, Hangzhou, China; Yidun Lab, NetEase Inc, Hangzhou, China; Yidun Lab, NetEase Inc, Hangzhou, China","2019 IEEE International Conference on Multimedia and Expo (ICME)","5 Aug 2019","2019","","","1780","1785","Linear INT8 quantization is presented to construct an end to end integer-only dataflow for efficient inference of modern CNNs. The INT8 method is implemented with unified layer representation, thus quantized CNNs can be partitioned into computation subgraphs consisting of stacked unified layers with simplified integer-only arithmetic flow and scaling back mechanism, indicating high effectiveness for specific hardware realization. Experimental results show that both the classification and object detection models quantized by proposed INT8 method suffer approximate 1% accuracy loss, exhibiting comparable results with TensorRT. As a result, the deep learning accelerator (DLA) with integer-only dataflow and efficient memory hierarchy is designed for CNN applications.","1945-788X","978-1-5386-9552-4","10.1109/ICME.2019.00306","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8784721","model compression;INT8 quantization;unified layer representation;FPGA implementation","Quantization (signal);Computational modeling;Two dimensional displays;Convolution;Field programmable gate arrays;Training;Computational efficiency","convolutional neural nets;data flow computing;graph theory;learning (artificial intelligence);object detection","convolutional neural networks;linear INT8 quantization;unified layer representation;computation subgraphs;stacked unified layers;classification;object detection models;simplified integer-only arithmetic flow;integer-only dataflow;deep learning accelerator","","3","","11","IEEE","5 Aug 2019","","","IEEE","IEEE Conferences"
"Real-Time Monocular Human Depth Estimation and Segmentation on Embedded Systems","S. An; F. Zhou; M. Yang; H. Zhu; C. Fu; K. A. Tsintotas","School of Computer Science and Engineering, Beihang University, Beijing, China; Tech & Data Center, JD.COM Inc, Beijing, China; Tech & Data Center, JD.COM Inc, Beijing, China; School of Computer Science and Engineering, Beihang University, Beijing, China; School of Mechanical Engineering, Tongji University, Shanghai, China; Department of Production and Management Engineering, Democritus University of Thrace, Xanthi, Greece","2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","16 Dec 2021","2021","","","55","62","Estimating a scene’s depth to achieve collision avoidance against moving pedestrians is a crucial and fundamental problem in the robotic field. This paper proposes a novel, low complexity network architecture for fast and accurate human depth estimation and segmentation in indoor environments, aiming to applications for resource-constrained platforms (including battery-powered aerial, micro-aerial, and ground vehicles) with a monocular camera being the primary perception module. Following the encoder-decoder structure, the proposed framework consists of two branches, one for depth prediction and another for semantic segmentation. Moreover, network structure optimization is employed to improve its forward inference speed. Exhaustive experiments on three self-generated datasets prove our pipeline’s capability to execute in real-time, achieving higher frame rates than contemporary state-of-the-art frameworks (114.6 frames per second on an NVIDIA Jetson Nano GPU with TensorRT) while maintaining comparable accuracy.","2153-0866","978-1-6654-1714-3","10.1109/IROS51168.2021.9636518","National Key Research and Development Program of China; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9636518","","Semantics;Robot vision systems;Estimation;Network architecture;Real-time systems;Land vehicles;Indoor environment","cameras;collision avoidance;computer vision;embedded systems;feature extraction;graphics processing units;image segmentation;image sequences;mobile robots;object detection;optimisation;robot vision","time monocular human depth estimation;embedded systems;scene;collision avoidance;moving pedestrians;crucial problem;fundamental problem;robotic field;low complexity network architecture;fast depth estimation;accurate human depth estimation;indoor environments;resource-constrained platforms;including battery-powered;monocular camera;primary perception module;encoder-decoder structure;depth prediction;semantic segmentation;network structure optimization","","2","","50","IEEE","16 Dec 2021","","","IEEE","IEEE Conferences"
"Efficient Integer-Arithmetic-Only Convolutional Networks with Bounded ReLU","H. Zhao; D. Liu; H. Li","CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, University of Science and Technology of China, Hefei, China; CAS Key Laboratory of Technology in Geo-Spatial Information Processing and Application System, University of Science and Technology of China, Hefei, China","2021 IEEE International Symposium on Circuits and Systems (ISCAS)","27 Apr 2021","2021","","","1","5","To facilitate large-scale deployment of convolutional networks, integer-arithmetic-only inference has been demonstrated effective, which not only reduces computational cost but also ensures cross-platform consistency. However, previous studies on integer networks usually report a decline in the inference accuracy, given the same number of parameters as floating-point-number (FPN) networks. In this paper, we propose to finetune and quantize a well-trained FPN convolutional network to obtain an integer convolutional network. Our key idea is to adjust the upper bound of a bounded rectified linear unit (ReLU), which replaces the normal ReLU and effectively controls the dynamic range of activations. Based on the tradeoff between learning ability and quantization error of networks, we managed to preserve full accuracy after quantization and obtain efficient integer networks. Our experiments on ResNet for image classification demonstrate that our 8-bit integer networks achieve state-of-the-art performance compared with Google's TensorFlow and NVIDIA's TensorRT. Moreover, we experiment on VDSR for image super-resolution and on VRCNN for compression artifact reduction, both of which serve regression tasks that natively require high inference accuracy. Besides ensuring the equivalent performance as the corresponding FPN networks, our integer networks have only 1/4 memory cost and run 2× faster on GPUs.","2158-1525","978-1-7281-9201-7","10.1109/ISCAS51556.2021.9401448","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401448","Bounded ReLU;convolutional neural network;integer arithmetic;quantization","Upper bound;Quantization (signal);Image coding;Superresolution;Dynamic range;Task analysis;Image classification","approximation theory;convolution;data compression;floating point arithmetic;image classification;image coding;image resolution;learning (artificial intelligence);neural nets;quantisation (signal);regression analysis","high inference accuracy;corresponding FPN networks;efficient integer-arithmetic-only;convolutional networks;bounded ReLU;integer-arithmetic-only inference;cross-platform consistency;floating-point-number;FPN convolutional network;integer convolutional network;bounded rectified linear unit;normal ReLU;quantization;efficient integer networks;8-bit integer networks achieve state-of-the-art performance","","1","","24","IEEE","27 Apr 2021","","","IEEE","IEEE Conferences"
"Optimized Deep Learning Object Recognition for Drones using Embedded GPU","P. A. Rad; D. Hofmann; S. A. Pertuz Mendez; D. Goehringer","Chair of Adaptive Dynamic Systems, Technische Universität Dresden, Germany; Chair of Adaptive Dynamic Systems, Technische Universität Dresden, Germany; Chair of Adaptive Dynamic Systems, Technische Universität Dresden, Germany; Chair of Adaptive Dynamic Systems, Technische Universität Dresden, Germany","2021 26th IEEE International Conference on Emerging Technologies and Factory Automation (ETFA )","30 Nov 2021","2021","","","1","7","Nowadays, drones can be seen in various applications in industry like surveillance and transportation. Industrial drones leverage fully-fledged computer vision techniques, such as object detection based on Deep Learning Neural Networks (DNN), to efficiently perform these objectives. Those techniques come with a high computational effort and are implemented on distributed schemes using ground devices with high performance and power consumption. This limits a drone's operational range since it has to communicate with the ground devices constantly. To alleviate such constraints, an optimized, low-power perception system on the drone is desirable. This work improves a trained DNN architecture to navigate a UAV introduced by the University of Zurich called DroNet. DroNet is computationally expensive and has a high power consumption, making it unsuitable for embedded platforms because of low memory and computational power. In this paper, a ROS-based architecture is first designed to port DroNet on a low-power Jetson Nano board, which conducts the drone's perception and control tasks. Secondly, tuning parameters and various schemes have been carried out to run the inference of the DNN efficiently. To implement the different layers in DNNs, Nvidia's TensorRT SDK is used to compile a high-performance inference engine for the Jetson Nano. Results showed that the Jetson Nano can achieve real-time performance, with 47 frames per second using a Winograd convolution and well-tuned parallelization parameters. The implementation can also achieve a speedup of 2× as compared with the Jetson Nanos ARM CPU while increasing the power consumption by 54%. Finally, the Jetson Nano's usability for drone inference algorithm is shown, achieving real-time response using the DroNet DNN without losing detection accuracy.","","978-1-7281-2989-1","10.1109/ETFA45728.2021.9613590","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9613590","drones;Jetson Nano;low power deep learning;deep learning;neural networks;object detection","Performance evaluation;Deep learning;Power demand;Estimation;Object detection;Real-time systems;Inference algorithms","autonomous aerial vehicles;computer vision;graphics processing units;inference mechanisms;learning (artificial intelligence);neural nets;object detection;object recognition;parallel architectures","optimized Deep Learning object recognition;embedded GPU;surveillance;transportation;industrial drones leverage;computer vision techniques;object detection;Neural Networks;high computational effort;distributed schemes;ground devices;low-power perception system;trained DNN architecture;high power consumption;embedded platforms;low memory;computational power;ROS-based architecture;port DroNet;low-power Jetson Nanoboard;high-performance inference engine;real-time performance;Jetson Nanos ARM CPU;Jetson Nano's usability;drone inference algorithm;DroNet DNN","","1","","27","IEEE","30 Nov 2021","","","IEEE","IEEE Conferences"
"Anatomy of Deep Learning Image Classification and Object Detection on Commercial Edge Devices: A Case Study on Face Mask Detection","D. Kolosov; V. Kelefouras; P. Kourtessis; I. Mporas","School of Physics, Engineering and Computer Science, University of Hertfordshire, Hatfield, U.K.; School of Engineering, Computing and Mathematics, University of Plymouth, Plymouth, U.K.; School of Physics, Engineering and Computer Science, University of Hertfordshire, Hatfield, U.K.; School of Physics, Engineering and Computer Science, University of Hertfordshire, Hatfield, U.K.","IEEE Access","19 Oct 2022","2022","10","","109167","109186","Developing efficient on-the-edge Deep Learning (DL) applications is a challenging and non-trivial task, as first different DL models need to be explored with different trade-offs between accuracy and complexity, second, various optimization options, frameworks and libraries are available that need to be explored, third, a wide range of edge devices are available with different computation and memory constraints. As such, trade-offs arise among inference time, energy consumption, efficiency (throughput/watt) and value (throughput/dollar). To shed some light in this problem, a case study is delivered where seven Image Classification (IC) and six Object Detection (OD) State-of-The-Art (SOTA) DL models were used to detect face masks on the following commercial off-the-shelf edge devices: Raspberry PI 4, Intel Neural Compute Stick 2, Jetson Nano, Jetson Xavier NX, and i.MX 8M Plus. First, a full end-to-end video pipeline face mask wearing detection architecture is developed. Then, the thirteen DL models were optimized, evaluated and compared on the edge devices, in terms of accuracy and inference time. To leverage the computational power of the edge devices, the models have been optimized, first, by using the SOTA optimization frameworks (TensorFlow Lite, OpenVINO, TensorRT, eIQ) and, second, by evaluating/comparing different optimization options, e.g., different levels of quantization. Note that the five edge devices are evaluated and compared too, in terms of inference time, value and efficiency. Last, we obtain insightful observations on which optimization frameworks, libraries and options to use and on how to select the right device depending on the target metric (inference time, efficiency and value). For example, we show that Jetson Xavier NX platform is the best in terms of latency and efficiency (FPS/Watt), while Jetson Nano is the best in terms of value (FPS/ $\$ $ ).","2169-3536","","10.1109/ACCESS.2022.3214214","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9918067","Image classification;object detection;edge computing;computer vision;performance evaluation","Integrated circuit modeling;Optimization;Hardware;Computational modeling;Integrated circuits;Image edge detection;Face recognition;Image classificatoin;Computer vision;Performance evaluation;Edge computing;Object detection","biomedical equipment;deep learning (artificial intelligence);diseases;epidemics;image classification;medical image processing;object detection;optimisation;protective clothing;video signal processing","COVID-19;object detection state-of-the-art DL models;state-of-the-art DL models;i.MX 8M Plus;Jetson Xavier NX;Raspberry PI 4;energy consumption;on-the-edge deep learning;end-to-end video pipeline face mask detection architecture;SOTA optimization frameworks;Jetson Nano;Intel Neural Compute Stick 2;commercial off-the-shelf edge devices;inference time;memory constraints;libraries;face mask detection;deep learning image classification","","","","70","CCBY","13 Oct 2022","","","IEEE","IEEE Journals"
"TransTracking for UAV: An Autonomous Real-time Target Tracking System for UAV via Transformer Tracking","X. Sun; F. Xie; Q. Wang; Y. Yao; H. Wang; W. Wang; W. Yang","Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, School of Automation, Southeast University, Nanjing, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, School of Automation, Southeast University, Nanjing, China; School of Automation, Nanjing University of Information Science and Technology, Nanjing, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, School of Automation, Southeast University, Nanjing, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, School of Automation, Southeast University, Nanjing, China; School of Automation, Nanjing University of Information Science and Technology, Nanjing, China; Key Laboratory of Measurement and Control of Complex Systems of Engineering, Ministry of Education, School of Automation, Southeast University, Nanjing, China","2021 International Conference on Intelligent Technology and Embedded Systems (ICITES)","20 Dec 2021","2021","","","102","107","Most of the existing Siamese-type trackers usually adopt pre-defined anchor boxes or anchor-free schemes to accurately estimate the bounding box of targets. Unfortunately, they suffer from complicated hand-designed components and tedious post-processings. It is not easy to adjust parameters for unique scenes in real applications. So, we propose a new scheme by formulating visual tracking as a direct set prediction problem to alleviate this issue. The main component is a transformer attached to the Siamese-type feature extraction networks. Thus, our new framework can be summarized as Siamese Network with Transformers (SiamTFR). With a fixed small set of learned object queries, we force the final set of predictions via bipartite matching, significantly reducing hyper-parameters associated with the candidate boxes. Due to the unique predictions of this framework, we significantly ease the heavy burden of hyper-parameters search of post-processings in visual tracking. Extensive experiments on visual tracking benchmarks, including GOT-10K, demonstrate that SiamTFR achieves competitive performance and runs at 50 FPS. Specifically, SiamTFR outperforms leading anchor-based tracker SiamRPN++ in the GOT-10K benchmark, confirming its effectiveness and efficiency. Furthermore, SiamTFR is deployed on the embedded device in which the algorithm can be run at 30FPS or 54FPS with TensorRT meeting the real-time requirements. In addition, we design the complete tracking system demo that can work in the real road to narrow the gap between the academic models and industrial deployments.","","978-1-6654-2755-5","10.1109/ICITES53477.2021.9637088","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9637088","UAV tracking;object tracking;transformer;siamese network","Performance evaluation;Visualization;Target tracking;Embedded systems;Roads;Force;Benchmark testing","autonomous aerial vehicles;feature extraction;learning (artificial intelligence);object detection;object tracking;query processing","UAV;autonomous real-time target tracking system;transformer tracking;predefined anchor boxes;anchor-free schemes;complicated hand-designed components;tedious post-processings;unique scenes;direct set prediction problem;Siamese-type feature extraction networks;Transformers;SiamTFR;fixed small set;learned object queries;bipartite matching;candidate boxes;hyper-parameters search;visual tracking benchmarks;FPS;anchor-based tracker SiamRPN;GOT-10K benchmark;real-time requirements;complete tracking system demo;Siamese network with transformers;Siamese-type trackers","","","","20","IEEE","20 Dec 2021","","","IEEE","IEEE Conferences"
"Towards Real-time Traffic Sign and Traffic Light Detection on Embedded Systems","O. Jayasinghe; S. Hemachandra; D. Anhettigama; S. Kariyawasam; T. Wickremasinghe; C. Ekanayake; R. Rodrigo; P. Jayasekara","Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka; Department of Electronic and Telecommunication Engineering, University of Moratuwa, Sri Lanka","2022 IEEE Intelligent Vehicles Symposium (IV)","19 Jul 2022","2022","","","723","728","Recent work done on traffic sign and traffic light detection focus on improving detection accuracy in complex scenarios, yet many fail to deliver real-time performance, specifically with limited computational resources. In this work, we propose a simple deep learning based end-to-end detection framework, which effectively tackles challenges inherent to traffic sign and traffic light detection such as small size, large number of classes and complex road scenarios. We optimize the detection models using TensorRT and integrate with Robot Operating System to deploy on an Nvidia Jetson AGX Xavier as our embedded device. The overall system achieves a high inference speed of 63 frames per second, demonstrating the capability of our system to perform in real-time. Furthermore, we introduce CeyRo, which is the first ever large-scale traffic sign and traffic light detection dataset for the Sri Lankan context. Our dataset consists of 7984 total images with 10176 traffic sign and traffic light instances covering 70 traffic sign and 5 traffic light classes. The images have a high resolution of 1920 x 1080 and capture a wide range of challenging road scenarios with different weather and lighting conditions. Our work is publicly available at https://github.com/oshadajay/CeyRo.","","978-1-6654-8821-1","10.1109/IV51971.2022.9827355","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9827355","","Performance evaluation;Deep learning;Embedded systems;Image resolution;Intelligent vehicles;Roads;Pipelines","embedded systems;image classification;learning (artificial intelligence);mobile robots;object detection;optimisation;road traffic;traffic engineering computing","embedded systems;traffic light detection focus;detection accuracy;real-time performance;end-to-end detection framework;complex road scenarios;detection models;Robot Operating System;large-scale traffic sign;traffic light detection dataset;10176 traffic sign;traffic light instances;70 traffic sign;5 traffic light classes;challenging road scenarios;different weather;lighting conditions","","","","36","IEEE","19 Jul 2022","","","IEEE","IEEE Conferences"
"An Improved Yolov5s based Real-time Spontaneous Combustion Point Detection Method","Z. Zhou; Y. Meng; R. Yu; T. Yang; D. Lu; Z. Wang","Department of Industrial Information, Zhejiang Energy Group R&D Institute Co., Ltd, Hangzhou, China; Department of Industrial Information, Zhejiang Energy Group R&D Institute Co., Ltd, Hangzhou, China; Department of Industrial Information, Zhejiang Energy Group R&D Institute Co., Ltd, Hangzhou, China; Department of Industrial Information, Zhejiang Energy Group R&D Institute Co., Ltd, Hangzhou, China; Department of Industrial Information, Zhejiang Energy Group R&D Institute Co., Ltd, Hangzhou, China; Department of Industrial Information, Zhejiang Energy Group R&D Institute Co., Ltd, Hangzhou, China","2021 IEEE International Conference on Computer Science, Electronic Information Engineering and Intelligent Control Technology (CEI)","29 Oct 2021","2021","","","213","218","The spontaneous combustion of coal pile is very easy to cause fires, especially in the closed coal yards. Early detection and disposal of spontaneous-combustion-point (SCP) could avoid large losses. However, the traditional SCP detection methods are confronted with problem of hysteresis, and cannot effectively detect the small SCP in early stage. This paper proposes a novel detection method based on Yolov5s, by using “FPN+ PAN” in the Neck layer to enhance the SCP expressions. To improve the performance of the model in learning the characteristic of small SCP, the block labeling approach is applied in the labeling process. In addition, TensorRT is adopted to accelerate the forward inference of the model, and the deployment test is carried out on Jetson nano. Through comparative analysis, the proposed method outperforms yolov3s and yolov4s in detecting the SCP in very early stage (smoke or flame), and achieves 7 FPS speed in real-time video stream.","","978-1-6654-3881-0","10.1109/CEI52496.2021.9574458","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9574458","SCP;image processing;block labeling;Yolov5s;target detection","Fires;Process control;Coal;Life estimation;Streaming media;Real-time systems;Combustion","coal;combustion;fires;object detection;video streaming","real-time spontaneous combustion point detection method;coal pile;closed coal yards;block labeling approach;improved Yolov5;SCP detection methods;yolov3s;yolov4s;7 FPS;video stream","","","","15","IEEE","29 Oct 2021","","","IEEE","IEEE Conferences"
"E.T.: Re-Thinking Self-Attention for Transformer Models on GPUs","S. Chen; S. Huang; S. Pandey; B. Li; G. R. Gao; L. Zheng; C. Ding; H. Liu",Stevens Institute of Technology; University of Connecticut; Stevens Institute of Technology; University of Connecticut; University of Delaware; University of Delaware; University of Connecticut; Stevens Institute of Technology,"SC21: International Conference for High Performance Computing, Networking, Storage and Analysis","18 Oct 2022","2021","","","1","14","Transformer-based deep learning models have become a ubiquitous vehicle to drive a variety of Natural Language Processing (NLP) related tasks beyond their accuracy ceiling. However, these models also suffer from two pronounced challenges, that is, gigantic model size and prolonged turnaround time. To this end, we introduce E.T. that rE-thinks self-attention computation for Transformer models on GPUs with the following contributions: First, we introduce a novel self-attention architecture, which encompasses two tailored self-attention operators with corresponding sequence length-aware optimizations, and operation reordering optimizations. Second, we present an attention-aware pruning design which judiciously uses various pruning algorithms to reduce more computations hence achieves significantly shorter turnaround time. For the pruning algorithms, we not only revamp the existing pruning algorithms, but also tailor new ones for transformer models. Taken together, we evaluate E.T. across a variety of benchmarks for Transformer, BERTBASE and DistilBERT, where E.T. presents superior performance over the mainstream projects, including the popular Nvidia Enterprise solutions, i.e., TensorRT and FasterTransformer.","2167-4337","978-1-4503-8442-1","10.1145/3458817.3476138","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9910069","","Deep learning;Tensors;Computational modeling;High performance computing;Computer architecture;Benchmark testing;Transformers","","","","","","63","","18 Oct 2022","","","IEEE","IEEE Conferences"
"MAPLE-Edge: A Runtime Latency Predictor for Edge Devices","S. Nair; S. Abbasi; A. Wong; M. J. Shafiee","University of Waterloo, Waterloo, Canada; University of Waterloo, Waterloo, Canada; DarwinAI, Waterloo, Canada; DarwinAI, Waterloo, Canada","2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","23 Aug 2022","2022","","","3659","3667","Neural Architecture Search (NAS) has enabled automatic discovery of more efficient neural network architectures, especially for mobile and embedded vision applications. Although recent research has proposed ways of quickly estimating latency on unseen hardware devices with just a few samples, little focus has been given to the challenges of estimating latency on runtimes using optimized graphs, such as TensorRT and specifically for edge devices. As devices like NVIDIA’s Jetsons get more popular in embedded computing and robotics, we observe a pressing need to more accurately estimate inference latency of neural network architectures on diverse runtimes, including highly optimized ones. In this work, we propose MAPLE-Edge, an edge device-oriented extension of MAPLE, the state-of-the-art latency predictor for general purpose hardware, where we train a regression network on architecture-latency pairs in conjunction with a hardware-runtime descriptor to effectively estimate latency on a diverse pool of edge devices. Compared to MAPLE, MAPLE-Edge can describe the runtime and target device platform using a much smaller set of CPU performance counters that are widely available on all Linux kernels, while still achieving up to +49.6% accuracy gains against previous state-of-the-art baseline methods on optimized edge device runtimes, using just 10 measurements from an unseen target device. We also demonstrate that unlike MAPLE which performs best when trained on a pool of devices sharing a common runtime, MAPLE-Edge can effectively generalize across runtimes by applying a trick of normalizing performance counters by the operator latency, in the measured hardware-runtime descriptor. Lastly, we show that for runtimes exhibiting lower than desired accuracy, performance can be boosted by collecting additional samples from the target device, with an extra 90 samples translating to gains of nearly +40%.","2160-7516","978-1-6654-8739-9","10.1109/CVPRW56347.2022.00410","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9857090","","Performance evaluation;Training;Runtime;Linux;Neural networks;Computer architecture;Pressing","computer vision;deep learning (artificial intelligence);embedded systems;graph theory;Linux;neural net architecture;optimisation;regression analysis","MAPLE-Edge;hardware-runtime descriptor;runtime latency predictor;neural architecture search;neural network architectures;embedded vision application;hardware devices;optimized edge device runtimes;mobile vision application;optimized graph;NVIDIA Jetsons;regression network;CPU performance;Linux kernel","","","","26","IEEE","23 Aug 2022","","","IEEE","IEEE Conferences"
"Real Time Object Detection for Traffic Based on Knowledge Distillation: 3rd Place Solution to Pair Competition","Y. Zhao; L. Wang; L. Hou; C. Gan; Z. Huang; X. Hu; H. Shen; J. Ye","AI Labs, Didi Chuxing, Beijing, China; AI Labs, Didi Chuxing, Beijing, China; Center for Research on Intelligent Perception and Computing (CRIPAC) Institute of Automation, Chinese Academy of Sciences, Beijing, China; AI Labs, Didi Chuxing, Beijing, China; AI Labs, Didi Chuxing, Beijing, China; Institute of Automation, Beijing University of Posts and Telecommunications, Beijing, China; AI Labs, Didi Chuxing, Beijing, China; AI Labs, Didi Chuxing, Beijing, China","2020 IEEE International Conference on Multimedia & Expo Workshops (ICMEW)","9 Jun 2020","2020","","","1","6","In practical applications, the purpose of object detection is to determine the target space position based on the image. At the same time, better performance is obtained under the premise of reducing the computational overhead. The dataset of the PAIR competition has the characteristics of imbalanced categories, low quality of images and inconsistent annotations. To address this issue, firstly we adopt an improved cross entropy loss function and data augmentations to rebalance the data distribution. Then the extra datasets are involved to neutralize the low images quality and annotation inconsistency issues. Secondly, this competition focuses on object detection on embedded device. So we apply knowledge distillation to fine-tune a lightweight detection model. Our detection model uses MobileNetV3 Small as backbone and SSDLite as detector head. In order to improve detection performance on small targets, FPNLite is included so that low-level features can be utilized. And we also apply TensorRT library to accelerate the inference procedure further. Eventually, our method achieves the 3rd place in the final score list of competition as the fastest, lightest and the most computation economically solution. Our code will soon be open source.","","978-1-7281-1485-9","10.1109/ICMEW46912.2020.9105963","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9105963","Object detection;knowledge distillation;lightweight model;embedded system","Object detection;Computational modeling;Training;Detectors;Feature extraction;Acceleration;Annotations","Bayes methods;entropy;image classification;learning (artificial intelligence);object detection;video signal processing","inconsistent annotations;improved cross entropy loss function;data augmentations;data distribution;extra datasets;low images quality;annotation inconsistency issues;knowledge distillation;lightweight detection model;MobileNetV3;detection performance;low-level features;computation economically solution;time object detection;rd place solution;pair competition;target space position;computational overhead;imbalanced categories","","","","35","IEEE","9 Jun 2020","","","IEEE","IEEE Conferences"
"Towards Real-time Analysis of Marine Phytoplankton Images Sampled at High Frame Rate by a YOLOX-based Object Detection Algorithm","J. Wang; C. Tang; J. Li","University of Chinese Academy of Sciences, Beijing, China; Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China; University of Chinese Academy of Sciences, Beijing, China","OCEANS 2022 - Chennai","19 May 2022","2022","","","1","9","Rapid and quantitative analysis of phytoplankton cells in natural seawater is of great need for marine ecological science research and harmful algae bloom monitoring applications. In this paper, we propose a YOLOX network-based object detection algorithm exclusively for high-throughput real-time analysis of phytoplankton fluorescence images collected by the FluoSieve® imaging flow cytometer. Based on an active learning strategy, we first annotate and construct a FluoPhyto dataset of red tide phytoplankton species fluorescence images commonly found in the South and East China Sea, which contains a total of 30,339 images in 32 different categories. Using the dataset, we train the Faster-RCNN, SSD, YOLOv3 and YOLOX networks, and the comparison result shows that the performance of YOLOX network outperforms the other networks, which can reach a mean average precision (mAP) of 90.9%. The trained YOLOX model is then deployed on an embedded GPU module and the inference speed is tested to reach 20 fps with the help of TensorRT optimization, which can hopefully meet the real-time detection requirements of the instrument. In addition, the algorithm is run on the embedded platform for detection of images collected in a red tide event that happened near the Pearl River Estuary, and the key parameters such as abundance and size spectrum of the dominant species, Cochlodinium geminatum, are obtained, which confirms the feasibility and superior performance of the detection algorithm.","","978-1-6654-1821-8","10.1109/OCEANSChennai45887.2022.9775330","Chinese Academy of Sciences; Shenzhen Science and Technology Innovation Program; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9775330","phytoplankton;object detection;imaging flow cytometry;machine vision;embedded system","Statistical analysis;Imaging;Object detection;Fluorescence;Real-time systems;Inference algorithms;Rivers","ecology;fluorescence;geophysical image processing;image sensors;learning (artificial intelligence);microorganisms;object detection;oceanographic regions;oceanographic techniques;oceanography;river pollution;rivers;seawater;tides;underwater optics","real-time analysis;marine phytoplankton images;high frame rate;YOLOX-based object detection algorithm;phytoplankton cells;natural seawater;marine ecological science research;harmful algae bloom monitoring applications;YOLOX network-based object detection algorithm;phytoplankton fluorescence images;imaging flow cytometer;active learning strategy;FluoPhyto dataset;red tide phytoplankton species;mean average precision;trained YOLOX model;real-time detection requirements;red tide event","","","","31","IEEE","19 May 2022","","","IEEE","IEEE Conferences"
"Edge-AI based Face Recognition System: Benchmarks and Analysis","A. Anwar; S. Nadeem; A. Tanvir","School of Electrical Engineering and Computer Sciences (SEECS), NUST, Islamabad, Pakistan; School of Electrical Engineering and Computer Sciences (SEECS), NUST, Islamabad, Pakistan; Centres of Excellence in Sciences and Applied Technologies (CESAT), Islamabad, Pakistan","2022 19th International Bhurban Conference on Applied Sciences and Technology (IBCAST)","30 Dec 2022","2022","","","302","307","The performance of real-time video processing tasks such as facial recognition has improved by many folds with the emergence of Artificial Intelligence (AI) and Convolutional Neural Networks (CNNs). However, face recognition models are computationally intensive and lack flexibility in deployment. Edge computing devices like Jetson (Nano, TX2) have overcome this gap by bringing high-speed and high-throughput computing capabilities to the edge. In this paper, we propose reliable integration of multi-face recognition system into compact and low-power edge devices. The deployed inference system uses a novel and light-weight face detector ‘FaceBoxes’, which is used for multi-face detection and extraction. ‘FaceNet’ is used as a face recognizer which outputs 128-dimensional embedding as feature vectors which is fed to a ‘Multilayer Perceptron (MLP)’ for face classification. We analyze the performance of facial recognition inference system on the two NVIDIA Jetson Edge computing boards (Nano, TX2). We also compare the performance of a TensorRT-based deep learning model optimization against a typical Tensorflow model implementation. The paper provides a detailed benchmarking analysis in terms of accuracy, FPS, execution time, memory usage and energy consumption. Additionally, the performance of the proposed face recognition system is compared with the state-of-the-art Multi-task Cascaded Convolutional Network (MTCNN) based face recognition system. Our findings can aid researchers in determining how this face recognition framework, when deployed on a particular Edge-GPU platform, might suit their needs for a specific face recognition application.","2151-1411","978-1-6654-6051-4","10.1109/IBCAST54850.2022.9990546","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9990546","Artificial Intelligence;Edge Inference;Deep Learning;Face Recognition;FaceNet;FaceBoxes;Jetson boards;Benchmarks","Performance evaluation;Face recognition;Computational modeling;Image edge detection;Memory management;Benchmark testing;Feature extraction","","","","","","19","IEEE","30 Dec 2022","","","IEEE","IEEE Conferences"
"A Security-Centric Deep Learning Enabled Camera Solution for Real-Time Human Fall Detection","H. R. Tohidypour; M. T. Pourazad; P. Nasiopoulos","Electrical & Computer Engineering, Univeristy of British Columbia, Vancouver, BC, Canada; Electrical & Computer Engineering, Univeristy of British Columbia, Vancouver, BC, Canada; Electrical & Computer Engineering, Univeristy of British Columbia, Vancouver, BC, Canada","2022 18th International Conference on Wireless and Mobile Computing, Networking and Communications (WiMob)","15 Nov 2022","2022","","","42","46","Automatic human real-time fall detection is a challenging task in remote healthcare, demanding a non-intrusive, secure and affordable solution. In this paper, we present a real-time hardware system that uses a deep learning model for fall detection embedded in a color camera. To reduce the startup delay and achieve real-time performance for the inference phase, we optimized our model using TensorRT. In addition, we addressed the board memory limitation using virtual memory and linear memory allocation and garbage collection. Moreover, GStreamer was used to perform most of the video processing using Jetson's GPU. Our live evaluation shows that our system achieved the accuracy of 84.44% and real-time performance.","2160-4894","978-1-6654-6975-3","10.1109/WiMob55322.2022.9941359","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9941359","fall detection;hardware;deep learning;smart home","Deep learning;Wireless communication;Graphics processing units;Streaming media;Cameras;Real-time systems;Hardware","cameras;health care;image motion analysis;learning (artificial intelligence);object detection;patient monitoring;storage management;video signal processing","affordable solution;automatic human real-time fall detection;board memory limitation;color camera;deep learning model;garbage collection;linear memory allocation;real-time hardware system;real-time human fall detection;real-time performance;remote healthcare;secure solution;security-centric deep learning enabled camera solution;startup delay;virtual memory","","","","14","IEEE","15 Nov 2022","","","IEEE","IEEE Conferences"
"A Steering Strategy for Self-Driving Automobile Systems Based on Lane-Line Detection","T. -D. Phan; T. -T. -N. Nguyen; M. -T. Duong; C. -T. Nguyen; H. -A. Le; M. -H. Le","Intelligent System Laboratory, HCMC University of Technology and Education, Ho Chi Minh City, Vietnam; Intelligent System Laboratory, HCMC University of Technology and Education, Ho Chi Minh City, Vietnam; Department of Information and Telecommunication Engineering, Soongsil University, Seoul, South Korea; Intelligent System Laboratory, HCMC University of Technology and Education, Ho Chi Minh City, Vietnam; Faculty of Mechatronics and Electronics, Lac Hong University, Bien Hoa city, Dong Nai province, Vietnam; Faculty of Electrical and Electronics Engineering, HCMC University of Technology and Education, Ho Chi Minh City, Vietnam","2022 6th International Conference on Green Technology and Sustainable Development (GTSD)","29 Dec 2022","2022","","","724","730","Steering strategy is an essential task for self-driving automobile systems. However, studies on this problem have not yet achieved satisfactory results and typically cause navigation tasks to be difficult. Therefore, this paper proposes a novel steering strategy based on the lane-line detection model. First and foremost, the row-based selecting strategy and CNN-based extraction were adopted to anticipate lane-line markers from the images captured from a front-view monocular camera. Next, the lane-line detection model output is utilized to estimate the next destination for the self-driving automobile, and then we converted the model to TensorRT pattern with Float16 format. Depending on the result of the lane-line detection model, we designed a strategy to control the steering wheel through a DC Servo motor. Last but not least, the whole algorithm is deployed on the golf cart to perform navigation tasks. The experimental result proves that our model achieves approximately 50 frames per second (50 fps) on our laptop with GTX 1650 graphic card during the testing stage and can work with satisfactory performance on the HCMUTE campus.","","978-1-6654-6628-8","10.1109/GTSD54989.2022.9989030","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9989030","Driving Strategy;Lane-line Detection;Motion Planning;Self-Driving Automobile Systems","Navigation;Wheels;Sensor fusion;Autonomous automobiles;Trajectory;Automobiles;Task analysis","","","","","","23","IEEE","29 Dec 2022","","","IEEE","IEEE Conferences"
"Human Action Recognition of Autonomous Mobile Robot Using Edge-AI","S. -T. Wang; I. -H. Li; W. -Y. Wang","Department of Electrical Engineering, National Taiwan Normal University, Taipei, Taiwan; Department of Mechanical and Electro-Mechanical Engineering, Tamkang University, New Taipei City, Taiwan; Department of Electrical Engineering, National Taiwan Normal University, Taipei, Taiwan","IEEE Sensors Journal","13 Jan 2023","2023","23","2","1671","1682","The development of autonomous mobile robots (AMRs) has brought with its requirements for intelligence and safety. Human action recognition (HAR) within AMR has become increasingly important because it provides interactive cognition between human and AMR. This study presents a full architecture for edge-artificial intelligence HAR (Edge-AI HAR) to allow AMR to detect human actions in real time. The architecture consists of three parts: a human detection and tracking network, a key frame extraction function, and a HAR network. The HAR network is a cascade of a DenseNet121 and a double-layer bidirectional long-short-term-memory (DLBiLSTM), in which the DenseNet121 is a pretrained model to extract spatial features from action key frames and the DLBiLSTM provides a deep two-directional LSTM inference to classify complicated time-series human actions. Edge-AI HAR undergoes two optimizations—ROS distributed computation and TensorRT structure optimization—to give a small model structure and high computational efficiency. Edge-AI HAR is demonstrated in two experiments using an AMR and is demonstrated to give an average precision of 97.58% for single action recognition and around 86% for continuous action recognition.","1558-1748","","10.1109/JSEN.2022.3225158","National Science and Technology Council, Taiwan(grant numbers:MOST 110-2221-E-032-036,MOST 110-2221-E-032-036,MOST 111-2221-E-003-025,MOST 110-2634-F-A49-004); ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9969627","Autonomous mobile robot (AMR);bidirectional long-short-term-memory (BiLSTM);edge artificial intelligence (Edge AI);human action recognition (HAR);ROS","Feature extraction;Image edge detection;Deep learning;Computational modeling;Sensors;Skeleton;Optimization","","","","","","43","IEEE","2 Dec 2022","","","IEEE","IEEE Journals"
"Vietnamese vehicles speed detection with video-based and deep learning for real-time traffic flow analysis system","P. H. Quang; P. P. Thanh; T. N. V. Anh; S. V. Phi; B. L. Nhat; H. N. Trong","Aviation Technique Falculty Vietnam Aviation Academy, Ho Chi Minh City, Vietnam; Electronics and Telecommunications Vietnam Aviation Academy, Ho Chi Minh City, Vietnam; Electronics and Telecommunications Vietnam Aviation Academy, Ho Chi Minh City, Vietnam; Electronics and Telecommunications Vietnam Aviation Academy, Ho Chi Minh City, Vietnam; Electronics and Telecommunications Vietnam Aviation Academy, Ho Chi Minh City, Vietnam; Ho Chi Minh City University of Technology HUTECH, Ho Chi Minh City, Vietnam","2021 15th International Conference on Advanced Computing and Applications (ACOMP)","10 Jan 2022","2021","","","62","69","In this paper, we have developed a system to leverage traffic surveillance cameras to detect vehicle speed. In this system, we use a detection-based tracking paradigm for multiple object tracking then speed is estimated. First, YOLOv4 with transfer learning is applied for vehicle detection, a comparative analysis is carried out to choose trackers that work well with YOLOv4 in this task. Finally tracked vehicles’ traveled distance is back-projected to the 3D world by Haversine method for speed estimation. In order to deploy to edge device, we take the advantage of tensorRT framework and ONXX technology to optimize models and modify model format as well as accelerate inferencing. For the suitability to the Vietnamese traffic scenario, feature extraction models of tracking task and detection task were fine-tuned. The system is then optimized and modified to detect common Vietnamese vehicles’ speed in normal conditions and low light intensity conditions with the video stream fed directly from the preinstalled traffic surveillance camera. The whole system proceeds AI inference and processing with the help of Nvidia Jetson NX Xavier. All modules are packed into a small box which helps simplify the integration to available traffic cameras. This would release the need for radar and sensors which are usually extremely expensive and need a lot of calibration and maintenance.","2688-0202","978-1-6654-0639-0","10.1109/ACOMP53746.2021.00015","","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9668251","speed detection;Yolo4;deep learning;Haversin;Jetson NX Xavier","Three-dimensional displays;Vehicle detection;Surveillance;Image edge detection;Estimation;Detectors;Traffic control","cameras;feature extraction;learning (artificial intelligence);object detection;object tracking;road traffic;road vehicles;target tracking;traffic engineering computing;video signal processing;video streaming;video surveillance","Vietnamese vehicles speed detection;video-based learning;deep learning;real-time traffic flow analysis system;traffic surveillance cameras;vehicle speed;detection-based;multiple object tracking;YOLOv4;transfer learning;vehicle detection;comparative analysis;speed estimation;model format;Vietnamese traffic scenario;feature extraction models;common Vietnamese vehicles;low light intensity conditions;video stream;preinstalled traffic surveillance camera;available traffic cameras","","","","41","IEEE","10 Jan 2022","","","IEEE","IEEE Conferences"
"Accelerating Multi-Object Tracking in Edge Computing Environment with Time-Spatial Optimization","M. Liu; A. Tang; H. Wang; L. Shen; Y. Chang; G. Cai; D. Yin; F. Dong; W. Zhao","School of Computer Science and Communication Engineering, Jiangsu University, Zhenjiang, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; School of Computer Science and Engineering, Southeast University, Nanjing, China; School of Computer Science and Technology, Anhui University of Technology, Ma’anshan, China","2021 Ninth International Conference on Advanced Cloud and Big Data (CBD)","11 Jul 2022","2022","","","279","284","Multi-Object Tracking (MOT) task is a key issue in computer vision. It has capacious application prospects in survelliance, self-driving, and augmented reality. Benefit from the continuous progress of Deep Neural Network (DNN), the accuracy of the MOT algorithm has been significantly improved. Nevertheless, limited to computing power, achieving real-time DNN-based MOT is difficult in embedded systems. In reality, there are many wasteful and unnecessary computations in traditional frame-by-frame full-size video analysis. Therefore, in this paper, we propose a strategy that optimizing the execution of a traditional MOT pipeline in the dimension of time and space. In the temporal dimension, DNN only works in periodic keyframes while using a lightweight model for quickly generating results in the common frames. In the spatial dimension, we design an image density region discriminator to narrow down the input size of DNN. An edge device is introduced to perform end-edge collaborative computing to further accelerating the execution. Additionally, an end-edge parallel computing mechanism is designed that performing dynamic decisions based on the computing power and network environment between end and edge. Moreover, we rebuild the DNN model by TensorRT to optimize the model structure of DNN. By integrating the above approaches, the system can achieve 17.6 ~ 38.1 × speedup ratio, while with 3%~10.4% absolute tracking accuracy sacrifice and can be deployed in an unstable network environment.","","978-1-6654-0745-8","10.1109/CBD54617.2021.00055","National Natural Science Foundation of China; Natural Science Foundation of Jiangsu Province; Ministry of Education; ","https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9816251","Multi-Object Tracking;Deep Learning;Inference Optimization;Edge Intelligence","Performance evaluation;Computational modeling;Image edge detection;Pipelines;Collaboration;Bandwidth;Streaming media","augmented reality;computer vision;embedded systems;neural nets;object detection;object tracking;optimisation;parallel processing;real-time systems;target tracking;video signal processing","edge computing environment;time-spatial optimization;MultiObject Tracking task;computer vision;capacious application prospects;continuous progress;Deep Neural Network;MOT algorithm;computing power;real-time DNN-based MOT;embedded systems;wasteful computations;unnecessary computations;frame-by-frame full-size video analysis;traditional MOT pipeline;temporal dimension;lightweight model;common frames;spatial dimension;input size;edge device;end-edge collaborative computing;end-edge parallel computing mechanism;DNN model;3%~10.4% absolute tracking accuracy sacrifice;unstable network environment","","","","17","IEEE","11 Jul 2022","","","IEEE","IEEE Conferences"
