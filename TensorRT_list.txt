
ORIGINAL LIST

+ A Deep Learning Compiler for Vector Processor, 10.1007/978-3-030-67720-6_46
+ Accelerated identification method of industrial instrument based on yolov5, 10.1109/ICICML57342.2022.10009712
+ Accelerating Depthwise Separable Convolutions with Vector Processor, 10.1007/978-3-030-86340-1_12
+ AdaNeRF: Adaptive Sampling for Real-Time Rendering of Neural Radiance Fields, 10.1007/978-3-031-19790-1_16
- Applying Recent Machine Learning Approaches to Accelerate the Algebraic Multigrid Method for Fluid Simulations, 10.1007/978-3-030-96498-6_3
+ A Performance Study Depending on Execution Times of Various Frameworks in Machine Learning Inference, 10.1109/UYMS54260.2021.9659677
+ Combining Task- and Data-Level Parallelism for High-Throughput CNN Inference on Embedded CPUs-GPUs MPSoCs, 10.1007/978-3-030-60939-9_2
+ Comprehensive Analysis of Heterogeneous Computing Performance of Dnns Under Typical Frameworks on Cloud and Edge Computing Platforms, 10.2139/ssrn.4176358 
+ DeepCuts: a deep learning optimization framework for versatile GPU workloads, 10.1145/3453483.3454038
+ DeepEdgeBench: Benchmarking Deep Neural Networks on Edge Devices, 10.1109/IC2E52221.2021.00016
+ Deep Learning at Scale on NVIDIA V100 Accelerators 10.1109/PMBS.2018.8641600
+ Deep Learning Based Super-Resolution for Medical Volume Visualization with Direct Volume Rendering, 10.1007/978-3-031-20713-6_8
+ Deep Learning Inference Parallelization on Heterogeneous Processors With TensorRT, 10.1109/LES.2021.3087707
+ Efficient Integer-Arithmetic-Only Convolutional Networks with Bounded ReLU, 10.1109/ISCAS51556.2021.9401448
+ Energy-Efficient and High-Throughput CNN Inference on Embedded CPUs-GPUs MPSoCs, 10.1007/978-3-031-04580-6_9
+ Evaluating and analyzing the energy efficiency of CNN inference on high-performance GPU, 10.1002/cpe.6064
+ Exploring TensorRT to Improve Real-Time Inference for Deep Learning
+ HALF: Holistic Auto Machine Learning for FPGAs, 10.1109/FPL53798.2021.00069
+ How Can Deep Neural Networks Be Generated Efficiently for Devices with Limited Resources?, 10.1007/978-3-319-94544-6_3
- Improving the performance of sea surface temperature predictions using a revised method, 10.1080/2150704X.2021.2005269
+ LANA: Latency Aware Network Acceleration, 10.1007/978-3-031-19775-8_9
+ Learning Compression from Limited Unlabeled Data, 10.1007/978-3-030-01246-5_46
+ Measuring Inference Performance of Machine-Learning Frameworks on Edge-class Devices with the MLMark™ Benchmark
+ On Practical Approach to Uniform Quantization of Non-redundant Neural Networks, 10.1007/978-3-030-30484-3_29
+ Optimizing Driver Assistance Systems for Real-Time performance on Resource Constrained GPUs, 10.1109/CICT48419.2019.9066239
+ Optimizing Winograd-Based Convolution with Tensor Cores, 10.1145/3472456.3472473
+ ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks, 10.48550/arXiv.2012.01671
+ SMOF: Squeezing More Out of Filters Yields Hardware-Friendly CNN Pruning, 10.1007/978-3-031-20497-5_20
+ Tensor-Based CUDA Optimization for ANN Inferencing Using Parallel Acceleration on Embedded GPU, 10.1007/978-3-030-49161-1_25


ANALYSIS

- A Deep Learning Compiler for Vector Processor

    They write their own compiler for ML models, it applies optimizations, but there is no comparison with respect to inference with TensorRT.

- Accelerated identification method of industrial instrument based on yolov5

    "The model acceleration was carried out on the Jetson TX2 platform based on tensorRT, and the instrument online detection speed of 30 frames/s was achieved"
    Using TensorRT is faster than using the inference of PyTorch.

- Accelerating Depthwise Separable Convolutions with Vector Processor

    "We found that using the TensorRT acceleration library, the GPU achieves high computational performance, but the computational efficiency is low. Increasing the batch size improves the computational efficiency and performance of GPU significantly. Therefore, we use one as the size to test the effectiveness of the method."
    The authors present a technique to run a specialized convolution on vector processors.
    Their hardware is more efficient, but performance or TensorRT are higher.

- AdaNeRF: Adaptive Sampling for Real-Time Rendering of Neural Radiance Fields

    "The resulting sparse, dual-network pipeline can be rendered in real-time on consumer GPUs using our custom real-time renderer based on CUDA and TensorRT."
    - Their network is the fastest, but they compare TensorRT against TensorRT, and only in one case the network is not executed via TensorRT; they also win in that case

- A Performance Study Depending on Execution Times of Various Frameworks in Machine Learning inference

    "In this most crucial part, we deployed our models in Libtorch, ONNX Runtime and TensorRT."
    "It is evident from Table I that ONNX + TensorRT (GPU) significantly outperforms other methods"
    The paper is not very detailed, but they conclude that ONNX+TensorRT is the fastest inference.


- Combining Task- and Data-Level Parallelism for High-Throughput CNN Inference on Embedded CPUs-GPUs MPSoCs

    "the best-known and state-of-the-art for the NVIDIA Jetson TX2 MPSoC, Tensorrt DL framework [12], which exploits only data-level parallelism, available in the CNN"
    Their own framework uses both data and task-parallelism.
    The performance results show that they can beat TensorRT, but performance are close.
    Performance improvement may come from this task+data parallel approach more than be implementation dependent.

- Comprehensive Analysis of Heterogeneous Computing Performance of Dnns Under Typical Frameworks on Cloud and Edge Computing Platforms

    "PyTorch framework is used for the training and inference aims on both devices, and TensorRT framework is only used for the inference purpose on Jetson Nano."
    "The inference framework of PyTorch was applied in cloud platform and edge platform. TensorRT inference framework was only applied in edge platform"
    "The results presented in Table 2 reveal that under PyTorch inference framework, the inference time of Colab and Jetson Nano will decrease with the increase of batch sizes, however when it decreases to a certain extent, it tends to be stable and will not continue to decrease."
    "when using the NVIDIA TensorRT framework or efficiently deploying NNs onto the embedded Jetson Nano computing platform, the inference time of the same network model is always at a nearly stable level under different batch size."
    Unfortunately they use TensorRT only on the Jetson and not on the discrete GPUs; anyway interesting.

- DeepCuts: a deep learning optimization framework for versatile GPU workloads

    "It achieves speedups of 1.25, 1.48, and 1.36 over TVM, TensorFlow XLA, and TensorRT, respectively, when including cuDNN primitives in the kernel selection process."
    "TensorRT[32] uses cuDNN-like pre-implemented GPU kernels for some specific patterns of DL operations (e.g., a convolution operation followed by a batch normalization operation and a ReLU operation). Although TensorRT shows good performance for the models that use the supported patterns, it fails to achieve good performance for the models that do not have the patterns."
    "Even though DeepCuts-no-fusion is worse than cuDNN (0.95×), DeepCuts-fusion outperforms cuDNN (1.11×). TensorFlow-XLA and TensorRT also perform better than cuDNN for the largest-batch CNN inference."
    "DeepCuts-mixed, DeepCuts-fusion, TensorFlow-XLA, and TensorRT are much better than cuDNN especially for MobileNet-v2 with a large batch size (64 or 256). This is because the inference workload of MobileNet-v2 is suitable for kernel fusion."
    "For small-batch BERT inference, TensorRT outperforms DeepCuts-mixed and cuDNN because of its manually-tuned kernels"
    "While DeepCuts spends a lot of time on finding the best-performing implementation parameters of convolution and matrix multiplication, TensorFlow-XLA just uses cuDNN/cuBLAS primitives and TensorRT uses pre-implemented kernels resulting in much faster code generation time."
    From this paper we can deduct that in theory it is possible to do better than TensorRT with code generation and tuning, and that kernel fusion seems to provide performance gains, but that it is a complex task, and in some cases TensorRT is still better.

- DeepEdgeBench: Benchmarking Deep Neural Networks on Edge Devices

    This is a comparison of hardware platform, software has a role but it is not the main focus. It is not very relevant for our research.

- Deep Learning at Scale on NVIDIA V100 Accelerators

    " For inference, we will use Nvidia’s TensorRT library as it not only supports inference with the commonly used 32-bit floating point (FP32), but also with 8- bit integers (INT8). We will compare the throughput advantage of INT8, and highlight the negligible difference in accuracy when compared to FP32."
    "This section quantifies the inference performance using NVIDIA’s TensorRT library."
    "running models with INT8 only works if the batch size is evenly divisible by 4"
    "We can see that INT8 mode is ∼3.7x faster than FP32."
    "In certain instances, delayed batch processing is acceptable in which case the focus is using large batch sizes to increase throughput."
    "t can be seen that without batch processing the inference performance is very low. This is because the GPU is not assigned enough workload to keep it busy. The larger the batch size was used, the higher the inference performance was produced. This begins to taper off as the batch size increases. The largest batch size is only limited by GPU memory."
    This is an interesting paper, and our findings so far seem to agree with it. Need to investigate INT8 for our application.

- Deep Learning Based Super-Resolution for Medical Volume Visualization with Direct Volume Rendering

    "). With run-time optimizations and integration of TensorRT, which can provide up to 6x faster accelerated inference[19], our system has the potential to achieve real-time frame-rate."
    They do not investigate inference.

- Deep Learning Inference Parallelization on Heterogeneous Processors With TensorRT

    "In this letter, we present a TensorRT-based parallelization method that uses both GPU and NPUs to maximize the throughput of a single DNN application."
    Not relevant for our work, only highlights the fact that using more of the available hardware could be beneficial in some cases.

- Efficient Integer-Arithmetic-Only Convolutional Networks with Bounded ReLU

    "We have proposed a method for converting FPN networks into integer-arithmetic-only networks without sacrificing accuracy"
    Not relevant for our work, although it seems to point to the fact that we should explore INT8 networks.

- Energy-Efficient and High-Throughput CNN Inference on Embedded CPUs-GPUs MPSoCs

    "The obtained results, in terms of system throughput and energy consumption, are compared with results obtained by the best-known DL framework for Jetson MPSoCs called TensorRT [15]. This comparison shows that our extended methodology can achieve CNN inference on embedded CPUs-GPUs MPSoCs with the same or higher throughput but with less energy consumption"
    "However, TensorRT relies on layer-by-layer (sequential) execution of CNN layers and only one CPU processor is utilized for launching GPU engines and sending data. In this way, the available CPUs-GPUs MPSoC hardware resources are not utilized in the most efficient way in terms of high-throughput."
    Again this is not directly applicable, and again the way to beat TensorRT is to utilize more hardware.

- Evaluating and analyzing the energy efficiency of CNN inference on high-performance GPUs

    "The energy efficiency of TensorRT is 1.53x than that of TensorFlow."
    "TensorRT can reduce DRAM read/write transactions by 85.87% on average"
    "In the experimental evaluation, we use TF-TRT35as the inference code, which is a part of TensorFlow that optimizes Tensor-Flow graphs using TensorRT"
    Although focused on energy efficiency, TensorRT looks good in this paper, thanks to the model transformation. Reducing memory transactions should not only lower energy consumption, but also improve performance.

- Exploring TensorRT to Improve Real-Time Inference for Deep Learning

    "In this paper, we examine the effectiveness of TensorRT by comparing it to the vanilla (without TensorRT) PyTorch framework. In particular, there are three workflows to integrate the TensorRT engines with PyTorch-compatible models. We evaluate the performance of these three TensorRT integration workflows under a variety of workloads"
    "In this work, we compare the conventional, default workflow in PyTorch that does not involve TensorRT at all (denoted as W0) with three workflows that do integrate TensorRT (tagged by W1, W2, W3, respectively)."
    "(W1) In this workflow, we accelerate inference using Torch-TensorRT, an integration of PyTorch with NVIDIA TensorRT."
    "(W2) There are three steps in this workflow: 1) exporting the PyTorch model to the ONNX file, 2)TensorRT engine building 3)TensorRT engine deployment in addition to model loading."
    "(W3) This workflow also starts with exporting a model from PyTorch to ONNX using torch.onnx.export() function in PyTorch library. After obtaining the ONNX model, we run the model inference using the ONNX Runtime execution provider(EP)."
    "Adopting TensorRT incurs minimum inference outputs difference compared to the default PyTorch framework. In other words, the benefits TensorRT brings in are not at the cost of noticeable sacrifice in inference accuracy."
    "Applying TensorRT can significantly improve the inference time. Light-weighted model architecture, such as MobileNet and SqueezeNet, can benefit even more from TensorRT."
    "By applying TensorRT, improvement on inference throughput can be obtained in most cases and may be very significant for certain network architectures. Inference throughput increases with batch size increases until reaches hardware limits."
    "TensorRT, especially via the workflow W2: ONNX to TensorRT Conversion, utilizes GPU memory in a significantly more efficient and scalable manner."

- HALF: Holistic Auto Machine Learning for FPGAs

    "he FPGA designs outperform the embedded GPU implementation regarding all shown metrics, although the GPU has a higher frequency, larger batch size, and a model optimized with TensorRT"
    Not really relevant as we are not considering FPGAs.

- How Can Deep Neural Networks Be Generated Efficiently for Devices with Limited Resources?

    Not relevant for our work.

- LANA: Latency Aware Network Acceleration

    "We measure latency in 2 settings: (i) in Pytorch framework, and (ii) TensorRT [54]."
    They use TensorRT and latency is lower than PyTorch.

- Learning Compression from Limited Unlabeled Data

    They only seem to use TensorRT, and it looks faster than their baseline.

- Measuring Inference Performance of Machine-Learning Frameworks on Edge-class Devices with the MLMark™ Benchmark

    "In Figure 6, the batch size was increased on the NVIDIA Jetson Xavier from 1 (a single image) to 32 simultaneous images per call to the inference engine for the MobileNet V1.0 workload. TensorRT also supports concurrent streams, which are plotted against the batch data for similar input sizes. Latency at the 95th percentile is plotted on the secondary vertical axis. While batching shows a significant increase in performance at the start of the curve, ROI begins to decrease rapidly after 16 images. Latency for batching is relatively linear. Concurrency shows no increase in throughput and even worse latency."
    It seems that using multiple batches is more effective than having multiple streams, but this is on a small embedded GPU.

- On Practical Approach to Uniform Quantization of Non-redundant Neural Networks

    TensorRT is mentioned, but there is no performance comparison for inference, so this paper is not relevant for our research.

- Optimizing Driver Assistance Systems for Real-Time performance on Resource Constrained GPUs

    "In order to optimize this algorithm for real-time functionality, utilization of TensorFlow TensorRT [25] is proposed"
    "Due to optimizations performed by TensorFlow TensorRT (TF-TRT), the model now yields an inference time of 7 seconds on TX2, which is over 50% improvement with the same accuracy of 78.75% as presented in Table 3"
    The paper is short but it shows again how the model transformation of TensorRT is effective for inference performance.

- Optimizing Winograd-Based Convolution with Tensor Cores

    "The experiments show that our mixed precision F (6 × 6, 3 × 3) Winograd convolution achieves up to 15.71x and 2.41x speedup on NVIDIA Ampere A100, compared with Winograd convolution and GEMM based convolution in cuDNN."
    "We integrate our F (6 × 6, 3 × 3) Winograd convolution into the inference TensorRT SDK developed by NVIDIA, using the interfaces of plugins provided by TensorRT for custom layers"
    The paper is not completely relevant, but it shows one potential of TensorRT, the integration of new kernels for some layers, either implemented manually or provided by NVIDIA.

- ResPerfNet: Deep Residual Learning for Regressional Performance Modeling of Deep Neural Networks

    In this paper they model the performance (accuracy/time) of neural networks, so it is not relevant for us.

- SMOF: Squeezing More Out of Filters Yields Hardware-Friendly CNN Pruning

    Pruning of networks, not really relevant.

- Tensor-Based CUDA Optimization for ANN Inferencing Using Parallel Acceleration on Embedded GPU

    "Running on the GPU for parallelization, the inference performance is evaluated with and without optimization using TensorRT inference accelerator."
    "with TensorRT optimization, the performance showed a drastic improvement from 147% in Max-P Core All to 254% improvement in Max-P Core ARM"
    On embedded GPU using TensorRT compared against native CUDA, for the same network, results in better performance.
